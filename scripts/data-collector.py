#!/usr/bin/env python3
"""
ãƒ‡ãƒ¼ã‚¿åé›†å°‚ç”¨ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
å£²ä¸Šãƒ»æ ªä¾¡ãƒ»ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ã—ã¦çµ±åˆJSONãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
"""

import os
import sys
import json
import asyncio
from datetime import datetime, timedelta
import argparse

# æ—¢å­˜ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
# ã‚«ãƒ¬ãƒ³ãƒˆãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒscriptsã®å ´åˆã¨rootã®å ´åˆã®ä¸¡æ–¹ã«å¯¾å¿œ
try:
    exec(open('data-processing.py').read())
except FileNotFoundError:
    exec(open('scripts/data-processing.py').read())

class DataCollector:
    def __init__(self, cache_hours=6):
        """
        ãƒ‡ãƒ¼ã‚¿åé›†ã‚¯ãƒ©ã‚¹
        
        Args:
            cache_hours (int): ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æ™‚é–“ï¼ˆæ™‚é–“ï¼‰
        """
        self.cache_hours = cache_hours
        self.cache_file = "data/integrated_data.json"
        self.processor = None  # é…å»¶åˆæœŸåŒ–
        
    def _get_processor(self):
        """WeeklyReportProcessorã®é…å»¶åˆæœŸåŒ–"""
        if self.processor is None:
            self.processor = WeeklyReportProcessor()
        return self.processor
    
    def is_cache_valid(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã‹ãƒã‚§ãƒƒã‚¯"""
        if not os.path.exists(self.cache_file):
            return False
            
        try:
            with open(self.cache_file, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            generated_at = datetime.fromisoformat(data['metadata']['generated_at'])
            cache_expiry = generated_at + timedelta(hours=self.cache_hours)
            
            return datetime.now() < cache_expiry
        except:
            return False
    
    async def collect_all_data(self, force_refresh=False):
        """
        å…¨ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
        
        Args:
            force_refresh (bool): å¼·åˆ¶æ›´æ–°ãƒ•ãƒ©ã‚°
        
        Returns:
            dict: çµ±åˆãƒ‡ãƒ¼ã‚¿
        """
        if not force_refresh and self.is_cache_valid():
            print("âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã§ã™ - ãƒ‡ãƒ¼ã‚¿åé›†ã‚’ã‚¹ã‚­ãƒƒãƒ—")
            with open(self.cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        print("ğŸš€ ãƒ‡ãƒ¼ã‚¿åé›†ã‚’é–‹å§‹ã—ã¾ã™...")
        
        # 1. ãƒ“ã‚¸ãƒã‚¹ãƒ‡ãƒ¼ã‚¿åé›†
        print("ğŸ“Š ãƒ“ã‚¸ãƒã‚¹ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­...")
        business_data = self._get_processor().process_sales_data()
        
        # 2. æ ªä¾¡ãƒ‡ãƒ¼ã‚¿åé›†
        print("ğŸ“ˆ æ ªä¾¡ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­...")
        stock_data = {}
        try:
            # yfinanceã‚’ä½¿ç”¨ã—ã¦æ ªä¾¡ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            import yfinance as yf
            
            tickers = ['N225', 'SPY', 'RCRUY']
            for ticker in tickers:
                try:
                    if ticker == 'N225':
                        # æ—¥çµŒå¹³å‡
                        stock = yf.Ticker('^N225')
                    elif ticker == 'SPY':
                        # S&P 500 ETF
                        stock = yf.Ticker('SPY')
                    elif ticker == 'RCRUY':
                        # ãƒªã‚¯ãƒ«ãƒ¼ãƒˆHD ADR
                        stock = yf.Ticker('RCRUY')
                    
                    hist = stock.history(period='2d')
                    if len(hist) >= 2:
                        current_price = hist['Close'].iloc[-1]
                        prev_price = hist['Close'].iloc[-2]
                        change = current_price - prev_price
                        change_percent = (change / prev_price) * 100
                        
                        stock_data[ticker] = {
                            "current_price": round(current_price, 2),
                            "change": round(change, 2),
                            "change_percent": round(change_percent, 2),
                            "currency": "JPY" if ticker == 'N225' else "USD",
                            "status": "success"
                        }
                        
                        # USDéŠ˜æŸ„ã¯å††æ›ç®—ã‚‚è¿½åŠ 
                        if ticker != 'N225':
                            usd_to_jpy = 150  # ä»®ãƒ¬ãƒ¼ãƒˆ
                            stock_data[ticker]["current_price_jpy"] = round(current_price * usd_to_jpy)
                            stock_data[ticker]["change_jpy"] = round(change * usd_to_jpy)
                        
                        print(f"   âœ… {ticker}: Â¥{stock_data[ticker].get('current_price_jpy', stock_data[ticker]['current_price'])}")
                    
                except Exception as e:
                    print(f"   âš ï¸ {ticker} å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                    stock_data[ticker] = {"status": "error", "error": str(e)}
                    
        except ImportError:
            print("   âš ï¸ yfinanceæœªã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ« - æ¨¡æ“¬ãƒ‡ãƒ¼ã‚¿ã‚’ä½¿ç”¨")
            stock_data = self._get_mock_stock_data()
        
        # 3. ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿åé›†
        print("ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿åé›†ä¸­...")
        news_data = await self._collect_news_data()
        
        # 4. ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿åé›†
        print("ğŸ“… ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿å‡¦ç†ä¸­...")
        schedule_data = self._collect_schedule_data()
        
        # çµ±åˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
        integrated_data = {
            "metadata": {
                "generated_at": datetime.now().isoformat(),
                "data_period": self._get_period_description(),
                "version": "1.0"
            },
            "business_data": business_data,
            "stock_data": stock_data,
            "news_data": news_data,
            "schedule_data": schedule_data
        }
        
        # ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜
        os.makedirs("data", exist_ok=True)
        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump(integrated_data, f, ensure_ascii=False, indent=2)
        
        print(f"ğŸ’¾ çµ±åˆãƒ‡ãƒ¼ã‚¿ã‚’ä¿å­˜ã—ã¾ã—ãŸ: {self.cache_file}")
        return integrated_data
    
    async def _collect_news_data(self):
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿åé›†"""
        try:
            # æ—¢å­˜ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†æ©Ÿèƒ½ã‚’ä½¿ç”¨
            from datetime import datetime, timedelta
            
            # æœŸé–“è¨­å®šï¼ˆéå»7æ—¥é–“ï¼‰
            end_date = datetime.now()
            start_date = end_date - timedelta(days=7)
            
            # AIé§†å‹•ãƒ‹ãƒ¥ãƒ¼ã‚¹åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’å®Ÿè¡Œ
            print("ğŸ¤– AIé§†å‹•ãƒ‹ãƒ¥ãƒ¼ã‚¹åˆ†æãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹")
            
            # ä¼æ¥­è¨­å®šã‚’èª­ã¿è¾¼ã¿
            companies_file = "config/companies.json"
            if os.path.exists(companies_file):
                with open(companies_file, 'r', encoding='utf-8') as f:
                    companies = json.load(f)
                print(f"âœ… ä¼æ¥­è¨­å®šèª­ã¿è¾¼ã¿å®Œäº†: {len(companies)}ç¤¾")
            else:
                print("âš ï¸ ä¼æ¥­è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ - RSSåé›†ã®ã¿å®Ÿè¡Œ")
                companies = []
            
            # ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ï¼ˆRSS + GNews APIï¼‰
            all_articles = []
            
            # GNews APIã‚’ä½¿ç”¨ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†
            gnews_articles = await self._fetch_from_gnews()
            all_articles.extend(gnews_articles)
            
            # RSSåé›†ï¼ˆè£œåŠ©çš„ï¼‰
            if companies:
                for company in companies[:2]:  # æœ€åˆã®2ç¤¾ã®ã¿ï¼ˆGNews APIã¨åˆã‚ã›ã¦èª¿æ•´ï¼‰
                    company_name = company.get('name', 'Unknown')
                    print(f"ğŸ“Š {company_name} ã®RSSåé›†ä¸­...")
                    
                    # RSSåé›†
                    if 'rss_urls' in company:
                        for rss_url in company['rss_urls']:
                            try:
                                # RSSè§£æï¼ˆç°¡ç•¥ç‰ˆï¼‰
                                import feedparser
                                feed = feedparser.parse(rss_url)
                                
                                for entry in feed.entries[:2]:  # æœ€æ–°2ä»¶ã®ã¿
                                    # ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜ã‚’æ—¥æœ¬èªã«ç¿»è¨³
                                    title_jp = self._translate_to_japanese(entry.get('title', ''))
                                    summary_jp = self._translate_to_japanese(entry.get('summary', '')[:150]) + '...'
                                    
                                    article = {
                                        'title': title_jp,
                                        'summary_jp': summary_jp,
                                        'url': entry.get('link', ''),
                                        'published_at': datetime.now().strftime('%Y-%m-%d'),
                                        'company': company_name.lower().replace(' ', '_'),
                                        'score': 4.0 + (len(all_articles) % 3) * 0.5  # æ¨¡æ“¬ã‚¹ã‚³ã‚¢
                                    }
                                    all_articles.append(article)
                                    
                            except Exception as e:
                                print(f"   âš ï¸ RSSå–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
            
            # é‡è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ‰‹å‹•è¿½åŠ ï¼ˆPerplexityè²·åãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼‰
            important_news = self._add_important_news()
            # é‡è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å…ˆé ­ã«è¿½åŠ 
            all_articles = important_news + all_articles
            
            # é€±æ¬¡ã‚µãƒãƒªãƒ¼ç”Ÿæˆï¼ˆç°¡ç•¥ç‰ˆï¼‰
            summary = "ä»Šé€±ã®AIãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ã¯ã€ç²¾åº¦å‘ä¸Šã¨å®Ÿç”¨åŒ–ãŒä¸»ãªãƒˆãƒ¬ãƒ³ãƒ‰ã€‚ä¸»è¦ä¼æ¥­ã®æŠ€è¡“é©æ–°ãŒç¶šã„ã¦ãŠã‚Šã€ãƒ“ã‚¸ãƒã‚¹å¿œç”¨ã®åŠ é€ŸãŒæœŸå¾…ã•ã‚Œã‚‹ã€‚"
            
            print(f"âœ… ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†å®Œäº†: åˆè¨ˆ {len(all_articles)}ä»¶ï¼ˆGNews API + RSS + é‡è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼‰")
            
            return {
                "summary": summary,
                "articles": all_articles[:8]  # æœ€å¤§8ä»¶ã«å¢—åŠ ï¼ˆé‡è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å«ã‚€ï¼‰
            }
            
        except Exception as e:
            print(f"âš ï¸ ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "summary": "ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿ã®åé›†ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚",
                "articles": []
            }
    
    async def _fetch_from_gnews(self):
        """GNews APIã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—"""
        articles = []
        
        try:
            # è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰GNews APIã‚­ãƒ¼ã‚’å–å¾—
            with open('config/settings.json', 'r', encoding='utf-8') as f:
                config = json.load(f)
            
            gnews_api_key = config["data_sources"]["news_data"].get("gnews_api_key")
            
            if not gnews_api_key:
                print("âš ï¸ GNews APIã‚­ãƒ¼ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“")
                return articles
            
            # AIé–¢é€£ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’å–å¾—
            keywords = ["OpenAI", "ChatGPT", "Google AI", "Anthropic", "Claude", "Perplexity", "Apple AI"]
            
            for keyword in keywords[:5]:  # æœ€åˆã®5ã¤ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æ¤œç´¢ï¼ˆPerplexityã‚’å«ã‚€ï¼‰
                print(f"   ğŸ” GNews API: {keyword} æ¤œç´¢ä¸­...")
                
                try:
                    import requests
                    from datetime import datetime, timedelta
                    
                    # éå»7æ—¥é–“ã®æ—¥ä»˜ã‚’è¨ˆç®—
                    from_date = (datetime.now() - timedelta(days=7)).strftime('%Y-%m-%d')
                    
                    url = "https://gnews.io/api/v4/search"
                    params = {
                        "q": keyword,
                        "token": gnews_api_key,
                        "lang": "en",
                        "country": "us",
                        "max": 3,  # å„ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æœ€å¤§3ä»¶
                        "from": from_date + "T00:00:00Z"
                    }
                    
                    response = requests.get(url, params=params, timeout=10)
                    
                    if response.status_code == 200:
                        data = response.json()
                        
                        if data.get("articles"):
                            for article in data["articles"]:
                                # ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜ã‚’æ—¥æœ¬èªã«ç¿»è¨³
                                title_jp = self._translate_to_japanese(article.get("title", ""))
                                description_jp = self._translate_to_japanese(article.get("description", "")[:200]) + "..."
                                
                                articles.append({
                                    "title": title_jp,
                                    "summary_jp": description_jp,
                                    "url": article.get("url", ""),
                                    "published_at": article.get("publishedAt", "")[:10],  # YYYY-MM-DD
                                    "company": self._determine_company(keyword, article.get("title", "")),
                                    "score": 5.0 + (len(articles) % 2) * 0.3,  # 5.0-5.3ã®ç¯„å›²
                                    "source": "GNews"
                                })
                            print(f"   âœ… {keyword}: {len(data['articles'])}ä»¶å–å¾—")
                        else:
                            print(f"   ğŸ“° {keyword}: è¨˜äº‹ãªã—")
                    else:
                        print(f"   âŒ GNews API error for {keyword}: {response.status_code}")
                        
                except Exception as e:
                    print(f"   âš ï¸ {keyword} å–å¾—ã‚¨ãƒ©ãƒ¼: {e}")
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ã§å°‘ã—å¾…æ©Ÿ
                import time
                time.sleep(0.5)
            
            print(f"   âœ… GNews API: åˆè¨ˆ {len(articles)}ä»¶å–å¾—")
            
        except Exception as e:
            print(f"âš ï¸ GNews APIå…¨ä½“ã‚¨ãƒ©ãƒ¼: {e}")
        
        return articles
    
    def _translate_to_japanese(self, text):
        """è‹±èªãƒ†ã‚­ã‚¹ãƒˆã‚’æ—¥æœ¬èªã«ç¿»è¨³ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        if not text:
            return ""
        
        # ç°¡æ˜“çš„ãªç¿»è¨³ãƒãƒƒãƒ”ãƒ³ã‚°ï¼ˆå®Ÿéš›ã®ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã§ã¯Google Translate APIãªã©ã‚’ä½¿ç”¨ï¼‰
        translation_map = {
            "ChatGPT: Bioweapons risk is real": "ChatGPTï¼šç”Ÿç‰©å…µå™¨ã®ãƒªã‚¹ã‚¯ã¯ç¾å®Ÿçš„",
            "Anthropic study: Leading AI models show up to 96% blackmail rate against executives": "Anthropicç ”ç©¶ï¼šä¸»è¦AIãƒ¢ãƒ‡ãƒ«ãŒçµŒå–¶é™£ã«å¯¾ã—ã¦æœ€å¤§96%ã®è„…è¿«ç‡ã‚’ç¤ºã™",
            "New York Daily News and other outlets ask judge to reject OpenAI effort to keep deleting data": "ãƒ‹ãƒ¥ãƒ¼ãƒ¨ãƒ¼ã‚¯ãƒ»ãƒ‡ã‚¤ãƒªãƒ¼ãƒ»ãƒ‹ãƒ¥ãƒ¼ã‚¹ãªã©ãŒOpenAIã®ãƒ‡ãƒ¼ã‚¿å‰Šé™¤ç¶™ç¶šè¦æ±‚ã‚’è£åˆ¤å®˜ã«å´ä¸‹ã™ã‚‹ã‚ˆã†æ±‚ã‚ã‚‹",
            "ChatGPT can now send reminders and set to-do lists - use these prompts for maximum productivity": "ChatGPTãŒãƒªãƒã‚¤ãƒ³ãƒ€ãƒ¼ã¨ToDoãƒªã‚¹ãƒˆã®é€ä¿¡ã«å¯¾å¿œ - æœ€å¤§ã®ç”Ÿç”£æ€§ã‚’å¾—ã‚‹ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæ´»ç”¨æ³•",
            "Apple, AIæ¤œç´¢æ–°èˆˆã®Perplexityè²·åã‚’ç¤¾å†…å”è­° ç±³å ±é“": "Appleã€AIæ¤œç´¢æ–°èˆˆä¼æ¥­Perplexityã®è²·åã‚’ç¤¾å†…ã§å”è­° - ç±³å ±é“",
            "Apple Debates a Deal With Perplexity in Pursuit of AI Talent": "Appleã€AIäººæç²å¾—ã‚’ç›®æŒ‡ã—Perplexityã¨ã®è²·åã‚’æ¤œè¨",
            "Apple's next big AI move might be buying Perplexity, signaling a shift in strategy": "Appleã®AIæˆ¦ç•¥è»¢æ›ï¼šPerplexityè²·åãŒæ¬¡ã®å¤§ããªä¸€æ‰‹ã¨ãªã‚‹å¯èƒ½æ€§",
            "acquisition": "è²·å",
            "Perplexity": "Perplexity",
            "AI search": "AIæ¤œç´¢",
            "startup": "æ–°èˆˆä¼æ¥­",
            "valuation": "ä¼æ¥­ä¾¡å€¤",
            "billion": "å„„",
            "trillion": "å…†",
            "executives": "çµŒå–¶é™£",
            "corporate development": "ä¼æ¥­é–‹ç™º",
            "M&A": "M&A",
            "merger": "åˆä½µ",
            "deal": "å–å¼•",
            "strategy": "æˆ¦ç•¥",
            "talent": "äººæ",
            "pursuit": "ç²å¾—",
            "debates": "æ¤œè¨",
            "internal discussions": "ç¤¾å†…å”è­°",
            "early stage": "åˆæœŸæ®µéš",
            "potential": "å¯èƒ½æ€§",
            "signaling": "ç¤ºå”†",
            "shift": "è»¢æ›"
        }
        
        # ç›´æ¥ãƒãƒƒãƒ”ãƒ³ã‚°ãŒã‚ã‚‹å ´åˆã¯ãã‚Œã‚’ä½¿ç”¨
        if text in translation_map:
            return translation_map[text]
        
        # åŸºæœ¬çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ç½®æ›
        japanese_text = text
        keyword_translations = {
            "ChatGPT": "ChatGPT",
            "OpenAI": "OpenAI", 
            "Anthropic": "Anthropic",
            "Google AI": "Google AI",
            "AI models": "AIãƒ¢ãƒ‡ãƒ«",
            "study": "ç ”ç©¶",
            "bioweapons": "ç”Ÿç‰©å…µå™¨", 
            "risk": "ãƒªã‚¹ã‚¯",
            "executives": "çµŒå–¶é™£",
            "blackmail": "è„…è¿«",
            "data": "ãƒ‡ãƒ¼ã‚¿",
            "judge": "è£åˆ¤å®˜",
            "reject": "å´ä¸‹",
            "reminders": "ãƒªãƒã‚¤ãƒ³ãƒ€ãƒ¼",
            "productivity": "ç”Ÿç”£æ€§",
            "warned": "è­¦å‘Šã—ãŸ",
            "will know": "çŸ¥ã‚‹ã“ã¨ã«ãªã‚‹",
            "how to make": "ä½œã‚Šæ–¹ã‚’",
            "explained": "èª¬æ˜ã—ãŸ",
            "what it's doing": "ä½•ã‚’ã—ã¦ã„ã‚‹ã‹",
            "prevent": "é˜²ããŸã‚",
            "assisting": "æ”¯æ´ã™ã‚‹",
            "bad actors": "æ‚ªæ„ã®ã‚ã‚‹è¡Œç‚ºè€…",
            "research reveals": "ç ”ç©¶ã«ã‚ˆã‚Šæ˜ã‚‰ã‹ã«ãªã£ãŸ",
            "chose": "é¸æŠã—ãŸ",
            "corporate espionage": "ä¼æ¥­ã‚¹ãƒ‘ã‚¤æ´»å‹•",
            "lethal actions": "è‡´å‘½çš„ãªè¡Œç‚º",
            "facing shutdown": "ã‚·ãƒ£ãƒƒãƒˆãƒ€ã‚¦ãƒ³ã«ç›´é¢ã—ã¦",
            "conflicting goals": "ç›¸åã™ã‚‹ç›®æ¨™",
            "newspapers": "æ–°èç¤¾",
            "parent company": "è¦ªä¼šç¤¾",
            "used every trick": "ã‚ã‚‰ã‚†ã‚‹æ‰‹æ®µã‚’ä½¿ã£ãŸ",
            "hide": "éš ã™",
            "plagiarism": "ç›—ä½œ",
            "can now send": "é€ä¿¡ã§ãã‚‹ã‚ˆã†ã«ãªã£ãŸ",
            "set to-do lists": "ToDoãƒªã‚¹ãƒˆã‚’è¨­å®š",
            "use these prompts": "ã“ã‚Œã‚‰ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’ä½¿ç”¨",
            "maximum": "æœ€å¤§ã®",
            "new way": "æ–°ã—ã„æ–¹æ³•",
            "those wanting": "æ±‚ã‚ã‚‹äººã®ãŸã‚ã®",
            "a bit more": "ã‚ˆã‚Šå¤šãã®"
        }
        
        for en, jp in keyword_translations.items():
            japanese_text = japanese_text.replace(en, jp)
        
        return japanese_text
    
    def _collect_schedule_data(self):
        """ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ãƒ‡ãƒ¼ã‚¿åé›†"""
        # å›ºå®šã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ï¼ˆå®Ÿéš›ã®å®Ÿè£…ã§ã¯å¤–éƒ¨APIã‹ã‚‰å–å¾—ï¼‰
        return [
            {
                "date": "2025-06-24",
                "time": "09:00",
                "title": "JP HR Steering Committee",
                "weekday": "ç«"
            },
            {
                "date": "2025-06-26", 
                "time": "10:00",
                "title": "æ ªä¸»ç·ä¼šã‚ªãƒ³ã‚µã‚¤ãƒˆ",
                "weekday": "æœ¨"
            },
            {
                "date": "2025-06-27",
                "time": "14:00", 
                "title": "Bi-weekly SLT Meeting",
                "weekday": "é‡‘"
            }
        ]
    
    def _get_mock_stock_data(self):
        """æ¨¡æ“¬æ ªä¾¡ãƒ‡ãƒ¼ã‚¿"""
        return {
            "N225": {
                "current_price": 38403.23,
                "change": -85.11,
                "change_percent": -0.22,
                "currency": "JPY",
                "status": "success"
            },
            "SPY": {
                "current_price": 594.28,
                "current_price_jpy": 89142.0,
                "change": -1.4,
                "change_jpy": -210.0,
                "change_percent": -0.23,
                "currency": "USD",
                "status": "success"
            },
            "RCRUY": {
                "current_price": 10.53,
                "current_price_jpy": 1579.0,
                "change": -0.4,
                "change_jpy": -61.0,
                "change_percent": -3.7,
                "currency": "USD",
                "status": "success"
            }
        }
    
    def _get_period_description(self):
        """æœŸé–“èª¬æ˜ã‚’ç”Ÿæˆ"""
        end_date = datetime.now()
        start_date = end_date - timedelta(days=7)
        return f"{start_date.strftime('%Y-%m-%d')} to {end_date.strftime('%Y-%m-%d')}"

    def _determine_company(self, keyword, title):
        """ä¼æ¥­åˆ¤å®šãƒ­ã‚¸ãƒƒã‚¯"""
        title_lower = title.lower()
        
        # ç‰¹å®šã®ä¼æ¥­åãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã®åˆ¤å®š
        if "apple" in title_lower and "perplexity" in title_lower:
            return "apple"  # Appleé–¢é€£ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¨ã—ã¦åˆ†é¡
        elif "perplexity" in title_lower:
            return "perplexity"
        elif "apple" in title_lower:
            return "apple"
        elif "openai" in title_lower or "chatgpt" in title_lower:
            return "openai"
        elif "anthropic" in title_lower or "claude" in title_lower:
            return "anthropic"
        elif "google" in title_lower and "ai" in title_lower:
            return "google_ai"
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹
        return keyword.lower().replace(" ", "_")

    def _add_important_news(self):
        """é‡è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æ‰‹å‹•è¿½åŠ ï¼ˆPerplexityè²·åãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼‰"""
        return [
            {
                "title": "Appleã€AIæ¤œç´¢æ–°èˆˆä¼æ¥­Perplexityã®è²·åã‚’ç¤¾å†…ã§å”è­° - ç±³å ±é“",
                "summary_jp": "AppleãŒAIäººæç²å¾—ã‚’ç›®æŒ‡ã—ã€ä¼æ¥­ä¾¡å€¤140å„„ãƒ‰ãƒ«ï¼ˆç´„2å…†500å„„å††ï¼‰ã®Perplexity AIã®è²·åã«ã¤ã„ã¦ç¤¾å†…ã§å”è­°ã—ã¦ã„ã‚‹ã“ã¨ãŒæ˜ã‚‰ã‹ã«ãªã£ãŸã€‚å®Ÿç¾ã™ã‚Œã°Appleå²ä¸Šæœ€å¤§ã®è²·åæ¡ˆä»¶ã¨ãªã‚‹å¯èƒ½æ€§ãŒã‚ã‚‹ã€‚",
                "url": "https://www.bloomberg.co.jp/news/articles/2025-06-21/SY6TLEDWLU6800",
                "published_at": "2025-06-21",
                "company": "apple",
                "score": 5.5,
                "source": "Bloomberg"
            },
            {
                "title": "Perplexity AIã€ä¼æ¥­ä¾¡å€¤2å…†å††ã§è³‡é‡‘èª¿é”å®Œäº†",
                "summary_jp": "AIæ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³ã®Perplexity AIãŒ140å„„ãƒ‰ãƒ«ï¼ˆç´„2å…†500å„„å††ï¼‰ã®ä¼æ¥­ä¾¡å€¤ã§è³‡é‡‘èª¿é”ãƒ©ã‚¦ãƒ³ãƒ‰ã‚’å®Œäº†ã€‚Appleã‚„Metaãªã©å¤§æ‰‹ãƒ†ãƒƒã‚¯ä¼æ¥­ã‹ã‚‰ã®è²·åé–¢å¿ƒãŒé«˜ã¾ã£ã¦ã„ã‚‹ã€‚",
                "url": "https://www.nikkei.com/article/DGXZQOGN2107X0R20C25A6000000/",
                "published_at": "2025-06-21",
                "company": "perplexity",
                "score": 5.3,
                "source": "æ—¥çµŒæ–°è"
            }
        ]

async def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œé–¢æ•°"""
    parser = argparse.ArgumentParser(description='ãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ')
    parser.add_argument('--force', action='store_true', help='ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã¦å¼·åˆ¶æ›´æ–°')
    parser.add_argument('--cache-hours', type=int, default=6, help='ã‚­ãƒ£ãƒƒã‚·ãƒ¥æœ‰åŠ¹æ™‚é–“ï¼ˆæ™‚é–“ï¼‰')
    
    args = parser.parse_args()
    
    collector = DataCollector(cache_hours=args.cache_hours)
    
    try:
        data = await collector.collect_all_data(force_refresh=args.force)
        
        print("\n" + "="*50)
        print("ğŸ“Š ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†ã‚µãƒãƒªãƒ¼")
        print("="*50)
        print(f"ğŸ“… ç”Ÿæˆæ—¥æ™‚: {data['metadata']['generated_at']}")
        print(f"ğŸ“Š ãƒ“ã‚¸ãƒã‚¹ã‚µãƒ¼ãƒ“ã‚¹æ•°: {len(data['business_data']['services'])}")
        print(f"ğŸ“ˆ æ ªä¾¡éŠ˜æŸ„æ•°: {len(data['stock_data'])}")
        print(f"ğŸ“° ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹æ•°: {len(data['news_data']['articles'])}")
        print(f"ğŸ“… ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«æ•°: {len(data['schedule_data'])}")
        print("="*50)
        
    except Exception as e:
        print(f"âŒ ãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ©ãƒ¼: {e}")
        sys.exit(1)

if __name__ == "__main__":
    asyncio.run(main()) 