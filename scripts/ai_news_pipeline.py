#!/usr/bin/env python3
"""
AIé§†å‹•ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³çµ±åˆã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ä¼æ¥­ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›† â†’ AIåˆ†æ â†’ æ—¥æœ¬èªè¦ç´„ â†’ ãƒ¬ãƒãƒ¼ãƒˆç”Ÿæˆ
"""

import os
import sys
import json
import argparse
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Any
from pathlib import Path

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’è¿½åŠ 
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from company_news_collector import CompanyNewsCollector
from news_analyzer import analyze_company_news, NewsAnalyzer, EfficientNewsAnalyzer

class AINewsPipeline:
    def __init__(self, config_path="config/target_companies.yaml"):
        """
        ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³åˆæœŸåŒ–
        
        Args:
            config_path: ä¼æ¥­è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        self.config_path = config_path
        self.collector = CompanyNewsCollector(config_path)
        self.analyzer = EfficientNewsAnalyzer()
        self.output_dir = Path("reports")
        self.output_dir.mkdir(exist_ok=True)
        
    async def run_pipeline(self, top_n: int = 10) -> dict:
        """ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        try:
            print("ğŸš€ AIé§†å‹•ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³é–‹å§‹")
            print("=" * 50)
            
            # 1. ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†
            print("ğŸ“° ä¼æ¥­åˆ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ä¸­...")
            raw_news = self.collector.collect_all_company_news(days_back=7)
            print(f"âœ… åé›†å®Œäº†: {len(raw_news)}ä»¶")
            
            if not raw_news:
                print("âŒ ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒåé›†ã§ãã¾ã›ã‚“ã§ã—ãŸ")
                return {"error": "No news collected"}
            
            # 2. AIåˆ†æãƒ»é‡è¦åº¦åˆ¤å®š
            print("\nğŸ¤– AIåˆ†æãƒ»é‡è¦åº¦åˆ¤å®šä¸­...")
            analyzed_news = await self.analyzer.analyze_news_efficient(raw_news)
            
            # 3. ä¸Šä½ãƒ‹ãƒ¥ãƒ¼ã‚¹é¸å‡º
            top_news = sorted(analyzed_news, key=lambda x: x.get('score', 0), reverse=True)[:top_n]
            
            # 4. å¯¾è±¡æœŸé–“æƒ…å ±å–å¾—
            period_info = self.analyzer.get_analysis_period()
            
            # 5. é€±æ¬¡ã‚µãƒãƒªãƒ¼ç”Ÿæˆ
            print("\nğŸ“ é€±æ¬¡ã‚µãƒãƒªãƒ¼ç”Ÿæˆä¸­...")
            weekly_summary = await self.analyzer.generate_weekly_summary(top_news)
            
            # 6. çµ±è¨ˆæƒ…å ±è¨ˆç®—
            stats = self.calculate_stats(analyzed_news, top_news)
            
            # 7. çµæœæ§‹é€ åŒ–
            result = {
                "generated_at": datetime.now().isoformat(),
                "period_info": period_info,
                "weekly_summary": weekly_summary,
                "total_items": len(analyzed_news),
                "top_items": len(top_news),
                "statistics": stats,
                "analysis_results": top_news
            }
            
            # 8. ãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
            await self.save_results(result, timestamp)
            
            # 9. çµæœè¡¨ç¤º
            self.display_results(result)
            
            return result
            
        except Exception as e:
            print(f"âŒ ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return {"error": str(e)}

    def calculate_stats(self, all_news: list, top_news: list) -> dict:
        """çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—"""
        if not all_news:
            return {}
        
        # ä¼æ¥­åˆ¥çµ±è¨ˆ
        company_stats = {}
        source_stats = {}
        
        for news in all_news:
            company = news.get('company_id', 'unknown')
            source = news.get('source_type', 'unknown')
            
            company_stats[company] = company_stats.get(company, 0) + 1
            source_stats[source] = source_stats.get(source, 0) + 1
        
        # ã‚¹ã‚³ã‚¢çµ±è¨ˆ
        scores = [news.get('score', 0) for news in all_news if news.get('score')]
        avg_score = sum(scores) / len(scores) if scores else 0
        top_avg_score = sum(news.get('score', 0) for news in top_news) / len(top_news) if top_news else 0
        
        return {
            "selection_rate": f"{(len(top_news)/len(all_news)*100):.1f}%" if all_news else "0%",
            "average_score": round(avg_score, 2),
            "top_average_score": round(top_avg_score, 2),
            "company_distribution": dict(sorted(company_stats.items(), key=lambda x: x[1], reverse=True)),
            "source_distribution": dict(sorted(source_stats.items(), key=lambda x: x[1], reverse=True))
        }

    async def save_results(self, result: dict, timestamp: str):
        """çµæœã‚’ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        # JSONå½¢å¼ã§ä¿å­˜
        json_file = self.output_dir / f"ai_news_analysis_{timestamp}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(result, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ JSONä¿å­˜: {json_file}")
        
        # Markdownå½¢å¼ã§ä¿å­˜
        md_file = self.output_dir / f"ai_news_report_{timestamp}.md"
        markdown_content = self.generate_markdown_report(result)
        with open(md_file, 'w', encoding='utf-8') as f:
            f.write(markdown_content)
        print(f"ğŸ“„ Markdownä¿å­˜: {md_file}")
        
        # Webç”¨JSONæ›´æ–°
        web_json = Path("web/news-data.json")
        web_data = []
        for news in result.get('analysis_results', []):
            web_data.append({
                "title": news.get('title', ''),
                "description": news.get('summary_jp', ''),
                "url": news.get('url', ''),
                "company": news.get('company_id', 'unknown'),
                "publishedAt": news.get('published_at', ''),
                "score": news.get('score', 0)
            })
        
        with open(web_json, 'w', encoding='utf-8') as f:
            json.dump(web_data, f, ensure_ascii=False, indent=2)
        print(f"ğŸŒ Webç”¨JSONæ›´æ–°: {web_json}")

    def generate_markdown_report(self, result: dict) -> str:
        """Markdownãƒ¬ãƒãƒ¼ãƒˆã‚’ç”Ÿæˆ"""
        period_info = result.get('period_info', {})
        content = f"""# AIé§†å‹•ãƒ‹ãƒ¥ãƒ¼ã‚¹åˆ†æãƒ¬ãƒãƒ¼ãƒˆ

## åŸºæœ¬æƒ…å ±
- **ç”Ÿæˆæ—¥æ™‚**: {result.get('generated_at', '')}
- **åˆ†æå¯¾è±¡æœŸé–“**: {period_info.get('period_description', 'éå»1é€±é–“')}
- **åé›†ä»¶æ•°**: {result.get('total_items', 0)}ä»¶
- **é¸å‡ºä»¶æ•°**: {result.get('top_items', 0)}ä»¶

## ä»Šé€±ã®ã‚µãƒãƒªãƒ¼

{result.get('weekly_summary', 'ä»Šé€±ã®ã‚µãƒãƒªãƒ¼ã¯ç”Ÿæˆã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚')}

## çµ±è¨ˆæƒ…å ±

### å…¨ä½“çµ±è¨ˆ
- **é¸å‡ºç‡**: {result.get('statistics', {}).get('selection_rate', 'N/A')}
- **å¹³å‡é‡è¦åº¦**: {result.get('statistics', {}).get('average_score', 'N/A')}/10
- **ä¸Šä½å¹³å‡é‡è¦åº¦**: {result.get('statistics', {}).get('top_average_score', 'N/A')}/10

### ä¼æ¥­åˆ¥åˆ†å¸ƒ
"""
        
        # ä¼æ¥­åˆ¥çµ±è¨ˆ
        company_dist = result.get('statistics', {}).get('company_distribution', {})
        for company, count in company_dist.items():
            content += f"- **{company}**: {count}ä»¶\n"
        
        content += "\n### æƒ…å ±æºåˆ¥åˆ†å¸ƒ\n"
        source_dist = result.get('statistics', {}).get('source_distribution', {})
        for source, count in source_dist.items():
            content += f"- **{source}**: {count}ä»¶\n"
        
        content += "\n## é‡è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹\n\n"
        
        # ãƒˆãƒƒãƒ—ãƒ‹ãƒ¥ãƒ¼ã‚¹
        for i, news in enumerate(result.get('analysis_results', []), 1):
            content += f"### {i}. {news.get('title', '')}\n\n"
            content += f"- **é‡è¦åº¦**: {news.get('score', 0):.1f}/10\n"
            content += f"- **ä¼æ¥­**: {news.get('company_id', 'unknown')}\n"
            content += f"- **æƒ…å ±æº**: {news.get('source_type', 'unknown')}\n"
            content += f"- **URL**: {news.get('url', '')}\n"
            content += f"- **è¦ç´„**: {news.get('summary_jp', '')}\n\n"
            if news.get('summary'):
                content += f"**è©³ç´°**: {news.get('summary', '')}\n\n"
            content += "---\n\n"
        
        return content

    def display_results(self, result: dict):
        """çµæœã‚’è¡¨ç¤º"""
        print("\n" + "=" * 50)
        print("ğŸ“Š AIåˆ†æçµæœã‚µãƒãƒªãƒ¼")
        print("=" * 50)
        
        period_info = result.get('period_info', {})
        print(f"ğŸ“… åˆ†ææœŸé–“: {period_info.get('period_description', 'éå»1é€±é–“')}")
        print(f"ğŸ“° ç·åé›†æ•°: {result.get('total_items', 0)}ä»¶")
        print(f"â­ é¸å‡ºæ•°: {result.get('top_items', 0)}ä»¶")
        
        stats = result.get('statistics', {})
        print(f"ğŸ“ˆ é¸å‡ºç‡: {stats.get('selection_rate', 'N/A')}")
        print(f"ğŸ¯ å¹³å‡é‡è¦åº¦: {stats.get('top_average_score', 'N/A')}/10")
        
        print(f"\nğŸ“ ä»Šé€±ã®ã‚µãƒãƒªãƒ¼:")
        print(f"{result.get('weekly_summary', '')}")
        
        print(f"\nğŸ† ãƒˆãƒƒãƒ—{len(result.get('analysis_results', []))}ãƒ‹ãƒ¥ãƒ¼ã‚¹:")
        for i, news in enumerate(result.get('analysis_results', []), 1):
            print(f"{i}. {news.get('title', '')} (é‡è¦åº¦: {news.get('score', 0):.1f})")
            print(f"   ä¼æ¥­: {news.get('company_id', '')} | è¦ç´„: {news.get('summary_jp', '')}")
        
        print("\nâœ… ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Œäº†")

def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(description="AIé§†å‹•ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³")
    parser.add_argument("--top", type=int, default=10, help="ä¸Šä½ä½•ä»¶ã‚’é¸å‡ºã™ã‚‹ã‹")
    parser.add_argument("--config", type=str, default="config/target_companies.yaml", help="è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹")
    
    args = parser.parse_args()
    
    # ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³å®Ÿè¡Œ
    pipeline = AINewsPipeline(config_path=args.config)
    result = asyncio.run(pipeline.run_pipeline(top_n=args.top))
    
    # çµ‚äº†ã‚³ãƒ¼ãƒ‰
    exit_code = 0 if result["status"] == "success" else 1
    sys.exit(exit_code)

if __name__ == "__main__":
    main() 