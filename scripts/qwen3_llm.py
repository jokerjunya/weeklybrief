import requests
import json
import re

class Qwen3Llm:
    """
    Qwen3 LLMãƒ©ãƒƒãƒ‘ãƒ¼ã‚¯ãƒ©ã‚¹
    
    ğŸ”§ Thinkingæ©Ÿèƒ½: OFFã«è¨­å®šæ¸ˆã¿
    - Qwen3ã®thinkingæ©Ÿèƒ½ã¯"think": Falseãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ç„¡åŠ¹åŒ–
    - ç”Ÿæˆé€Ÿåº¦ã®å‘ä¸Šã¨ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã®ç°¡æ½”åŒ–ã‚’å®Ÿç¾
    - <think>ã‚¿ã‚°ãŒå«ã¾ã‚Œã‚‹å ´åˆã¯æ­£è¦è¡¨ç¾ã§é™¤å»
    """
    def __init__(self, model="ollama/qwen3:30b-a3b", api_url="http://localhost:11434/api/generate"):
        self.model = model
        self.api_url = api_url

    async def generate_content_async(self, prompt, agent_name=None, show_progress=True, progress_callback=None, **kwargs):
        import asyncio
        
        def sync_request():
            # æ—¥æœ¬èªå¿œç­”ã‚’å¼·åˆ¶ã™ã‚‹ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆæŒ‡ç¤ºã‚’è¿½åŠ 
            enhanced_prompt = f"{prompt}\n\nâ€»å¿…ãšæ—¥æœ¬èªã§å›ç­”ã—ã¦ãã ã•ã„ã€‚è‹±èªã‚„ä¸­å›½èªã¯ä½¿ç”¨ã—ãªã„ã§ãã ã•ã„ã€‚"
            
            # é€²æ—è¡¨ç¤ºé–‹å§‹
            if show_progress and agent_name:
                print(f"ğŸ¤– {agent_name}ãŒå›ç­”ã‚’ç”Ÿæˆä¸­", end="", flush=True)
                if progress_callback:
                    progress_callback(f"ğŸ¤– {agent_name}ãŒå›ç­”ã‚’ç”Ÿæˆä¸­")
            elif show_progress:
                print("ğŸ¤– AIå›ç­”ã‚’ç”Ÿæˆä¸­", end="", flush=True)
                if progress_callback:
                    progress_callback("ğŸ¤– AIå›ç­”ã‚’ç”Ÿæˆä¸­")
            
            response = requests.post(
                self.api_url,
                json={
                    "model": self.model.split("/")[-1],
                    "prompt": enhanced_prompt,
                    "stream": True,
                    "think": False,
                    **kwargs
                },
                stream=True
            )
            
            full_response = ""
            dot_count = 0
            
            try:
                for line in response.iter_lines():
                    if line:
                        try:
                            chunk = json.loads(line.decode('utf-8'))
                            if 'response' in chunk:
                                full_response += chunk['response']
                                
                                # é€²æ—è¡¨ç¤ºï¼ˆãƒ‰ãƒƒãƒˆè¿½åŠ ï¼‰
                                if show_progress:
                                    dot_count += 1
                                    if dot_count % 10 == 0:  # 10ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«ãƒ‰ãƒƒãƒˆã‚’è¡¨ç¤º
                                        print(".", end="", flush=True)
                                
                                if chunk.get('done', False):
                                    break
                        except json.JSONDecodeError:
                            continue
            finally:
                # å®Œäº†ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
                if show_progress:
                    print(" âœ…å®Œäº†", flush=True)
                    if progress_callback:
                        progress_callback("")  # é€²æ—è¡¨ç¤ºã‚’ã‚¯ãƒªã‚¢
            
            # thinkingéƒ¨åˆ†ã‚’é™¤å»
            full_response = re.sub(r'<think>.*?</think>', '', full_response, flags=re.DOTALL)
            
            # è¿½åŠ ã®ä¸è¦ãªè‹±èªãƒ»ä¸­å›½èªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’é™¤å»
            full_response = re.sub(r'Okay.*?\.', '', full_response, flags=re.DOTALL)
            full_response = re.sub(r'å¥½çš„.*?ã€‚', '', full_response, flags=re.DOTALL)
            full_response = re.sub(r'Wait.*?\.', '', full_response, flags=re.DOTALL)
            full_response = re.sub(r'Let me.*?\.', '', full_response, flags=re.DOTALL)
            
            return full_response.strip()
        
        # éåŒæœŸå®Ÿè¡Œ
        return await asyncio.get_event_loop().run_in_executor(None, sync_request) 