#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼ˆQwen3ï¼‰ãƒ‹ãƒ¥ãƒ¼ã‚¹è¦ç´„æ©Ÿèƒ½

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯Ollamaã‚’ä½¿ç”¨ã—ã¦Qwen3ãƒ¢ãƒ‡ãƒ«ã§
è‹±èªã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’æ—¥æœ¬èªã«è¦ç´„ã—ã¾ã™ã€‚
"""

import json
import requests
import time
from typing import Dict, List, Optional, Any
from datetime import datetime


class LocalLLMSummarizer:
    """
    ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼ˆQwen3ï¼‰ã‚’ä½¿ç”¨ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹è¦ç´„æ©Ÿèƒ½
    """
    
    def __init__(self, config_path: str = "config/settings.json"):
        """
        åˆæœŸåŒ–
        
        Args:
            config_path (str): è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        
        self.llm_config = self.config["data_sources"].get("local_llm", {})
        self.enabled = self.llm_config.get("enabled", False)
        self.ollama_url = self.llm_config.get("ollama_url", "http://localhost:11434")
        self.model_name = self.llm_config.get("model_name", "qwen3:8b")
        self.thinking_mode = self.llm_config.get("thinking_mode", False)
        
        # æ¥ç¶šãƒ†ã‚¹ãƒˆ
        self.available = self._test_ollama_connection()
    
    def _test_ollama_connection(self) -> bool:
        """
        Ollamaã‚µãƒ¼ãƒãƒ¼ã¨ã®æ¥ç¶šã‚’ãƒ†ã‚¹ãƒˆ
        
        Returns:
            bool: æ¥ç¶šæˆåŠŸã®å ´åˆTrue
        """
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                # æŒ‡å®šã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ã‹ãƒã‚§ãƒƒã‚¯
                model_names = [model['name'] for model in models]
                return any(self.model_name in name for name in model_names)
            return False
        except Exception as e:
            print(f"Ollamaæ¥ç¶šãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def generate_summary_japanese(self, title: str, description: str, url: str = "", content: str = "") -> Optional[str]:
        """
        è‹±èªã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‹ã‚‰æ—¥æœ¬èªè¦ç´„ã‚’ç”Ÿæˆ
        
        Args:
            title (str): è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆè‹±èªï¼‰
            description (str): è¨˜äº‹èª¬æ˜ï¼ˆè‹±èªï¼‰
            url (str): è¨˜äº‹URLï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            content (str): è¨˜äº‹æœ¬æ–‡ï¼ˆè‹±èªã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
        Returns:
            Optional[str]: æ—¥æœ¬èªè¦ç´„ï¼ˆå¤±æ•—æ™‚ã¯Noneï¼‰
        """
        if not self.enabled or not self.available:
            return self._improved_fallback_summary(title, description, content)
        
        try:
            return self._summarize_with_qwen3(title, description, content)
        except Exception as e:
            print(f"Qwen3è¦ç´„ã‚¨ãƒ©ãƒ¼: {e}")
            return self._improved_fallback_summary(title, description, content)
    
    def _summarize_with_qwen3(self, title: str, description: str, content: str = "") -> Optional[str]:
        """
        Qwen3ã‚’ä½¿ç”¨ã—ã¦æ—¥æœ¬èªè¦ç´„ã‚’ç”Ÿæˆ
        
        Args:
            title (str): è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
            description (str): è¨˜äº‹èª¬æ˜
            content (str): è¨˜äº‹æœ¬æ–‡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
        Returns:
            Optional[str]: æ—¥æœ¬èªè¦ç´„
        """
        # ã‚ˆã‚Šå¤šãã®æƒ…å ±ã‚’æ´»ç”¨ã—ã¦è¦ç´„ã®è³ªã‚’å‘ä¸Š
        full_text = f"{title}. {description or ''} {content or ''}".strip()
        
        # éthinking modeã‚’å¼·åˆ¶ã—ã€ç›´æ¥çš„ãªæ—¥æœ¬èªè¦ç´„ã‚’ç”Ÿæˆ
        # Qwen3ã®chat templateã‚’ä½¿ç”¨ã—ã¦thinking modeã‚’å›é¿
        prompt = f"""<|user|>
ä»¥ä¸‹ã®è‹±èªã®AIæ¥­ç•Œãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’èª­ã‚“ã§ã€é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’æ—¥æœ¬èªã§50æ–‡å­—ä»¥å†…ã§è¦ç´„ã—ã¦ãã ã•ã„ã€‚

è¨˜äº‹å†…å®¹: {full_text[:500]}

å…·ä½“çš„ãªäº‹å®Ÿã¨çµè«–ãŒåˆ†ã‹ã‚‹ã‚ˆã†ã«æ—¥æœ¬èªè¦ç´„ã‚’ç°¡æ½”ã«ä½œæˆã—ã¦ãã ã•ã„ã€‚
<|assistant|>
"""
        
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.3,
                "top_p": 0.8,
                "top_k": 30,
                "num_predict": 100,
                "stop": ["<|user|>", "<|assistant|>", "\n\n"],
                "repeat_penalty": 1.1,
                "seed": 42  # ä¸€è²«æ€§ã®ãŸã‚å›ºå®šã‚·ãƒ¼ãƒ‰
            }
        }
        
        try:
            print(f"ğŸ¤– Qwen3ã§è¦ç´„ç”Ÿæˆä¸­: {title[:30]}...")
            
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                timeout=40
            )
            
            if response.status_code == 200:
                result = response.json()
                raw_summary = result.get('response', '').strip()
                
                print(f"ğŸ“ ç”Ÿæˆã•ã‚ŒãŸè¦ç´„ï¼ˆç”Ÿï¼‰: {raw_summary}")
                
                # thinkingã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®å®Œå…¨é™¤å»
                if '<think>' in raw_summary:
                    if '</think>' in raw_summary:
                        # thinkingéƒ¨åˆ†å¾Œã®å†…å®¹ã‚’å–å¾—
                        parts = raw_summary.split('</think>')
                        raw_summary = parts[-1].strip()
                    else:
                        # thinkingé€”ä¸­ã®å ´åˆã¯ã‚ˆã‚Šå¼·åˆ¶çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒ
                        print("âš ï¸ Thinking mode detected, trying direct approach...")
                        return self._direct_translation_approach(title, description)
                
                # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                summary = self._clean_summary(raw_summary)
                
                # å“è³ªãƒã‚§ãƒƒã‚¯
                if self._is_valid_summary(summary, title, description):
                    print(f"âœ… æœ€çµ‚è¦ç´„: {summary}")
                    return summary
                else:
                    print("âš ï¸ Summary quality check failed, trying direct approach...")
                    return self._direct_translation_approach(title, description, content)
                    
            else:
                print(f"âŒ Ollama APIã‚¨ãƒ©ãƒ¼: {response.status_code}")
                return self._direct_translation_approach(title, description, content)
                
        except Exception as e:
            print(f"âŒ Qwen3 APIå‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
            return self._direct_translation_approach(title, description, content)
    
    def _direct_translation_approach(self, title: str, description: str, content: str = "") -> Optional[str]:
        """
        ã‚ˆã‚Šç›´æ¥çš„ãªã‚¢ãƒ—ãƒ­ãƒ¼ãƒã§ç¿»è¨³ãƒ»è¦ç´„
        
        Args:
            title (str): è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
            description (str): è¨˜äº‹èª¬æ˜
            content (str): è¨˜äº‹æœ¬æ–‡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
        Returns:
            Optional[str]: æ—¥æœ¬èªè¦ç´„
        """
        # ã‚ˆã‚Šå¤šãã®æƒ…å ±ã‚’æ´»ç”¨
        text_to_summarize = f"{title}. {description or ''} {content or ''}".strip()
        
        prompt = f"""Translate to Japanese and summarize in 50 characters:

{text_to_summarize}

Japanese:"""
        
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.1,
                "num_predict": 80,
                "stop": ["\n", "English:", "Translate:"],
                "repeat_penalty": 1.2
            }
        }
        
        try:
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                timeout=25
            )
            
            if response.status_code == 200:
                result = response.json()
                summary = result.get('response', '').strip()
                
                # thinking contentãŒã‚ã‚‹å ´åˆã¯ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                if '<think>' in summary:
                    print("âš ï¸ Still in thinking mode, using improved fallback...")
                    return self._improved_fallback_summary(title, description)
                
                summary = self._clean_summary(summary)
                
                if self._is_valid_summary(summary, title, description):
                    return summary
                else:
                    return self._improved_fallback_summary(title, description)
            else:
                return self._improved_fallback_summary(title, description)
                
        except Exception:
            return self._improved_fallback_summary(title, description)
    
    def _is_valid_summary(self, summary: str, title: str, description: str) -> bool:
        """
        è¦ç´„ã®å“è³ªã‚’ãƒã‚§ãƒƒã‚¯
        
        Args:
            summary (str): ç”Ÿæˆã•ã‚ŒãŸè¦ç´„
            title (str): å…ƒã®ã‚¿ã‚¤ãƒˆãƒ«
            description (str): å…ƒã®èª¬æ˜
        
        Returns:
            bool: æœ‰åŠ¹ãªè¦ç´„ã‹ã©ã†ã‹
        """
        if not summary or len(summary) < 10:
            return False
        
        # æ—¥æœ¬èªãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        import re
        has_japanese = bool(re.search(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', summary))
        
        # å…ƒã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’ãã®ã¾ã¾ä½¿ã£ã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯
        title_words = title.lower().split()[:3]  # æœ€åˆã®3å˜èª
        summary_lower = summary.lower()
        
        similarity_count = sum(1 for word in title_words if word in summary_lower)
        too_similar = len(title_words) > 0 and similarity_count >= len(title_words)
        
        return has_japanese and not too_similar
    
    def _improved_fallback_summary(self, title: str, description: str, content: str = "") -> str:
        """
        æ”¹å–„ã•ã‚ŒãŸãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¦ç´„
        å…·ä½“çš„ãªäº‹å®Ÿã¨çµè«–ãŒåˆ†ã‹ã‚‹è¦ç´„ã‚’ä½œæˆ
        
        Args:
            title (str): è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
            description (str): è¨˜äº‹èª¬æ˜
            content (str): è¨˜äº‹æœ¬æ–‡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
        Returns:
            str: æ—¥æœ¬èªè¦ç´„
        """
        # å…¨æ–‡ã‚’çµåˆã—ã¦ã‚ˆã‚Šå¤šãã®æƒ…å ±ã‚’æ´»ç”¨
        full_content = f"{title}. {description or ''} {content or ''}".strip()
        
        # å…·ä½“çš„ãªäº‹å®Ÿã‚’æŠ½å‡ºã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        summary = self._extract_key_facts(full_content)
        
        # 50æ–‡å­—åˆ¶é™
        if len(summary) > 50:
            summary = summary[:47] + "..."
        
        return summary
    
    def _extract_key_facts(self, content: str) -> str:
        """
        ãƒ‹ãƒ¥ãƒ¼ã‚¹å†…å®¹ã‹ã‚‰å…·ä½“çš„ãªäº‹å®Ÿã¨çµè«–ã‚’æŠ½å‡º
        
        Args:
            content (str): ãƒ‹ãƒ¥ãƒ¼ã‚¹å…¨æ–‡
        
        Returns:
            str: å…·ä½“çš„ãªäº‹å®Ÿã‚’å«ã‚€æ—¥æœ¬èªè¦ç´„
        """
        content_lower = content.lower()
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ä¼šç¤¾é–“ã®å‹•ãï¼ˆè²·åã€ææºã€ç«¶äº‰ç­‰ï¼‰
        if "meta" in content_lower and "openai" in content_lower:
            if "bonus" in content_lower or "million" in content_lower:
                return "Metaã€OpenAIç¤¾å“¡ã«å·¨é¡ãƒœãƒ¼ãƒŠã‚¹æç¤ºã§å¼•ãæŠœã"
            elif "compete" in content_lower or "competition" in content_lower:
                return "Metaã¨OpenAIã€AIäººæã‚’å·¡ã‚Šç«¶äº‰æ¿€åŒ–"
            else:
                return "Metaã¨OpenAIé–“ã§æ–°ãŸãªå‹•ã"
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: æ–°è£½å“ãƒ»æ©Ÿèƒ½ç™ºè¡¨
        if any(word in content_lower for word in ["announce", "launch", "release", "unveil"]):
            # OpenAIé–¢é€£
            if "openai" in content_lower:
                if "gpt" in content_lower and ("5" in content or "new" in content_lower):
                    return "OpenAIã€æ–°å‹GPTãƒ¢ãƒ‡ãƒ«ã‚’ç™ºè¡¨"
                elif "pentagon" in content_lower or "defense" in content_lower:
                    return "OpenAIã€ç±³å›½é˜²ç·çœã¨å¥‘ç´„ç· çµ"
                else:
                    return "OpenAIã€æ–°ã‚µãƒ¼ãƒ“ã‚¹ãƒ»æ©Ÿèƒ½ã‚’ç™ºè¡¨"
            
            # Google/Geminié–¢é€£
            elif "gemini" in content_lower or "google" in content_lower:
                if "video" in content_lower:
                    return "Google Geminiã€å‹•ç”»åˆ†ææ©Ÿèƒ½ã‚’è¿½åŠ "
                elif "update" in content_lower:
                    return "Google Geminiã€æ©Ÿèƒ½å¼·åŒ–ç‰ˆã‚’ãƒªãƒªãƒ¼ã‚¹"
                else:
                    return "Googleã€AIæ–°æ©Ÿèƒ½ã‚’ç™ºè¡¨"
            
            # ãã®ä»–ä¼æ¥­
            elif "microsoft" in content_lower:
                return "Microsoftã€AIé–¢é€£æ–°è£½å“ã‚’ç™ºè¡¨"
            elif "amazon" in content_lower:
                return "Amazonã€AIæˆ¦ç•¥ã‚’ç™ºè¡¨"
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: æŠ•è³‡ãƒ»è³‡é‡‘èª¿é”
        if any(word in content_lower for word in ["funding", "investment", "raise", "million", "billion"]):
            if "startup" in content_lower:
                return "AIé–¢é€£ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ã€å¤§å‹è³‡é‡‘èª¿é”"
            else:
                return "AIæ¥­ç•Œã§å¤§å‹æŠ•è³‡æ¡ˆä»¶ãŒç™ºç”Ÿ"
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³4: æŠ€è¡“é©æ–°ãƒ»ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼
        if any(word in content_lower for word in ["breakthrough", "advancement", "improve", "enhance"]):
            if "reasoning" in content_lower or "logic" in content_lower:
                return "AIæ¨è«–èƒ½åŠ›ãŒå¤§å¹…å‘ä¸Šã€æ–°æŠ€è¡“ã‚’é–‹ç™º"
            elif "performance" in content_lower:
                return "AIæ€§èƒ½å‘ä¸Šã€å‡¦ç†é€Ÿåº¦ãŒæ”¹å–„"
            else:
                return "AIæŠ€è¡“ã§æ–°ãŸãªãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼"
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³5: äººäº‹ãƒ»çµ„ç¹”å¤‰æ›´
        if any(word in content_lower for word in ["hire", "employee", "ceo", "executive"]):
            return "AIä¼æ¥­ã§é‡è¦äººäº‹ã€çµ„ç¹”ä½“åˆ¶ã‚’å¤‰æ›´"
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³6: è¦åˆ¶ãƒ»æ”¿ç­–
        if any(word in content_lower for word in ["regulation", "policy", "government", "law"]):
            return "AIè¦åˆ¶ãƒ»æ”¿ç­–ã«é–¢ã™ã‚‹é‡è¦å‹•å‘"
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³7: ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚·ãƒƒãƒ—ãƒ»ææº
        if any(word in content_lower for word in ["partnership", "collaborate", "team up"]):
            companies = self._extract_companies(content)
            if len(companies) >= 2:
                return f"{companies[0]}ã¨{companies[1]}ãŒææº"
            else:
                return "AIæ¥­ç•Œã§æ–°ãŸãªææºãŒç™ºè¡¨"
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³8: PyPI/GitHubç­‰é–‹ç™ºãƒ„ãƒ¼ãƒ«
        if "pypi" in content_lower or "github" in content_lower:
            return "AIé–‹ç™ºãƒ„ãƒ¼ãƒ«ã€æ–°ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå…¬é–‹"
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æœ€é‡è¦æƒ…å ±ã‚’æŠ½å‡º
        return self._extract_from_title(content)
    
    def _extract_companies(self, content: str) -> List[str]:
        """
        æ–‡ç« ã‹ã‚‰ä¼æ¥­åã‚’æŠ½å‡º
        
        Args:
            content (str): ãƒ‹ãƒ¥ãƒ¼ã‚¹å†…å®¹
        
        Returns:
            List[str]: æŠ½å‡ºã•ã‚ŒãŸä¼æ¥­åãƒªã‚¹ãƒˆ
        """
        companies = []
        company_names = [
            ("OpenAI", "OpenAI"), ("Google", "Google"), ("Meta", "Meta"), 
            ("Microsoft", "Microsoft"), ("Amazon", "Amazon"), ("Apple", "Apple"),
            ("Anthropic", "Anthropic"), ("Tesla", "Tesla"), ("xAI", "xAI")
        ]
        
        content_lower = content.lower()
        for english, japanese in company_names:
            if english.lower() in content_lower:
                companies.append(japanese)
        
        return companies
    
    def _extract_from_title(self, content: str) -> str:
        """
        ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰é‡è¦æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ã‚ˆã‚Šæ„å‘³ã®ã‚ã‚‹è¦ç´„ã‚’ä½œæˆ
        
        Args:
            content (str): ãƒ‹ãƒ¥ãƒ¼ã‚¹å†…å®¹
        
        Returns:
            str: æ„å‘³ã®ã‚ã‚‹è¦ç´„
        """
        # ã‚¿ã‚¤ãƒˆãƒ«ã®æœ€åˆã®éƒ¨åˆ†ï¼ˆé€šå¸¸æœ€ã‚‚é‡è¦ï¼‰ã‚’å–å¾—
        title = content.split('.')[0]
        
        # ä¼æ¥­åã‚’ç‰¹å®š
        companies = self._extract_companies(title)
        company = companies[0] if companies else "AIä¼æ¥­"
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®å‹•ä½œæŠ½å‡º
        title_lower = title.lower()
        
        if "warn" in title_lower or "warning" in title_lower:
            return f"{company}CEOã€AIé–¢é€£ã§é‡è¦è­¦å‘Š"
        elif "cut" in title_lower and "time" in title_lower:
            return f"{company}ã€AIæ´»ç”¨ã§ä½œæ¥­æ™‚é–“ã‚’å¤§å¹…çŸ­ç¸®"
        elif "hire" in title_lower or "talent" in title_lower:
            return f"{company}ã€AIäººæç¢ºä¿ã«ç©æ¥µæŠ•è³‡"
        elif "video" in title_lower and "analysis" in title_lower:
            return f"{company}ã€å‹•ç”»AIåˆ†æã‚µãƒ¼ãƒ“ã‚¹é–‹å§‹"
        elif "chatbot" in title_lower:
            return f"{company}ã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆæŠ€è¡“ã‚’å‘ä¸Š"
        else:
            # æœ€å¾Œã®æ‰‹æ®µï¼šã‚¿ã‚¤ãƒˆãƒ«å‰åŠã®é‡è¦éƒ¨åˆ†ã‚’æ—¥æœ¬èªåŒ–
            important_part = title[:30].strip()
            return f"AIæ¥­ç•Œ: {important_part}..."
    
    def _clean_summary(self, summary: str) -> str:
        """
        è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        
        Args:
            summary (str): ç”Ÿã®è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆ
        
        Returns:
            str: ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã•ã‚ŒãŸè¦ç´„
        """
        # ä¸è¦ãªå‰ç½®ãã‚„è¨˜å·ã‚’é™¤å»
        prefixes_to_remove = [
            "æ—¥æœ¬èªè¦ç´„:",
            "è¦ç´„:",
            "ã¾ã¨ã‚:",
            "æ¦‚è¦:",
            "Japanese Summary:",
            "Japanese:",
            "Summary:",
            "ã“ã®è¨˜äº‹ã¯",
            "ã“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯",
            "**",
            "##",
            "- "
        ]
        
        for prefix in prefixes_to_remove:
            if summary.startswith(prefix):
                summary = summary[len(prefix):].strip()
        
        # æ”¹è¡Œã‚’é™¤å»
        summary = summary.replace('\n', ' ').replace('\r', ' ')
        
        # è¤‡æ•°ã‚¹ãƒšãƒ¼ã‚¹ã‚’å˜ä¸€ã‚¹ãƒšãƒ¼ã‚¹ã«
        import re
        summary = re.sub(r'\s+', ' ', summary)
        
        # 50æ–‡å­—åˆ¶é™
        if len(summary) > 50:
            summary = summary[:47] + "..."
        
        return summary.strip()
    
    def _fallback_summary(self, title: str, description: str) -> str:
        """
        ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¦ç´„ï¼ˆLLMãŒåˆ©ç”¨ã§ããªã„å ´åˆï¼‰
        æ”¹å–„ç‰ˆï¼šdescriptionã®å†…å®¹ã‚‚è€ƒæ…®
        
        Args:
            title (str): è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
            description (str): è¨˜äº‹èª¬æ˜
        
        Returns:
            str: ç°¡æ˜“è¦ç´„
        """
        # descriptionã‹ã‚‰é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        important_keywords = [
            "AI", "artificial intelligence", "machine learning", "ChatGPT", "GPT",
            "model", "technology", "startup", "funding", "release", "launch",
            "announce", "update", "improve", "enhance", "breakthrough"
        ]
        
        content = description if description else title
        
        # é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€æ–‡ã‚’å„ªå…ˆ
        sentences = content.split('.')
        best_sentence = ""
        
        for sentence in sentences:
            if any(keyword.lower() in sentence.lower() for keyword in important_keywords):
                best_sentence = sentence.strip()
                break
        
        if not best_sentence:
            best_sentence = sentences[0] if sentences else title
        
        # æ—¥æœ¬èªãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ 
        summary = f"AIæ¥­ç•Œ: {best_sentence}"
        
        if len(summary) > 47:
            summary = summary[:47] + "..."
        
        return summary
    
    def process_news_articles(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒªã‚¹ãƒˆã‚’å‡¦ç†ã—ã¦æ—¥æœ¬èªè¦ç´„ã‚’è¿½åŠ 
        
        Args:
            articles (List[Dict]): ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒªã‚¹ãƒˆ
        
        Returns:
            List[Dict]: æ—¥æœ¬èªè¦ç´„ä»˜ããƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒªã‚¹ãƒˆ
        """
        processed_articles = []
        
        for i, article in enumerate(articles):
            processed_article = article.copy()
            
            print(f"ğŸ“ è¨˜äº‹ {i+1}/{len(articles)} ã‚’è¦ç´„ä¸­...")
            
            # æ—¥æœ¬èªè¦ç´„ã‚’ç”Ÿæˆï¼ˆcontentã‚‚å«ã‚ã‚‹ï¼‰
            summary_jp = self.generate_summary_japanese(
                title=article.get("title", ""),
                description=article.get("description", ""),
                url=article.get("url", ""),
                content=article.get("content", "")
            )
            
            processed_article["summary_jp"] = summary_jp
            processed_articles.append(processed_article)
            
            # APIå‘¼ã³å‡ºã—é–“éš”ã‚’èª¿æ•´ï¼ˆOllamaã‚µãƒ¼ãƒãƒ¼ã®è² è·è»½æ¸›ï¼‰
            if i < len(articles) - 1:
                time.sleep(1)
        
        return processed_articles
    
    def get_status(self) -> Dict[str, Any]:
        """
        ãƒ­ãƒ¼ã‚«ãƒ«LLMæ©Ÿèƒ½ã®çŠ¶æ…‹ã‚’å–å¾—
        
        Returns:
            Dict: çŠ¶æ…‹æƒ…å ±
        """
        return {
            "enabled": self.enabled,
            "model_name": self.model_name,
            "ollama_available": self.available,
            "ollama_url": self.ollama_url,
            "thinking_mode": self.thinking_mode,
            "fallback_enabled": True
        }

    def generate_weekly_news_summary(self, articles: List[Dict[str, Any]]) -> str:
        """
        é€±ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹å…¨ä½“ã‚’æ—¥æœ¬èªã§300æ–‡å­—ç¨‹åº¦ã«ã‚µãƒãƒ©ã‚¤ã‚º
        
        Args:
            articles (List[Dict]): å‡¦ç†æ¸ˆã¿ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒªã‚¹ãƒˆ
        
        Returns:
            str: é€±é–“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µãƒãƒªãƒ¼ï¼ˆæ—¥æœ¬èªã€300æ–‡å­—ç¨‹åº¦ï¼‰
        """
        if not articles:
            return "ä»Šé€±ã¯æ³¨ç›®ã™ã¹ãAIæ¥­ç•Œãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        # è¨˜äº‹ã®ä¸»è¦æƒ…å ±ã‚’æŠ½å‡º
        titles = [article.get("title", "") for article in articles[:8]]  # ä¸Šä½8ä»¶
        summaries = [article.get("summary_jp", "") for article in articles[:8]]
        
        if not self.enabled or not self.available:
            return self._create_fallback_weekly_summary(articles)
        
        try:
            return self._generate_weekly_summary_with_qwen3(titles, summaries)
        except Exception as e:
            print(f"é€±é–“ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return self._create_fallback_weekly_summary(articles)
    
    def _generate_weekly_summary_with_qwen3(self, titles: List[str], summaries: List[str]) -> str:
        """
        Qwen3ã‚’ä½¿ç”¨ã—ã¦é€±é–“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ
        
        Args:
            titles (List[str]): è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ãƒªã‚¹ãƒˆ
            summaries (List[str]): å€‹åˆ¥è¨˜äº‹è¦ç´„ãƒªã‚¹ãƒˆ
        
        Returns:
            str: é€±é–“ã‚µãƒãƒªãƒ¼
        """
        # è¨˜äº‹æƒ…å ±ã‚’æ•´ç†
        article_info = []
        for i, (title, summary) in enumerate(zip(titles, summaries)):
            if title and summary:
                article_info.append(f"{i+1}. {summary}")
        
        articles_text = "\n".join(article_info[:6])  # ä¸Šä½6ä»¶
        
        prompt = f"""<|user|>
ä»¥ä¸‹ã¯ä»Šé€±ã®AIæ¥­ç•Œãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ã™ã€‚å…¨ä½“çš„ãªãƒˆãƒ¬ãƒ³ãƒ‰ã¨é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’æ—¥æœ¬èªã§300æ–‡å­—ä»¥å†…ã§ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

ä»Šé€±ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹:
{articles_text}

æ¥­ç•Œå…¨ä½“ã®å‹•å‘ã€ä¸»è¦ä¼æ¥­ã®å‹•ãã€æ³¨ç›®ã™ã¹ãæŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’å«ã‚ã¦é€±é–“ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
<|assistant|>
"""
        
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.4,
                "top_p": 0.9,
                "num_predict": 400,
                "stop": ["<|user|>", "<|assistant|>"],
                "repeat_penalty": 1.1
            }
        }
        
        try:
            print("ğŸ” é€±é–“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆä¸­...")
            
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                timeout=45
            )
            
            if response.status_code == 200:
                result = response.json()
                raw_summary = result.get('response', '').strip()
                
                # thinking modeã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                if '<think>' in raw_summary:
                    if '</think>' in raw_summary:
                        raw_summary = raw_summary.split('</think>')[-1].strip()
                    else:
                        return self._create_fallback_weekly_summary_from_summaries(summaries)
                
                # ã‚µãƒãƒªãƒ¼ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                summary = self._clean_weekly_summary(raw_summary)
                
                # å“è³ªãƒã‚§ãƒƒã‚¯
                if len(summary) > 50 and any(char in summary for char in 'ã‚ã„ã†ãˆãŠã‹ããã‘ã“'):
                    return summary
                else:
                    return self._create_fallback_weekly_summary_from_summaries(summaries)
            else:
                return self._create_fallback_weekly_summary_from_summaries(summaries)
                
        except Exception as e:
            print(f"é€±é–“ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return self._create_fallback_weekly_summary_from_summaries(summaries)
    
    def _create_fallback_weekly_summary(self, articles: List[Dict[str, Any]]) -> str:
        """
        ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é€±é–“ã‚µãƒãƒªãƒ¼ä½œæˆ
        
        Args:
            articles (List[Dict]): ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒªã‚¹ãƒˆ
        
        Returns:
            str: é€±é–“ã‚µãƒãƒªãƒ¼
        """
        summaries = [article.get("summary_jp", "") for article in articles if article.get("summary_jp")]
        return self._create_fallback_weekly_summary_from_summaries(summaries)
    
    def _create_fallback_weekly_summary_from_summaries(self, summaries: List[str]) -> str:
        """
        å€‹åˆ¥è¦ç´„ã‹ã‚‰é€±é–“ã‚µãƒãƒªãƒ¼ã‚’æ§‹ç¯‰
        
        Args:
            summaries (List[str]): å€‹åˆ¥è¨˜äº‹è¦ç´„ãƒªã‚¹ãƒˆ
        
        Returns:
            str: é€±é–“ã‚µãƒãƒªãƒ¼
        """
        if not summaries:
            return "ä»Šé€±ã¯æ³¨ç›®ã™ã¹ãAIæ¥­ç•Œãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        # ä¸»è¦ãƒˆãƒ”ãƒƒã‚¯ã‚’åˆ†æ
        topics = {
            "ä¼æ¥­ç«¶äº‰": 0,
            "æŠ€è¡“ç™ºè¡¨": 0,
            "æŠ•è³‡ãƒ»è³‡é‡‘èª¿é”": 0,
            "äººäº‹ãƒ»çµ„ç¹”": 0,
            "è£½å“ãƒªãƒªãƒ¼ã‚¹": 0,
            "æ”¿åºœãƒ»è¦åˆ¶": 0
        }
        
        key_companies = set()
        
        for summary in summaries[:8]:
            summary_lower = summary.lower()
            
            # ãƒˆãƒ”ãƒƒã‚¯åˆ†æ
            if any(word in summary for word in ["å¼•ãæŠœã", "ç«¶äº‰", "ãƒœãƒ¼ãƒŠã‚¹"]):
                topics["ä¼æ¥­ç«¶äº‰"] += 1
            if any(word in summary for word in ["ç™ºè¡¨", "ãƒªãƒªãƒ¼ã‚¹", "é–‹å§‹"]):
                topics["æŠ€è¡“ç™ºè¡¨"] += 1
            if any(word in summary for word in ["æŠ•è³‡", "è³‡é‡‘èª¿é”", "å¤§å‹"]):
                topics["æŠ•è³‡ãƒ»è³‡é‡‘èª¿é”"] += 1
            if any(word in summary for word in ["äººäº‹", "çµ„ç¹”", "CEO"]):
                topics["äººäº‹ãƒ»çµ„ç¹”"] += 1
            if any(word in summary for word in ["ã‚µãƒ¼ãƒ“ã‚¹", "æ©Ÿèƒ½", "å‹•ç”»"]):
                topics["è£½å“ãƒªãƒªãƒ¼ã‚¹"] += 1
            if any(word in summary for word in ["æ”¿åºœ", "è¦åˆ¶", "æ”¿ç­–"]):
                topics["æ”¿åºœãƒ»è¦åˆ¶"] += 1
            
            # ä¼æ¥­åæŠ½å‡º
            for company in ["OpenAI", "Meta", "Google", "Microsoft", "Amazon", "Apple"]:
                if company in summary:
                    key_companies.add(company)
        
        # ä¸»è¦ãƒˆãƒ”ãƒƒã‚¯ã‚’ç‰¹å®š
        main_topic = max(topics.items(), key=lambda x: x[1])
        
        # ã‚µãƒãƒªãƒ¼æ§‹ç¯‰
        companies_str = "ã€".join(list(key_companies)[:3]) if key_companies else "AIé–¢é€£ä¼æ¥­"
        
        if main_topic[1] > 0:
            if main_topic[0] == "ä¼æ¥­ç«¶äº‰":
                summary_text = f"ä»Šé€±ã®AIæ¥­ç•Œã§ã¯{companies_str}ã‚’ä¸­å¿ƒã¨ã—ãŸäººæç²å¾—ç«¶äº‰ãŒæ¿€åŒ–ã€‚"
            elif main_topic[0] == "æŠ€è¡“ç™ºè¡¨":
                summary_text = f"ä»Šé€±ã¯{companies_str}ã«ã‚ˆã‚‹æ–°æŠ€è¡“ãƒ»ã‚µãƒ¼ãƒ“ã‚¹ç™ºè¡¨ãŒç›¸æ¬¡ãã€AIæ©Ÿèƒ½ã®å®Ÿç”¨åŒ–ãŒåŠ é€Ÿã€‚"
            elif main_topic[0] == "æŠ•è³‡ãƒ»è³‡é‡‘èª¿é”":
                summary_text = f"AIæ¥­ç•Œã§å¤§å‹æŠ•è³‡æ¡ˆä»¶ãŒæ´»ç™ºåŒ–ã€‚{companies_str}é–¢é€£ã®è³‡é‡‘èª¿é”ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒç›®ç«‹ã¤ã€‚"
            elif main_topic[0] == "è£½å“ãƒªãƒªãƒ¼ã‚¹":
                summary_text = f"{companies_str}ãŒæ–°è£½å“ãƒ»æ©Ÿèƒ½ã‚’ç›¸æ¬¡ã„ã§ç™ºè¡¨ã€‚ç‰¹ã«å‹•ç”»AIåˆ†æãªã©ã®å®Ÿç”¨çš„ã‚µãƒ¼ãƒ“ã‚¹ãŒæ³¨ç›®ã€‚"
            else:
                summary_text = f"ä»Šé€±ã®AIæ¥­ç•Œã§ã¯{companies_str}ã‚’ä¸­å¿ƒã¨ã—ãŸå‹•ããŒæ´»ç™ºåŒ–ã€‚"
        else:
            summary_text = f"ä»Šé€±ã®AIæ¥­ç•Œã§ã¯{companies_str}ã«ã‚ˆã‚‹æ§˜ã€…ãªå–ã‚Šçµ„ã¿ãŒå ±å‘Šã•ã‚Œã€"
        
        # å…·ä½“çš„ãªå‹•å‘ã‚’è¿½åŠ 
        specific_trends = []
        for summary in summaries[:3]:
            if len(summary) > 10 and summary not in summary_text:
                specific_trends.append(summary.replace("AIæ¥­ç•Œ: ", "").replace("...", ""))
        
        if specific_trends:
            summary_text += f" ä¸»ãªå‹•å‘ã¨ã—ã¦{specific_trends[0]}ãªã©ã€"
            if len(specific_trends) > 1:
                summary_text += f"{specific_trends[1]}ã¨ã„ã£ãŸå±•é–‹ã‚‚è¦‹ã‚‰ã‚Œã‚‹ã€‚"
            else:
                summary_text += "æ¥­ç•Œå…¨ä½“ã®å¤‰åŒ–ãŒç¶™ç¶šã—ã¦ã„ã‚‹ã€‚"
        else:
            summary_text += " æ¥­ç•Œå…¨ä½“ã§AIæŠ€è¡“ã®å®Ÿç”¨åŒ–ã¨ç«¶äº‰ãŒåŠ é€Ÿã—ã¦ã„ã‚‹çŠ¶æ³ãŒç¶šã„ã¦ã„ã‚‹ã€‚"
        
        # 300æ–‡å­—åˆ¶é™
        if len(summary_text) > 300:
            summary_text = summary_text[:297] + "..."
        
        return summary_text
    
    def _clean_weekly_summary(self, summary: str) -> str:
        """
        é€±é–“ã‚µãƒãƒªãƒ¼ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        
        Args:
            summary (str): ç”Ÿã®é€±é–“ã‚µãƒãƒªãƒ¼
        
        Returns:
            str: ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã•ã‚ŒãŸé€±é–“ã‚µãƒãƒªãƒ¼
        """
        # ä¸è¦ãªå‰ç½®ãã‚’é™¤å»
        prefixes_to_remove = [
            "ä»Šé€±ã®AIæ¥­ç•Œãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ã¾ã¨ã‚ã‚‹ã¨ã€",
            "é€±é–“ã‚µãƒãƒªãƒ¼:",
            "ã¾ã¨ã‚:",
            "æ¦‚è¦:",
            "ä»Šé€±ã¯ã€"
        ]
        
        for prefix in prefixes_to_remove:
            if summary.startswith(prefix):
                summary = summary[len(prefix):].strip()
        
        # æ”¹è¡Œã‚’é™¤å»
        summary = summary.replace('\n', ' ').replace('\r', ' ')
        
        # è¤‡æ•°ã‚¹ãƒšãƒ¼ã‚¹ã‚’å˜ä¸€ã‚¹ãƒšãƒ¼ã‚¹ã«
        import re
        summary = re.sub(r'\s+', ' ', summary)
        
        # 300æ–‡å­—åˆ¶é™
        if len(summary) > 300:
            summary = summary[:297] + "..."
        
        return summary.strip()


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    summarizer = LocalLLMSummarizer()
    
    print("=== ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼ˆQwen3ï¼‰è¦ç´„æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ ===")
    status = summarizer.get_status()
    print(f"çŠ¶æ…‹: {status}")
    
    # ã‚µãƒ³ãƒ—ãƒ«è¨˜äº‹ã§ãƒ†ã‚¹ãƒˆ
    sample_articles = [
        {
            "title": "OpenAI announces new GPT-5 model with enhanced reasoning capabilities",
            "description": "The latest AI model from OpenAI shows significant improvements in logical reasoning, mathematics, and code generation, outperforming previous versions in benchmark tests.",
            "url": "https://example.com/openai-gpt5",
            "keyword": "OpenAI"
        },
        {
            "title": "Google's Gemini AI now supports video analysis and understanding",
            "description": "Google has updated its Gemini AI model to include advanced video processing capabilities, allowing users to analyze and interact with video content in natural language.",
            "url": "https://example.com/gemini-video",
            "keyword": "Gemini"
        }
    ]
    
    if summarizer.enabled:
        print("\n=== è¦ç´„ãƒ†ã‚¹ãƒˆé–‹å§‹ ===")
        processed = summarizer.process_news_articles(sample_articles)
        
        print("\n=== å‡¦ç†çµæœ ===")
        for article in processed:
            print(f"ğŸ“° ã‚¿ã‚¤ãƒˆãƒ«: {article['title'][:50]}...")
            print(f"ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªè¦ç´„: {article.get('summary_jp', 'æœªç”Ÿæˆ')}")
            print("---")
    else:
        print("âš ï¸  ãƒ­ãƒ¼ã‚«ãƒ«LLMæ©Ÿèƒ½ãŒç„¡åŠ¹ã¾ãŸã¯OllamaãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        print("Ollamaã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã€ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:")
        print("  ollama pull qwen3:8b")
        print("  ollama serve") 