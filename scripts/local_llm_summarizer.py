#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼ˆQwen3ï¼‰ãƒ‹ãƒ¥ãƒ¼ã‚¹è¦ç´„æ©Ÿèƒ½

ã“ã®ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«ã¯Ollamaã‚’ä½¿ç”¨ã—ã¦Qwen3ãƒ¢ãƒ‡ãƒ«ã§
è‹±èªã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‚’æ—¥æœ¬èªã«è¦ç´„ã—ã¾ã™ã€‚
"""

import json
import requests
import time
from typing import Dict, List, Optional, Any
from datetime import datetime

# Ollama Python APIã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å¯¾å¿œï¼‰
try:
    import ollama
    OLLAMA_CLIENT_AVAILABLE = True
except ImportError:
    OLLAMA_CLIENT_AVAILABLE = False
    print("âš ï¸ Ollama Python client not found. Install with: pip install ollama")


class LocalLLMSummarizer:
    """
    ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼ˆQwen3ï¼‰ã‚’ä½¿ç”¨ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹è¦ç´„æ©Ÿèƒ½
    """
    
    def __init__(self, config_path: str = "config/settings.json"):
        """
        åˆæœŸåŒ–
        
        Args:
            config_path (str): è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        with open(config_path, 'r', encoding='utf-8') as f:
            self.config = json.load(f)
        
        self.llm_config = self.config["data_sources"].get("local_llm", {})
        self.enabled = self.llm_config.get("enabled", False)
        self.ollama_url = self.llm_config.get("ollama_url", "http://localhost:11434")
        self.model_name = self.llm_config.get("model_name", "qwen3:8b")
        self.thinking_mode = self.llm_config.get("thinking_mode", False)
        
        # Ollama Pythonã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆã®åˆæœŸåŒ–
        self.ollama_client = None
        if OLLAMA_CLIENT_AVAILABLE:
            try:
                self.ollama_client = ollama.Client(host=self.ollama_url)
                print("âœ… Ollama Python client initialized successfully")
            except Exception as e:
                print(f"âš ï¸ Failed to initialize Ollama client: {e}")
                self.ollama_client = None
        
        # æ¥ç¶šãƒ†ã‚¹ãƒˆ
        self.available = self._test_ollama_connection()
    
    def _test_ollama_connection(self) -> bool:
        """
        Ollamaã‚µãƒ¼ãƒãƒ¼ã¨ã®æ¥ç¶šã‚’ãƒ†ã‚¹ãƒˆ
        
        Returns:
            bool: æ¥ç¶šæˆåŠŸã®å ´åˆTrue
        """
        try:
            if self.ollama_client:
                # Python clientã§ãƒ†ã‚¹ãƒˆ
                models = self.ollama_client.list()
                model_names = [model['name'] for model in models['models']]
                return any(self.model_name in name for name in model_names)
            else:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šç›´æ¥APIãƒ†ã‚¹ãƒˆ
                response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
                if response.status_code == 200:
                    models = response.json().get('models', [])
                    model_names = [model['name'] for model in models]
                    return any(self.model_name in name for name in model_names)
            return False
        except Exception as e:
            print(f"Ollamaæ¥ç¶šãƒ†ã‚¹ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def generate_summary_japanese(self, title: str, description: str, url: str = "", content: str = "") -> Optional[str]:
        """
        è‹±èªã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã‹ã‚‰æ—¥æœ¬èªè¦ç´„ã‚’ç”Ÿæˆ
        
        Args:
            title (str): è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ï¼ˆè‹±èªï¼‰
            description (str): è¨˜äº‹èª¬æ˜ï¼ˆè‹±èªï¼‰
            url (str): è¨˜äº‹URLï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
            content (str): è¨˜äº‹æœ¬æ–‡ï¼ˆè‹±èªã€ã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
        Returns:
            Optional[str]: æ—¥æœ¬èªè¦ç´„ï¼ˆå¤±æ•—æ™‚ã¯Noneï¼‰
        """
        if not self.enabled or not self.available:
            return self._create_intelligent_fallback(title, description, content)
        
        try:
            return self._summarize_with_qwen3(title, description, content)
        except Exception as e:
            print(f"Qwen3è¦ç´„ã‚¨ãƒ©ãƒ¼: {e}")
            return self._create_intelligent_fallback(title, description, content)
    
    def _summarize_with_qwen3(self, title: str, description: str, content: str = "") -> Optional[str]:
        """
        Qwen3ã‚’ä½¿ç”¨ã—ã¦æ—¥æœ¬èªè¦ç´„ã‚’ç”Ÿæˆï¼ˆthinking modeç„¡åŠ¹åŒ–ï¼‰
        
        Args:
            title (str): è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
            description (str): è¨˜äº‹èª¬æ˜
            content (str): è¨˜äº‹æœ¬æ–‡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
        Returns:
            Optional[str]: æ—¥æœ¬èªè¦ç´„
        """
        # ã‚ˆã‚Šå¤šãã®æƒ…å ±ã‚’æ´»ç”¨ã—ã¦è¦ç´„ã®è³ªã‚’å‘ä¸Š
        full_text = f"{title}. {description or ''} {content or ''}".strip()
        
        # æœ€æ–°ã®Ollama Python APIã‚’ä½¿ç”¨ã—ã¦thinking modeã‚’å®Œå…¨ç„¡åŠ¹åŒ–
        if self.ollama_client:
            return self._summarize_with_ollama_client(full_text, title)
        else:
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼šå¾“æ¥ã®requestsæ–¹å¼
            return self._summarize_with_requests_fallback(full_text, title, description, content)
    
    def _summarize_with_ollama_client(self, full_text: str, title: str) -> Optional[str]:
        """
        Ollama Python clientã‚’ä½¿ç”¨ã—ãŸè¦ç´„ç”Ÿæˆï¼ˆthinking modeå®Œå…¨ç„¡åŠ¹åŒ–ï¼‰
        
        Args:
            full_text (str): å…¨è¨˜äº‹ãƒ†ã‚­ã‚¹ãƒˆ
            title (str): è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
        
        Returns:
            Optional[str]: æ—¥æœ¬èªè¦ç´„
        """
        try:
            print(f"ğŸ¤– Qwen3ï¼ˆthinking OFFï¼‰ã§è¦ç´„ç”Ÿæˆä¸­: {title[:30]}...")
            
            # thinking modeå®Œå…¨ç„¡åŠ¹åŒ–ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
            prompt = f"""You are a professional Japanese business news summarizer.

Article: {full_text[:800]}

Task: Create a concise Japanese summary (max 50 characters) that includes:
1. What happened (å…·ä½“çš„ãªå‡ºæ¥äº‹)
2. Who is involved (é–¢ä¿‚è€…ãƒ»ä¼æ¥­å) 
3. Key impact (é‡è¦ãªå½±éŸ¿)

Requirements: 
- Output ONLY the Japanese summary
- No thinking, no explanation, no analysis
- Direct answer only

Japanese summary:"""
            
            # Ollama Python clientã§thinking modeç„¡åŠ¹åŒ–
            response = self.ollama_client.chat(
                model=self.model_name,
                messages=[{
                    'role': 'user',
                    'content': prompt
                }],
                stream=False,
                think=False,  # ğŸ”‘ Key: thinking modeå®Œå…¨ç„¡åŠ¹åŒ–
                options={
                    'temperature': 0.3,
                    'top_p': 0.9,
                    'top_k': 40,
                    'num_predict': 80,
                    'repeat_penalty': 1.15,
                    'seed': 42
                }
            )
            
            if response and 'message' in response:
                raw_summary = response['message']['content'].strip()
                print(f"ğŸ“ ç”Ÿæˆã•ã‚ŒãŸè¦ç´„ï¼ˆOllama Clientï¼‰: {raw_summary}")
                
                # ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã¨å“è³ªãƒã‚§ãƒƒã‚¯
                summary = self._clean_summary(raw_summary)
                
                if self._is_high_quality_summary(summary, title, ""):
                    print(f"âœ… é«˜å“è³ªè¦ç´„ï¼ˆthinkingç„¡åŠ¹åŒ–ï¼‰: {summary}")
                    return summary
                else:
                    print("âš ï¸ å“è³ªä¸è¶³ã€ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨...")
                    return self._create_intelligent_fallback(title, "", "")
            else:
                print("âŒ Ollama client response invalid")
                return self._create_intelligent_fallback(title, "", "")
                
        except Exception as e:
            print(f"âŒ Ollama client error: {e}")
            return self._create_intelligent_fallback(title, "", "")
    
    def _summarize_with_requests_fallback(self, full_text: str, title: str, description: str, content: str) -> Optional[str]:
        """
        å¾“æ¥ã®requestsæ–¹å¼ã§ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¦ç´„ç”Ÿæˆ
        
        Args:
            full_text (str): å…¨è¨˜äº‹ãƒ†ã‚­ã‚¹ãƒˆ
            title (str): è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
            description (str): è¨˜äº‹èª¬æ˜
            content (str): è¨˜äº‹æœ¬æ–‡
        
        Returns:
            Optional[str]: æ—¥æœ¬èªè¦ç´„
        """
        print(f"ğŸ¤– Qwen3ï¼ˆrequests fallbackï¼‰ã§è¦ç´„ç”Ÿæˆä¸­: {title[:30]}...")
        
        # ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«/no_thinkã‚’è¿½åŠ ã—ã¦thinking modeæŠ‘åˆ¶ã‚’è©¦è¡Œ
        prompt = f"""/no_think

You are a professional Japanese business news summarizer.

Article: {full_text[:800]}

Task: Create a concise Japanese summary (max 50 characters).
Output ONLY the Japanese summary, no thinking, no explanation.

Japanese summary:

/no_think"""
        
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.2,
                "top_p": 0.9,
                "top_k": 40,
                "num_predict": 80,
                "stop": ["<|user|>", "<|assistant|>", "\n\n", "English:", "Article:", "<think>"],
                "repeat_penalty": 1.15,
                "seed": 42
            }
        }
        
        try:
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                timeout=40
            )
            
            if response.status_code == 200:
                result = response.json()
                raw_summary = result.get('response', '').strip()
                print(f"ğŸ“ ç”Ÿæˆã•ã‚ŒãŸè¦ç´„ï¼ˆrequestsï¼‰: {raw_summary}")
                
                # thinking content detection
                if '<think>' in raw_summary or 'thinking' in raw_summary.lower():
                    print("âš ï¸ Thinking mode detected in requests fallback, using intelligent fallback...")
                    return self._create_intelligent_fallback(title, description, content)
                
                summary = self._clean_summary(raw_summary)
                
                if self._is_high_quality_summary(summary, title, description):
                    print(f"âœ… è¦ç´„ï¼ˆrequests fallbackï¼‰: {summary}")
                    return summary
                else:
                    print("âš ï¸ å“è³ªä¸è¶³ã€ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ä½¿ç”¨...")
                    return self._create_intelligent_fallback(title, description, content)
                    
            else:
                print(f"âŒ Requests API error: {response.status_code}")
                return self._create_intelligent_fallback(title, description, content)
                
        except Exception as e:
            print(f"âŒ Requests API error: {e}")
            return self._create_intelligent_fallback(title, description, content)
    
    def _create_intelligent_fallback(self, title: str, description: str, content: str = "") -> str:
        """
        ã‚¤ãƒ³ãƒ†ãƒªã‚¸ã‚§ãƒ³ãƒˆãªãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¦ç´„ã‚’ç”Ÿæˆ
        
        Args:
            title (str): è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
            description (str): è¨˜äº‹èª¬æ˜
            content (str): è¨˜äº‹æœ¬æ–‡
        
        Returns:
            str: æ”¹å–„ã•ã‚ŒãŸãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¦ç´„
        """
        # å…¨ãƒ†ã‚­ã‚¹ãƒˆã‚’çµåˆ
        full_text = f"{title} {description or ''} {content or ''}".lower()
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®åˆ†æ
        summary_parts = []
        
        # ä¼æ¥­ãƒ»ã‚µãƒ¼ãƒ“ã‚¹åã®æŠ½å‡º
        companies = self._extract_companies_enhanced(full_text)
        if companies:
            summary_parts.append(companies[0])
        
        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆã®æŠ½å‡º
        actions = self._extract_key_actions(full_text)
        if actions:
            summary_parts.append(actions[0])
        
        # é‡‘é¡ãƒ»æ•°å€¤ã®æŠ½å‡º
        amounts = self._extract_amounts(full_text)
        if amounts:
            summary_parts.append(amounts[0])
        
        # è¦ç´„ã‚’æ§‹ç¯‰
        if summary_parts:
            base_summary = "ã€".join(summary_parts)
        else:
            # æœ€ä½é™ã®æƒ…å ±ã‹ã‚‰ã§ã‚‚æ„å‘³ã®ã‚ã‚‹è¦ç´„ã‚’ä½œæˆ
            base_summary = self._extract_core_meaning(title, description)
        
        # 50æ–‡å­—ä»¥å†…ã«èª¿æ•´
        if len(base_summary) > 50:
            base_summary = base_summary[:47] + "..."
        
        return base_summary

    def _extract_companies_enhanced(self, text: str) -> List[str]:
        """
        ä¼æ¥­åã‚’æŠ½å‡ºï¼ˆå¼·åŒ–ç‰ˆï¼‰
        """
        companies = []
        text_lower = text.lower()
        
        # ã‚ˆã‚Šè©³ç´°ãªä¼æ¥­ãƒ»ã‚µãƒ¼ãƒ“ã‚¹åãƒãƒƒãƒ”ãƒ³ã‚°
        company_patterns = [
            ('openai', 'OpenAI'),
            ('google', 'Google'),
            ('gboard', 'Google'),
            ('microsoft', 'Microsoft'),
            ('anthropic', 'Anthropic'),
            ('meta', 'Meta'),
            ('nvidia', 'NVIDIA'),
            ('apple', 'Apple'),
            ('amazon', 'Amazon'),
            ('tesla', 'Tesla'),
            ('netflix', 'Netflix'),
            ('polar', 'Polar'),
            ('polyhedra', 'Polyhedra'),
            ('deepgram', 'Deepgram'),
            ('gemini', 'Google Gemini'),
            ('gpt', 'OpenAI'),
            ('chatgpt', 'OpenAI'),
            ('claude', 'Anthropic'),
            ('bard', 'Google'),
            ('pypi', 'Python'),
            ('python', 'Python')
        ]
        
        for pattern, company_name in company_patterns:
            if pattern in text_lower:
                companies.append(company_name)
                break
        
        # è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰å…·ä½“çš„ãªä¼æ¥­ã‚’æŠ½å‡º
        if not companies:
            if 'ai industry' in text_lower:
                companies.append('AIæ¥­ç•Œ')
            elif 'ecommerce' in text_lower:
                companies.append('Eã‚³ãƒãƒ¼ã‚¹')
            elif 'ad industry' in text_lower:
                companies.append('åºƒå‘Šæ¥­ç•Œ')
            else:
                companies.append('ãƒ†ãƒƒã‚¯ä¼æ¥­')
        
        return companies

    def _extract_key_actions(self, text: str) -> List[str]:
        """
        ä¸»è¦ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ãƒ»ã‚¤ãƒ™ãƒ³ãƒˆã‚’æŠ½å‡ºï¼ˆæ”¹å–„ç‰ˆï¼‰
        """
        actions = []
        text_lower = text.lower()
        
        # ã‚ˆã‚Šå…·ä½“çš„ãªãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒãƒ³ã‚°
        if 'raise' in text_lower and ('million' in text_lower or 'billion' in text_lower):
            actions.append('è³‡é‡‘èª¿é”')
        elif 'replace' in text_lower and 'human' in text_lower:
            actions.append('AIäººæç½®ãæ›ãˆ')
        elif 'gboard' in text_lower and 'change' in text_lower:
            actions.append('Gboardæ©Ÿèƒ½æ›´æ–°')
        elif 'ecommerce' in text_lower and 'tool' in text_lower:
            actions.append('Eã‚³ãƒãƒ¼ã‚¹ãƒ„ãƒ¼ãƒ«ç™ºè¡¨')
        elif 'environment' in text_lower and 'impact' in text_lower:
            actions.append('AIç’°å¢ƒè² è·å¯¾ç­–')
        elif 'voice' in text_lower and 'chat' in text_lower:
            actions.append('éŸ³å£°ãƒãƒ£ãƒƒãƒˆæ©Ÿèƒ½')
        elif 'podcast' in text_lower or 'interview' in text_lower:
            actions.append('æ¥­ç•Œã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼')
        elif 'cannes' in text_lower and 'ad' in text_lower:
            actions.append('ã‚«ãƒ³ãƒŒåºƒå‘Šæ¥­ç•Œå‹•å‘')
        elif 'pandas' in text_lower and 'collective' in text_lower:
            actions.append('å‹•ç‰©å­¦ç”¨èªè©±é¡Œ')
        elif 'netflix' in text_lower and 'review' in text_lower:
            actions.append('Netflixæ–°ä½œãƒ¬ãƒ“ãƒ¥ãƒ¼')
        elif 'mindhunter' in text_lower:
            actions.append('ä¿³å„ªã‚¤ãƒ³ã‚¿ãƒ“ãƒ¥ãƒ¼')
        elif 'school bus' in text_lower:
            actions.append('ç¤¾ä¼šå•é¡Œå ±é“')
        elif 'pypi' in text_lower or 'python' in text_lower:
            actions.append('Pythonãƒ©ã‚¤ãƒ–ãƒ©ãƒªå…¬é–‹')
        elif 'ai industry' in text_lower and 'veteran' in text_lower:
            actions.append('AIæ¥­ç•Œå°‚é–€å®¶è¬›æ¼”')
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã‚¢ã‚¯ã‚·ãƒ§ãƒ³
            if 'launch' in text_lower or 'release' in text_lower:
                actions.append('æ–°è£½å“ç™ºè¡¨')
            elif 'update' in text_lower or 'improve' in text_lower:
                actions.append('æ©Ÿèƒ½æ”¹å–„')
            elif 'announce' in text_lower:
                actions.append('é‡è¦ç™ºè¡¨')
            else:
                actions.append('æ¥­ç•Œå‹•å‘')
        
        return actions

    def _extract_amounts(self, text: str) -> List[str]:
        """
        é‡‘é¡ãƒ»æ•°å€¤ã‚’æŠ½å‡º
        """
        amounts = []
        
        # é‡‘é¡ãƒ‘ã‚¿ãƒ¼ãƒ³
        import re
        money_patterns = [
            r'\$(\d+(?:,\d{3})*(?:\.\d+)?)\s*million',
            r'\$(\d+(?:,\d{3})*(?:\.\d+)?)\s*billion',
            r'(\d+(?:,\d{3})*)\s*million',
            r'(\d+(?:,\d{3})*)\s*billion'
        ]
        
        for pattern in money_patterns:
            matches = re.findall(pattern, text, re.IGNORECASE)
            if matches:
                amount = matches[0]
                if 'million' in text:
                    amounts.append(f"{amount}ç™¾ä¸‡ãƒ‰ãƒ«")
                elif 'billion' in text:
                    amounts.append(f"{amount}å„„ãƒ‰ãƒ«")
                break
        
        return amounts

    def _extract_core_meaning(self, title: str, description: str) -> str:
        """
        ã‚¿ã‚¤ãƒˆãƒ«ã¨èª¬æ˜ã‹ã‚‰æ ¸å¿ƒçš„ãªæ„å‘³ã‚’æŠ½å‡º
        """
        # ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        title_lower = title.lower()
        
        # é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ‘ã‚¿ãƒ¼ãƒ³
        keyword_map = {
            'ai': 'AI',
            'artificial intelligence': 'AI',
            'machine learning': 'æ©Ÿæ¢°å­¦ç¿’',
            'deep learning': 'æ·±å±¤å­¦ç¿’',
            'neural network': 'ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆ',
            'chatbot': 'ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ',
            'voice': 'éŸ³å£°æŠ€è¡“',
            'automation': 'è‡ªå‹•åŒ–',
            'startup': 'ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—',
            'funding': 'è³‡é‡‘èª¿é”',
            'investment': 'æŠ•è³‡',
            'technology': 'æŠ€è¡“',
            'innovation': 'ã‚¤ãƒãƒ™ãƒ¼ã‚·ãƒ§ãƒ³',
            'platform': 'ãƒ—ãƒ©ãƒƒãƒˆãƒ•ã‚©ãƒ¼ãƒ ',
            'service': 'ã‚µãƒ¼ãƒ“ã‚¹',
            'tool': 'ãƒ„ãƒ¼ãƒ«',
            'model': 'ãƒ¢ãƒ‡ãƒ«',
            'feature': 'æ©Ÿèƒ½',
            'update': 'ã‚¢ãƒƒãƒ—ãƒ‡ãƒ¼ãƒˆ',
            'launch': 'ãƒ­ãƒ¼ãƒ³ãƒ',
            'release': 'ãƒªãƒªãƒ¼ã‚¹'
        }
        
        found_keywords = []
        for eng, jp in keyword_map.items():
            if eng in title_lower:
                found_keywords.append(jp)
        
        if found_keywords:
            return f"{found_keywords[0]}é–¢é€£ã®æ–°å±•é–‹"
        else:
            # æœ€å¾Œã®æ‰‹æ®µï¼šã‚¿ã‚¤ãƒˆãƒ«ã®æœ€åˆã®éƒ¨åˆ†ã‚’ä½¿ç”¨
            title_parts = title.split()[:3]
            return f"{''.join(title_parts)[:20]}é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹"

    def _is_high_quality_summary(self, summary: str, title: str, description: str) -> bool:
        """
        è¦ç´„ã®å“è³ªã‚’ãƒã‚§ãƒƒã‚¯ï¼ˆå¼·åŒ–ç‰ˆï¼‰
        
        Args:
            summary (str): ç”Ÿæˆã•ã‚ŒãŸè¦ç´„
            title (str): å…ƒã®ã‚¿ã‚¤ãƒˆãƒ«
            description (str): å…ƒã®èª¬æ˜
        
        Returns:
            bool: é«˜å“è³ªã®å ´åˆTrue
        """
        if not summary or len(summary.strip()) < 10:
            return False
        
        # ä¸é©åˆ‡ãªãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ãƒã‚§ãƒƒã‚¯
        bad_patterns = [
            'AIæ¥­ç•Œ:',  # å¤ã„ãƒ‘ã‚¿ãƒ¼ãƒ³
            '...',  # çœç•¥è¨˜å·ã®ã¿
            'calls for',  # è‹±èªã®æ®‹å­˜
            'industry veteran',  # è‹±èªã®æ®‹å­˜
            '<think>',  # thinking mode
            'assistant',  # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
            'user',  # ã‚·ã‚¹ãƒ†ãƒ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸
        ]
        
        for pattern in bad_patterns:
            if pattern in summary:
                return False
        
        # æ—¥æœ¬èªã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ç¢ºèª
        japanese_chars = sum(1 for char in summary if '\u3040' <= char <= '\u309F' or '\u30A0' <= char <= '\u30FF' or '\u4E00' <= char <= '\u9FAF')
        if japanese_chars < len(summary) * 0.3:  # 30%ä»¥ä¸ŠãŒæ—¥æœ¬èªã§ã‚ã‚‹å¿…è¦
            return False
        
        return True
    
    def _is_valid_summary(self, summary: str, title: str, description: str) -> bool:
        """
        è¦ç´„ã®å“è³ªã‚’ãƒã‚§ãƒƒã‚¯
        
        Args:
            summary (str): ç”Ÿæˆã•ã‚ŒãŸè¦ç´„
            title (str): å…ƒã®ã‚¿ã‚¤ãƒˆãƒ«
            description (str): å…ƒã®èª¬æ˜
        
        Returns:
            bool: æœ‰åŠ¹ãªè¦ç´„ã‹ã©ã†ã‹
        """
        if not summary or len(summary) < 10:
            return False
        
        # æ—¥æœ¬èªãŒå«ã¾ã‚Œã¦ã„ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        import re
        has_japanese = bool(re.search(r'[\u3040-\u309F\u30A0-\u30FF\u4E00-\u9FAF]', summary))
        
        # å…ƒã®ã‚¿ã‚¤ãƒˆãƒ«ã‚’ãã®ã¾ã¾ä½¿ã£ã¦ã„ãªã„ã‹ãƒã‚§ãƒƒã‚¯
        title_words = title.lower().split()[:3]  # æœ€åˆã®3å˜èª
        summary_lower = summary.lower()
        
        similarity_count = sum(1 for word in title_words if word in summary_lower)
        too_similar = len(title_words) > 0 and similarity_count >= len(title_words)
        
        return has_japanese and not too_similar
    
    def _improved_fallback_summary(self, title: str, description: str, content: str = "") -> str:
        """
        æ”¹å–„ã•ã‚ŒãŸãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¦ç´„
        å…·ä½“çš„ãªäº‹å®Ÿã¨çµè«–ãŒåˆ†ã‹ã‚‹è¦ç´„ã‚’ä½œæˆ
        
        Args:
            title (str): è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
            description (str): è¨˜äº‹èª¬æ˜
            content (str): è¨˜äº‹æœ¬æ–‡ï¼ˆã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰
        
        Returns:
            str: æ—¥æœ¬èªè¦ç´„
        """
        # å…¨æ–‡ã‚’çµåˆã—ã¦ã‚ˆã‚Šå¤šãã®æƒ…å ±ã‚’æ´»ç”¨
        full_content = f"{title}. {description or ''} {content or ''}".strip()
        
        # å…·ä½“çš„ãªäº‹å®Ÿã‚’æŠ½å‡ºã™ã‚‹ãƒ‘ã‚¿ãƒ¼ãƒ³åˆ†æ
        summary = self._extract_key_facts(full_content)
        
        # 50æ–‡å­—åˆ¶é™
        if len(summary) > 50:
            summary = summary[:47] + "..."
        
        return summary
    
    def _extract_key_facts(self, content: str) -> str:
        """
        ãƒ‹ãƒ¥ãƒ¼ã‚¹å†…å®¹ã‹ã‚‰å…·ä½“çš„ãªäº‹å®Ÿã¨çµè«–ã‚’æŠ½å‡º
        
        Args:
            content (str): ãƒ‹ãƒ¥ãƒ¼ã‚¹å…¨æ–‡
        
        Returns:
            str: å…·ä½“çš„ãªäº‹å®Ÿã‚’å«ã‚€æ—¥æœ¬èªè¦ç´„
        """
        content_lower = content.lower()
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³1: ä¼šç¤¾é–“ã®å‹•ãï¼ˆè²·åã€ææºã€ç«¶äº‰ç­‰ï¼‰
        if "meta" in content_lower and "openai" in content_lower:
            if "bonus" in content_lower or "million" in content_lower:
                return "Metaã€OpenAIç¤¾å“¡ã«å·¨é¡ãƒœãƒ¼ãƒŠã‚¹æç¤ºã§å¼•ãæŠœã"
            elif "compete" in content_lower or "competition" in content_lower:
                return "Metaã¨OpenAIã€AIäººæã‚’å·¡ã‚Šç«¶äº‰æ¿€åŒ–"
            else:
                return "Metaã¨OpenAIé–“ã§æ–°ãŸãªå‹•ã"
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³2: æ–°è£½å“ãƒ»æ©Ÿèƒ½ç™ºè¡¨
        if any(word in content_lower for word in ["announce", "launch", "release", "unveil"]):
            # OpenAIé–¢é€£
            if "openai" in content_lower:
                if "gpt" in content_lower and ("5" in content or "new" in content_lower):
                    return "OpenAIã€æ–°å‹GPTãƒ¢ãƒ‡ãƒ«ã‚’ç™ºè¡¨"
                elif "pentagon" in content_lower or "defense" in content_lower:
                    return "OpenAIã€ç±³å›½é˜²ç·çœã¨å¥‘ç´„ç· çµ"
                else:
                    return "OpenAIã€æ–°ã‚µãƒ¼ãƒ“ã‚¹ãƒ»æ©Ÿèƒ½ã‚’ç™ºè¡¨"
            
            # Google/Geminié–¢é€£
            elif "gemini" in content_lower or "google" in content_lower:
                if "video" in content_lower:
                    return "Google Geminiã€å‹•ç”»åˆ†ææ©Ÿèƒ½ã‚’è¿½åŠ "
                elif "update" in content_lower:
                    return "Google Geminiã€æ©Ÿèƒ½å¼·åŒ–ç‰ˆã‚’ãƒªãƒªãƒ¼ã‚¹"
                else:
                    return "Googleã€AIæ–°æ©Ÿèƒ½ã‚’ç™ºè¡¨"
            
            # ãã®ä»–ä¼æ¥­
            elif "microsoft" in content_lower:
                return "Microsoftã€AIé–¢é€£æ–°è£½å“ã‚’ç™ºè¡¨"
            elif "amazon" in content_lower:
                return "Amazonã€AIæˆ¦ç•¥ã‚’ç™ºè¡¨"
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³3: æŠ•è³‡ãƒ»è³‡é‡‘èª¿é”
        if any(word in content_lower for word in ["funding", "investment", "raise", "million", "billion"]):
            if "startup" in content_lower:
                return "AIé–¢é€£ã‚¹ã‚¿ãƒ¼ãƒˆã‚¢ãƒƒãƒ—ã€å¤§å‹è³‡é‡‘èª¿é”"
            else:
                return "AIæ¥­ç•Œã§å¤§å‹æŠ•è³‡æ¡ˆä»¶ãŒç™ºç”Ÿ"
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³4: æŠ€è¡“é©æ–°ãƒ»ãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼
        if any(word in content_lower for word in ["breakthrough", "advancement", "improve", "enhance"]):
            if "reasoning" in content_lower or "logic" in content_lower:
                return "AIæ¨è«–èƒ½åŠ›ãŒå¤§å¹…å‘ä¸Šã€æ–°æŠ€è¡“ã‚’é–‹ç™º"
            elif "performance" in content_lower:
                return "AIæ€§èƒ½å‘ä¸Šã€å‡¦ç†é€Ÿåº¦ãŒæ”¹å–„"
            else:
                return "AIæŠ€è¡“ã§æ–°ãŸãªãƒ–ãƒ¬ã‚¤ã‚¯ã‚¹ãƒ«ãƒ¼"
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³5: äººäº‹ãƒ»çµ„ç¹”å¤‰æ›´
        if any(word in content_lower for word in ["hire", "employee", "ceo", "executive"]):
            return "AIä¼æ¥­ã§é‡è¦äººäº‹ã€çµ„ç¹”ä½“åˆ¶ã‚’å¤‰æ›´"
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³6: è¦åˆ¶ãƒ»æ”¿ç­–
        if any(word in content_lower for word in ["regulation", "policy", "government", "law"]):
            return "AIè¦åˆ¶ãƒ»æ”¿ç­–ã«é–¢ã™ã‚‹é‡è¦å‹•å‘"
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³7: ãƒ‘ãƒ¼ãƒˆãƒŠãƒ¼ã‚·ãƒƒãƒ—ãƒ»ææº
        if any(word in content_lower for word in ["partnership", "collaborate", "team up"]):
            companies = self._extract_companies(content)
            if len(companies) >= 2:
                return f"{companies[0]}ã¨{companies[1]}ãŒææº"
            else:
                return "AIæ¥­ç•Œã§æ–°ãŸãªææºãŒç™ºè¡¨"
        
        # ãƒ‘ã‚¿ãƒ¼ãƒ³8: PyPI/GitHubç­‰é–‹ç™ºãƒ„ãƒ¼ãƒ«
        if "pypi" in content_lower or "github" in content_lower:
            return "AIé–‹ç™ºãƒ„ãƒ¼ãƒ«ã€æ–°ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒå…¬é–‹"
        
        # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰æœ€é‡è¦æƒ…å ±ã‚’æŠ½å‡º
        return self._extract_from_title(content)
    
    def _extract_companies(self, content: str) -> List[str]:
        """
        æ–‡ç« ã‹ã‚‰ä¼æ¥­åã‚’æŠ½å‡º
        
        Args:
            content (str): ãƒ‹ãƒ¥ãƒ¼ã‚¹å†…å®¹
        
        Returns:
            List[str]: æŠ½å‡ºã•ã‚ŒãŸä¼æ¥­åãƒªã‚¹ãƒˆ
        """
        companies = []
        company_names = [
            ("OpenAI", "OpenAI"), ("Google", "Google"), ("Meta", "Meta"), 
            ("Microsoft", "Microsoft"), ("Amazon", "Amazon"), ("Apple", "Apple"),
            ("Anthropic", "Anthropic"), ("Tesla", "Tesla"), ("xAI", "xAI")
        ]
        
        content_lower = content.lower()
        for english, japanese in company_names:
            if english.lower() in content_lower:
                companies.append(japanese)
        
        return companies
    
    def _extract_from_title(self, content: str) -> str:
        """
        ã‚¿ã‚¤ãƒˆãƒ«ã‹ã‚‰é‡è¦æƒ…å ±ã‚’æŠ½å‡ºã—ã¦ã‚ˆã‚Šæ„å‘³ã®ã‚ã‚‹è¦ç´„ã‚’ä½œæˆ
        
        Args:
            content (str): ãƒ‹ãƒ¥ãƒ¼ã‚¹å†…å®¹
        
        Returns:
            str: æ„å‘³ã®ã‚ã‚‹è¦ç´„
        """
        # ã‚¿ã‚¤ãƒˆãƒ«ã®æœ€åˆã®éƒ¨åˆ†ï¼ˆé€šå¸¸æœ€ã‚‚é‡è¦ï¼‰ã‚’å–å¾—
        title = content.split('.')[0]
        
        # ä¼æ¥­åã‚’ç‰¹å®š
        companies = self._extract_companies(title)
        company = companies[0] if companies else "AIä¼æ¥­"
        
        # ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒ™ãƒ¼ã‚¹ã®å‹•ä½œæŠ½å‡º
        title_lower = title.lower()
        
        if "warn" in title_lower or "warning" in title_lower:
            return f"{company}CEOã€AIé–¢é€£ã§é‡è¦è­¦å‘Š"
        elif "cut" in title_lower and "time" in title_lower:
            return f"{company}ã€AIæ´»ç”¨ã§ä½œæ¥­æ™‚é–“ã‚’å¤§å¹…çŸ­ç¸®"
        elif "hire" in title_lower or "talent" in title_lower:
            return f"{company}ã€AIäººæç¢ºä¿ã«ç©æ¥µæŠ•è³‡"
        elif "video" in title_lower and "analysis" in title_lower:
            return f"{company}ã€å‹•ç”»AIåˆ†æã‚µãƒ¼ãƒ“ã‚¹é–‹å§‹"
        elif "chatbot" in title_lower:
            return f"{company}ã€ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆæŠ€è¡“ã‚’å‘ä¸Š"
        else:
            # æœ€å¾Œã®æ‰‹æ®µï¼šã‚¿ã‚¤ãƒˆãƒ«å‰åŠã®é‡è¦éƒ¨åˆ†ã‚’æ—¥æœ¬èªåŒ–
            important_part = title[:30].strip()
            return f"AIæ¥­ç•Œ: {important_part}..."
    
    def _clean_summary(self, summary: str) -> str:
        """
        è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        
        Args:
            summary (str): ç”Ÿã®è¦ç´„ãƒ†ã‚­ã‚¹ãƒˆ
        
        Returns:
            str: ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã•ã‚ŒãŸè¦ç´„
        """
        # ä¸è¦ãªå‰ç½®ãã‚„è¨˜å·ã‚’é™¤å»
        prefixes_to_remove = [
            "æ—¥æœ¬èªè¦ç´„:",
            "è¦ç´„:",
            "ã¾ã¨ã‚:",
            "æ¦‚è¦:",
            "Japanese Summary:",
            "Japanese:",
            "Summary:",
            "ã“ã®è¨˜äº‹ã¯",
            "ã“ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯",
            "**",
            "##",
            "- "
        ]
        
        for prefix in prefixes_to_remove:
            if summary.startswith(prefix):
                summary = summary[len(prefix):].strip()
        
        # æ”¹è¡Œã‚’é™¤å»
        summary = summary.replace('\n', ' ').replace('\r', ' ')
        
        # è¤‡æ•°ã‚¹ãƒšãƒ¼ã‚¹ã‚’å˜ä¸€ã‚¹ãƒšãƒ¼ã‚¹ã«
        import re
        summary = re.sub(r'\s+', ' ', summary)
        
        # 50æ–‡å­—åˆ¶é™
        if len(summary) > 50:
            summary = summary[:47] + "..."
        
        return summary.strip()
    
    def _fallback_summary(self, title: str, description: str) -> str:
        """
        ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯è¦ç´„ï¼ˆLLMãŒåˆ©ç”¨ã§ããªã„å ´åˆï¼‰
        æ”¹å–„ç‰ˆï¼šdescriptionã®å†…å®¹ã‚‚è€ƒæ…®
        
        Args:
            title (str): è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«
            description (str): è¨˜äº‹èª¬æ˜
        
        Returns:
            str: ç°¡æ˜“è¦ç´„
        """
        # descriptionã‹ã‚‰é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’æŠ½å‡º
        important_keywords = [
            "AI", "artificial intelligence", "machine learning", "ChatGPT", "GPT",
            "model", "technology", "startup", "funding", "release", "launch",
            "announce", "update", "improve", "enhance", "breakthrough"
        ]
        
        content = description if description else title
        
        # é‡è¦ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å«ã‚€æ–‡ã‚’å„ªå…ˆ
        sentences = content.split('.')
        best_sentence = ""
        
        for sentence in sentences:
            if any(keyword.lower() in sentence.lower() for keyword in important_keywords):
                best_sentence = sentence.strip()
                break
        
        if not best_sentence:
            best_sentence = sentences[0] if sentences else title
        
        # æ—¥æœ¬èªãƒ—ãƒ¬ãƒ•ã‚£ãƒƒã‚¯ã‚¹ã‚’è¿½åŠ 
        summary = f"AIæ¥­ç•Œ: {best_sentence}"
        
        if len(summary) > 47:
            summary = summary[:47] + "..."
        
        return summary
    
    def process_news_articles(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """
        ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒªã‚¹ãƒˆã‚’å‡¦ç†ã—ã¦æ—¥æœ¬èªè¦ç´„ã‚’è¿½åŠ 
        
        Args:
            articles (List[Dict]): ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒªã‚¹ãƒˆ
        
        Returns:
            List[Dict]: æ—¥æœ¬èªè¦ç´„ä»˜ããƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒªã‚¹ãƒˆ
        """
        processed_articles = []
        
        for i, article in enumerate(articles):
            processed_article = article.copy()
            
            print(f"ğŸ“ è¨˜äº‹ {i+1}/{len(articles)} ã‚’è¦ç´„ä¸­...")
            
            # æ—¥æœ¬èªè¦ç´„ã‚’ç”Ÿæˆï¼ˆcontentã‚‚å«ã‚ã‚‹ï¼‰
            summary_jp = self.generate_summary_japanese(
                title=article.get("title", ""),
                description=article.get("description", ""),
                url=article.get("url", ""),
                content=article.get("content", "")
            )
            
            processed_article["summary_jp"] = summary_jp
            processed_articles.append(processed_article)
            
            # APIå‘¼ã³å‡ºã—é–“éš”ã‚’èª¿æ•´ï¼ˆOllamaã‚µãƒ¼ãƒãƒ¼ã®è² è·è»½æ¸›ï¼‰
            if i < len(articles) - 1:
                time.sleep(1)
        
        return processed_articles
    
    def get_status(self) -> Dict[str, Any]:
        """
        ãƒ­ãƒ¼ã‚«ãƒ«LLMæ©Ÿèƒ½ã®çŠ¶æ…‹ã‚’å–å¾—
        
        Returns:
            Dict: çŠ¶æ…‹æƒ…å ±
        """
        return {
            "enabled": self.enabled,
            "model_name": self.model_name,
            "ollama_available": self.available,
            "ollama_url": self.ollama_url,
            "thinking_mode": self.thinking_mode,
            "fallback_enabled": True
        }

    def generate_weekly_news_summary(self, articles: List[Dict[str, Any]]) -> str:
        """
        é€±ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹å…¨ä½“ã‚’æ—¥æœ¬èªã§300æ–‡å­—ç¨‹åº¦ã«ã‚µãƒãƒ©ã‚¤ã‚º
        
        Args:
            articles (List[Dict]): å‡¦ç†æ¸ˆã¿ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒªã‚¹ãƒˆ
        
        Returns:
            str: é€±é–“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µãƒãƒªãƒ¼ï¼ˆæ—¥æœ¬èªã€300æ–‡å­—ç¨‹åº¦ï¼‰
        """
        if not articles:
            return "ä»Šé€±ã¯æ³¨ç›®ã™ã¹ãAIæ¥­ç•Œãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        # è¨˜äº‹ã®ä¸»è¦æƒ…å ±ã‚’æŠ½å‡º
        titles = [article.get("title", "") for article in articles[:8]]  # ä¸Šä½8ä»¶
        summaries = [article.get("summary_jp", "") for article in articles[:8]]
        
        if not self.enabled or not self.available:
            return self._create_fallback_weekly_summary(articles)
        
        try:
            return self._generate_weekly_summary_with_qwen3(titles, summaries)
        except Exception as e:
            print(f"é€±é–“ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return self._create_fallback_weekly_summary(articles)
    
    def _generate_weekly_summary_with_qwen3(self, titles: List[str], summaries: List[str]) -> str:
        """
        Qwen3ã‚’ä½¿ç”¨ã—ã¦é€±é–“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆ
        
        Args:
            titles (List[str]): è¨˜äº‹ã‚¿ã‚¤ãƒˆãƒ«ãƒªã‚¹ãƒˆ
            summaries (List[str]): å€‹åˆ¥è¨˜äº‹è¦ç´„ãƒªã‚¹ãƒˆ
        
        Returns:
            str: é€±é–“ã‚µãƒãƒªãƒ¼
        """
        # è¨˜äº‹æƒ…å ±ã‚’æ•´ç†
        article_info = []
        for i, (title, summary) in enumerate(zip(titles, summaries)):
            if title and summary:
                article_info.append(f"{i+1}. {summary}")
        
        articles_text = "\n".join(article_info[:6])  # ä¸Šä½6ä»¶
        
        prompt = f"""<|user|>
ä»¥ä¸‹ã¯ä»Šé€±ã®AIæ¥­ç•Œãƒ‹ãƒ¥ãƒ¼ã‚¹ã§ã™ã€‚å…¨ä½“çš„ãªãƒˆãƒ¬ãƒ³ãƒ‰ã¨é‡è¦ãªãƒã‚¤ãƒ³ãƒˆã‚’æ—¥æœ¬èªã§300æ–‡å­—ä»¥å†…ã§ã¾ã¨ã‚ã¦ãã ã•ã„ã€‚

ä»Šé€±ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹:
{articles_text}

æ¥­ç•Œå…¨ä½“ã®å‹•å‘ã€ä¸»è¦ä¼æ¥­ã®å‹•ãã€æ³¨ç›®ã™ã¹ãæŠ€è¡“ãƒˆãƒ¬ãƒ³ãƒ‰ã‚’å«ã‚ã¦é€±é–“ã‚µãƒãƒªãƒ¼ã‚’ä½œæˆã—ã¦ãã ã•ã„ã€‚
<|assistant|>
"""
        
        payload = {
            "model": self.model_name,
            "prompt": prompt,
            "stream": False,
            "options": {
                "temperature": 0.4,
                "top_p": 0.9,
                "num_predict": 400,
                "stop": ["<|user|>", "<|assistant|>"],
                "repeat_penalty": 1.1
            }
        }
        
        try:
            print("ğŸ” é€±é–“ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆä¸­...")
            
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json=payload,
                timeout=45
            )
            
            if response.status_code == 200:
                result = response.json()
                raw_summary = result.get('response', '').strip()
                
                # thinking modeã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                if '<think>' in raw_summary:
                    if '</think>' in raw_summary:
                        raw_summary = raw_summary.split('</think>')[-1].strip()
                    else:
                        return self._create_fallback_weekly_summary_from_summaries(summaries)
                
                # ã‚µãƒãƒªãƒ¼ã®ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
                summary = self._clean_weekly_summary(raw_summary)
                
                # å“è³ªãƒã‚§ãƒƒã‚¯
                if len(summary) > 50 and any(char in summary for char in 'ã‚ã„ã†ãˆãŠã‹ããã‘ã“'):
                    return summary
                else:
                    return self._create_fallback_weekly_summary_from_summaries(summaries)
            else:
                return self._create_fallback_weekly_summary_from_summaries(summaries)
                
        except Exception as e:
            print(f"é€±é–“ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            return self._create_fallback_weekly_summary_from_summaries(summaries)
    
    def _create_fallback_weekly_summary(self, articles: List[Dict[str, Any]]) -> str:
        """
        ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯é€±é–“ã‚µãƒãƒªãƒ¼ä½œæˆ
        
        Args:
            articles (List[Dict]): ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ãƒªã‚¹ãƒˆ
        
        Returns:
            str: é€±é–“ã‚µãƒãƒªãƒ¼
        """
        summaries = [article.get("summary_jp", "") for article in articles if article.get("summary_jp")]
        return self._create_fallback_weekly_summary_from_summaries(summaries)
    
    def _create_fallback_weekly_summary_from_summaries(self, summaries: List[str]) -> str:
        """
        å€‹åˆ¥è¦ç´„ã‹ã‚‰é€±é–“ã‚µãƒãƒªãƒ¼ã‚’æ§‹ç¯‰
        
        Args:
            summaries (List[str]): å€‹åˆ¥è¨˜äº‹è¦ç´„ãƒªã‚¹ãƒˆ
        
        Returns:
            str: é€±é–“ã‚µãƒãƒªãƒ¼
        """
        if not summaries:
            return "ä»Šé€±ã¯æ³¨ç›®ã™ã¹ãAIæ¥­ç•Œãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ã‚ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        # ä¸»è¦ãƒˆãƒ”ãƒƒã‚¯ã‚’åˆ†æ
        topics = {
            "ä¼æ¥­ç«¶äº‰": 0,
            "æŠ€è¡“ç™ºè¡¨": 0,
            "æŠ•è³‡ãƒ»è³‡é‡‘èª¿é”": 0,
            "äººäº‹ãƒ»çµ„ç¹”": 0,
            "è£½å“ãƒªãƒªãƒ¼ã‚¹": 0,
            "æ”¿åºœãƒ»è¦åˆ¶": 0
        }
        
        key_companies = set()
        
        for summary in summaries[:8]:
            summary_lower = summary.lower()
            
            # ãƒˆãƒ”ãƒƒã‚¯åˆ†æ
            if any(word in summary for word in ["å¼•ãæŠœã", "ç«¶äº‰", "ãƒœãƒ¼ãƒŠã‚¹"]):
                topics["ä¼æ¥­ç«¶äº‰"] += 1
            if any(word in summary for word in ["ç™ºè¡¨", "ãƒªãƒªãƒ¼ã‚¹", "é–‹å§‹"]):
                topics["æŠ€è¡“ç™ºè¡¨"] += 1
            if any(word in summary for word in ["æŠ•è³‡", "è³‡é‡‘èª¿é”", "å¤§å‹"]):
                topics["æŠ•è³‡ãƒ»è³‡é‡‘èª¿é”"] += 1
            if any(word in summary for word in ["äººäº‹", "çµ„ç¹”", "CEO"]):
                topics["äººäº‹ãƒ»çµ„ç¹”"] += 1
            if any(word in summary for word in ["ã‚µãƒ¼ãƒ“ã‚¹", "æ©Ÿèƒ½", "å‹•ç”»"]):
                topics["è£½å“ãƒªãƒªãƒ¼ã‚¹"] += 1
            if any(word in summary for word in ["æ”¿åºœ", "è¦åˆ¶", "æ”¿ç­–"]):
                topics["æ”¿åºœãƒ»è¦åˆ¶"] += 1
            
            # ä¼æ¥­åæŠ½å‡º
            for company in ["OpenAI", "Meta", "Google", "Microsoft", "Amazon", "Apple"]:
                if company in summary:
                    key_companies.add(company)
        
        # ä¸»è¦ãƒˆãƒ”ãƒƒã‚¯ã‚’ç‰¹å®š
        main_topic = max(topics.items(), key=lambda x: x[1])
        
        # ã‚µãƒãƒªãƒ¼æ§‹ç¯‰
        companies_str = "ã€".join(list(key_companies)[:3]) if key_companies else "AIé–¢é€£ä¼æ¥­"
        
        if main_topic[1] > 0:
            if main_topic[0] == "ä¼æ¥­ç«¶äº‰":
                summary_text = f"ä»Šé€±ã®AIæ¥­ç•Œã§ã¯{companies_str}ã‚’ä¸­å¿ƒã¨ã—ãŸäººæç²å¾—ç«¶äº‰ãŒæ¿€åŒ–ã€‚"
            elif main_topic[0] == "æŠ€è¡“ç™ºè¡¨":
                summary_text = f"ä»Šé€±ã¯{companies_str}ã«ã‚ˆã‚‹æ–°æŠ€è¡“ãƒ»ã‚µãƒ¼ãƒ“ã‚¹ç™ºè¡¨ãŒç›¸æ¬¡ãã€AIæ©Ÿèƒ½ã®å®Ÿç”¨åŒ–ãŒåŠ é€Ÿã€‚"
            elif main_topic[0] == "æŠ•è³‡ãƒ»è³‡é‡‘èª¿é”":
                summary_text = f"AIæ¥­ç•Œã§å¤§å‹æŠ•è³‡æ¡ˆä»¶ãŒæ´»ç™ºåŒ–ã€‚{companies_str}é–¢é€£ã®è³‡é‡‘èª¿é”ãƒ‹ãƒ¥ãƒ¼ã‚¹ãŒç›®ç«‹ã¤ã€‚"
            elif main_topic[0] == "è£½å“ãƒªãƒªãƒ¼ã‚¹":
                summary_text = f"{companies_str}ãŒæ–°è£½å“ãƒ»æ©Ÿèƒ½ã‚’ç›¸æ¬¡ã„ã§ç™ºè¡¨ã€‚ç‰¹ã«å‹•ç”»AIåˆ†æãªã©ã®å®Ÿç”¨çš„ã‚µãƒ¼ãƒ“ã‚¹ãŒæ³¨ç›®ã€‚"
            else:
                summary_text = f"ä»Šé€±ã®AIæ¥­ç•Œã§ã¯{companies_str}ã‚’ä¸­å¿ƒã¨ã—ãŸå‹•ããŒæ´»ç™ºåŒ–ã€‚"
        else:
            summary_text = f"ä»Šé€±ã®AIæ¥­ç•Œã§ã¯{companies_str}ã«ã‚ˆã‚‹æ§˜ã€…ãªå–ã‚Šçµ„ã¿ãŒå ±å‘Šã•ã‚Œã€"
        
        # å…·ä½“çš„ãªå‹•å‘ã‚’è¿½åŠ 
        specific_trends = []
        for summary in summaries[:3]:
            if len(summary) > 10 and summary not in summary_text:
                specific_trends.append(summary.replace("AIæ¥­ç•Œ: ", "").replace("...", ""))
        
        if specific_trends:
            summary_text += f" ä¸»ãªå‹•å‘ã¨ã—ã¦{specific_trends[0]}ãªã©ã€"
            if len(specific_trends) > 1:
                summary_text += f"{specific_trends[1]}ã¨ã„ã£ãŸå±•é–‹ã‚‚è¦‹ã‚‰ã‚Œã‚‹ã€‚"
            else:
                summary_text += "æ¥­ç•Œå…¨ä½“ã®å¤‰åŒ–ãŒç¶™ç¶šã—ã¦ã„ã‚‹ã€‚"
        else:
            summary_text += " æ¥­ç•Œå…¨ä½“ã§AIæŠ€è¡“ã®å®Ÿç”¨åŒ–ã¨ç«¶äº‰ãŒåŠ é€Ÿã—ã¦ã„ã‚‹çŠ¶æ³ãŒç¶šã„ã¦ã„ã‚‹ã€‚"
        
        # 300æ–‡å­—åˆ¶é™
        if len(summary_text) > 300:
            summary_text = summary_text[:297] + "..."
        
        return summary_text
    
    def _clean_weekly_summary(self, summary: str) -> str:
        """
        é€±é–“ã‚µãƒãƒªãƒ¼ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
        
        Args:
            summary (str): ç”Ÿã®é€±é–“ã‚µãƒãƒªãƒ¼
        
        Returns:
            str: ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã•ã‚ŒãŸé€±é–“ã‚µãƒãƒªãƒ¼
        """
        # ä¸è¦ãªå‰ç½®ãã‚’é™¤å»
        prefixes_to_remove = [
            "ä»Šé€±ã®AIæ¥­ç•Œãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’ã¾ã¨ã‚ã‚‹ã¨ã€",
            "é€±é–“ã‚µãƒãƒªãƒ¼:",
            "ã¾ã¨ã‚:",
            "æ¦‚è¦:",
            "ä»Šé€±ã¯ã€"
        ]
        
        for prefix in prefixes_to_remove:
            if summary.startswith(prefix):
                summary = summary[len(prefix):].strip()
        
        # æ”¹è¡Œã‚’é™¤å»
        summary = summary.replace('\n', ' ').replace('\r', ' ')
        
        # è¤‡æ•°ã‚¹ãƒšãƒ¼ã‚¹ã‚’å˜ä¸€ã‚¹ãƒšãƒ¼ã‚¹ã«
        import re
        summary = re.sub(r'\s+', ' ', summary)
        
        # 300æ–‡å­—åˆ¶é™
        if len(summary) > 300:
            summary = summary[:297] + "..."
        
        return summary.strip()


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    summarizer = LocalLLMSummarizer()
    
    print("=== ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼ˆQwen3ï¼‰è¦ç´„æ©Ÿèƒ½ãƒ†ã‚¹ãƒˆ ===")
    status = summarizer.get_status()
    print(f"çŠ¶æ…‹: {status}")
    
    # ã‚µãƒ³ãƒ—ãƒ«è¨˜äº‹ã§ãƒ†ã‚¹ãƒˆ
    sample_articles = [
        {
            "title": "OpenAI announces new GPT-5 model with enhanced reasoning capabilities",
            "description": "The latest AI model from OpenAI shows significant improvements in logical reasoning, mathematics, and code generation, outperforming previous versions in benchmark tests.",
            "url": "https://example.com/openai-gpt5",
            "keyword": "OpenAI"
        },
        {
            "title": "Google's Gemini AI now supports video analysis and understanding",
            "description": "Google has updated its Gemini AI model to include advanced video processing capabilities, allowing users to analyze and interact with video content in natural language.",
            "url": "https://example.com/gemini-video",
            "keyword": "Gemini"
        }
    ]
    
    if summarizer.enabled:
        print("\n=== è¦ç´„ãƒ†ã‚¹ãƒˆé–‹å§‹ ===")
        processed = summarizer.process_news_articles(sample_articles)
        
        print("\n=== å‡¦ç†çµæœ ===")
        for article in processed:
            print(f"ğŸ“° ã‚¿ã‚¤ãƒˆãƒ«: {article['title'][:50]}...")
            print(f"ğŸ‡¯ğŸ‡µ æ—¥æœ¬èªè¦ç´„: {article.get('summary_jp', 'æœªç”Ÿæˆ')}")
            print("---")
    else:
        print("âš ï¸  ãƒ­ãƒ¼ã‚«ãƒ«LLMæ©Ÿèƒ½ãŒç„¡åŠ¹ã¾ãŸã¯OllamaãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚")
        print("Ollamaã‚’ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã—ã€ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„:")
        print("  ollama pull qwen3:8b")
        print("  ollama serve") 