#!/usr/bin/env python3
"""
ä¼æ¥­åˆ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ã‚¯ãƒ©ã‚¹ï¼ˆç„¡æ–™ç‰ˆï¼‰
RSSã€Web scrapingã€NewsAPIã‚’çµ„ã¿åˆã‚ã›ã¦ä¼æ¥­ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’åé›†
"""

import yaml
import requests
import feedparser
import time
import json
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse
import re
import os
import sys

class CompanyNewsCollector:
    def __init__(self, config_path="config/target_companies.yaml", newsapi_key=None):
        """
        åˆæœŸåŒ–
        
        Args:
            config_path: ä¼æ¥­è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            newsapi_key: NewsAPIã‚­ãƒ¼
        """
        self.companies = self.load_company_config(config_path)
        self.newsapi_key = newsapi_key or os.getenv('NEWSAPI_KEY') or "5d88b85486d641faba9a410aca9c138b"
        
        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°è¨­å®šï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ï¼‰
        self.delay_seconds = 2
        self.timeout_seconds = 15
        self.max_retries = 2
        self.user_agent = 'WeeklyBrief-NewsCollector/1.0'
        
        # ã‚»ãƒƒã‚·ãƒ§ãƒ³è¨­å®š
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': self.user_agent,
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
        })
        
        # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
        self.last_newsapi_request = 0
        self.newsapi_min_interval = 1.5  # NewsAPIãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”ï¼ˆç§’ï¼‰
        
    def load_company_config(self, config_path: str) -> Dict:
        """ä¼æ¥­è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ï¼ˆçµ±åˆç‰ˆï¼‰"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                config = yaml.safe_load(f)
            
            # companiesã¨additional_targetsã‚’çµ±åˆ
            all_companies = {}
            
            if 'companies' in config:
                all_companies.update(config['companies'])
            
            if 'additional_targets' in config:
                all_companies.update(config['additional_targets'])
            
            # è¨­å®šã®æ­£è¦åŒ–ï¼ˆRSSãƒ•ã‚£ãƒ¼ãƒ‰è¨­å®šã‚’çµ±ä¸€ï¼‰
            for company_id, company_info in all_companies.items():
                # blog_rss ã‚’ rss_feeds ã«å¤‰æ›
                if 'blog_rss' in company_info:
                    company_info.setdefault('rss_feeds', []).append(company_info['blog_rss'])
                
                # blog_url ã‚’ blog_urls ã«å¤‰æ›
                if 'blog_url' in company_info:
                    company_info.setdefault('blog_urls', []).append(company_info['blog_url'])
                
                # å„ç¨®URLã‚’ãƒªã‚¹ãƒˆã«å¤‰æ›
                for url_key in ['news_url', 'research_url', 'deepmind_url']:
                    if url_key in company_info:
                        company_info.setdefault('news_urls', []).append(company_info[url_key])
            
            print(f"âœ… ä¼æ¥­è¨­å®šèª­ã¿è¾¼ã¿å®Œäº†: {len(all_companies)}ç¤¾")
            return all_companies
            
        except Exception as e:
            print(f"âŒ ä¼æ¥­è¨­å®šèª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return {}
    
    def collect_all_company_news(self, days_back: int = 7) -> List[Dict[str, Any]]:
        """å…¨ä¼æ¥­ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ï¼ˆçµ±åˆç‰ˆï¼‰"""
        all_news = []
        
        print(f"ğŸ¢ ä¼æ¥­ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†é–‹å§‹ - éå»{days_back}æ—¥é–“")
        
        # çµ±åˆã•ã‚ŒãŸä¼æ¥­ãƒªã‚¹ãƒˆã‚’å‡¦ç†
        for company_id, company_info in self.companies.items():
            try:
                company_news = self.collect_company_news(company_id, company_info, days_back)
                all_news.extend(company_news)
            except Exception as e:
                print(f"âŒ {company_info.get('name', company_id)} åé›†ã‚¨ãƒ©ãƒ¼: {e}")
                continue
        
        print(f"\nğŸ‰ å…¨ä¼æ¥­åé›†å®Œäº†: åˆè¨ˆ{len(all_news)}ä»¶")
        return all_news
    
    def collect_company_news(self, company_id: str, company_info: Dict, days_back: int) -> List[Dict[str, Any]]:
        """ç‰¹å®šä¼æ¥­ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
        all_items = []
        
        print(f"\nğŸ“Š {company_info['name']} ã®åé›†ä¸­...")
        
        # RSS ãƒ•ã‚£ãƒ¼ãƒ‰ãŒã‚ã‚‹å ´åˆã¯å„ªå…ˆ
        if company_info.get('rss_feeds'):
            rss_items = self.collect_rss_feeds(company_id, company_info, days_back)
            all_items.extend(rss_items)
        
        # Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        if company_info.get('blog_urls') or company_info.get('news_urls'):
            web_items = self.collect_web_content(company_id, company_info, days_back)
            all_items.extend(web_items)
        
        # NewsAPIã¯ä¸»è¦ä¼æ¥­ã®ã¿ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼‰
        priority_companies = ['openai', 'google_ai', 'anthropic', 'microsoft', 'meta']
        if company_id in priority_companies and company_info.get('keywords'):
            newsapi_items = self.collect_newsapi_content(company_id, company_info, days_back)
            all_items.extend(newsapi_items)
        elif company_info.get('keywords'):
            print(f"    âš ï¸  NewsAPI: ä¸»è¦ä¼æ¥­ä»¥å¤–ã¯ã‚¹ã‚­ãƒƒãƒ—ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼‰")
        
        # é‡è¤‡é™¤å»
        unique_items = self.remove_duplicates(all_items)
        
        print(f"                                        âœ… {company_info['name']}: {len(unique_items)}ä»¶åé›†")
        return unique_items
    
    def collect_rss_feeds(self, company_id: str, company_info: Dict, days_back: int) -> List[Dict[str, Any]]:
        """RSS ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†"""
        items = []
        
        # ãƒ¡ã‚¤ãƒ³RSS
        if company_info.get('blog_rss'):
            rss_items = self.fetch_rss_feed(company_info['blog_rss'], company_id, days_back)
            items.extend(rss_items)
        
        # è¿½åŠ RSSï¼ˆDeepMindç­‰ï¼‰
        if company_info.get('deepmind_rss'):
            rss_items = self.fetch_rss_feed(company_info['deepmind_rss'], company_id, days_back)
            items.extend(rss_items)
        
        return items
    
    def fetch_rss_feed(self, rss_url: str, company_id: str, days_back: int) -> List[Dict[str, Any]]:
        """
        RSS ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—ãƒ»è§£æï¼ˆçµ±ä¸€ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ç‰ˆï¼‰
        
        Args:
            rss_url: RSS URL
            company_id: ä¼æ¥­ID
            days_back: éå»ä½•æ—¥åˆ†ã‹
            
        Returns:
            è¨˜äº‹ãƒªã‚¹ãƒˆ
        """
        items = []
        
        for attempt in range(self.max_retries):
            try:
                print(f"  ğŸ“¡ RSSå–å¾—ä¸­: {rss_url}")
                
                response = self.session.get(rss_url, timeout=self.timeout_seconds)
                response.raise_for_status()
                
                feed = feedparser.parse(response.content)
                
                if feed.entries:
                    for entry in feed.entries:
                        # è¨˜äº‹æƒ…å ±ã‚’æ§‹é€ åŒ–
                        item = {
                            'title': entry.get('title', ''),
                            'url': entry.get('link', ''),
                            'published_at': self.parse_date(entry),
                            'summary': entry.get('summary', ''),
                            'company_id': company_id,
                            'source_type': 'rss',
                            'source_url': rss_url,
                            'content': ''  # ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã®ãŸã‚ç„¡åŠ¹åŒ–
                        }
                        
                        items.append(item)
                
                # çµ±ä¸€ã•ã‚ŒãŸæ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’é©ç”¨
                filtered_items = self.filter_by_date_range(items, days_back)
                
                print(f"    âœ… RSSè§£æå®Œäº†: {len(filtered_items)}ä»¶ï¼ˆãƒ•ã‚£ãƒ«ã‚¿å¾Œï¼‰")
                return filtered_items
                
            except Exception as e:
                print(f"    âŒ RSSå–å¾—ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt + 1}/{self.max_retries}): {e}")
                if attempt == self.max_retries - 1:
                    return []
                time.sleep(2 ** attempt)
        
        return []
    
    def collect_web_content(self, company_id: str, company_info: Dict, days_back: int) -> List[Dict[str, Any]]:
        """Web scraping ã§ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†"""
        items = []
        
        # ãƒ–ãƒ­ã‚°ãƒšãƒ¼ã‚¸
        if company_info.get('blog_url'):
            blog_items = self.scrape_blog_page(company_info['blog_url'], company_id, days_back)
            items.extend(blog_items)
        
        # ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸
        if company_info.get('news_url'):
            news_items = self.scrape_news_page(company_info['news_url'], company_id, days_back)
            items.extend(news_items)
        
        # ç ”ç©¶ãƒšãƒ¼ã‚¸
        if company_info.get('research_url'):
            research_items = self.scrape_research_page(company_info['research_url'], company_id, days_back)
            items.extend(research_items)
        
        return items
    
    def scrape_blog_page(self, blog_url: str, company_id: str, days_back: int) -> List[Dict[str, Any]]:
        """ãƒ–ãƒ­ã‚°ãƒšãƒ¼ã‚¸ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆæ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¼·åŒ–ç‰ˆï¼‰"""
        items = []
        
        try:
            print(f"  ğŸ•·ï¸  ãƒ–ãƒ­ã‚°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°: {blog_url}")
            
            response = self.session.get(blog_url, timeout=self.timeout_seconds)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ä¸€èˆ¬çš„ãªãƒ–ãƒ­ã‚°è¨˜äº‹ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦è¡Œ
            article_selectors = [
                'article',
                '.post',
                '.blog-post',
                '.entry',
                '.news-item',
                '.article-item',
                '[class*="post"]',
                '[class*="article"]'
            ]
            
            articles = []
            for selector in article_selectors:
                articles = soup.select(selector)
                if articles:
                    break
            
            if not articles:
                # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ãƒªãƒ³ã‚¯ã‚’æ¢ã™
                articles = soup.find_all('a', href=True)
            
            cutoff_date = datetime.now() - timedelta(days=days_back)
            valid_items = []
            
            for article in articles[:10]:  # å‡¦ç†ä»¶æ•°ã‚’10ä»¶ã«åˆ¶é™ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ï¼‰
                try:
                    # ã‚¿ã‚¤ãƒˆãƒ«ã¨URLã‚’æŠ½å‡º
                    if article.name == 'a':
                        title = article.get_text(strip=True)
                        url = article['href']
                    else:
                        title_elem = article.find(['h1', 'h2', 'h3', 'h4', 'a'])
                        if not title_elem:
                            continue
                        title = title_elem.get_text(strip=True)
                        
                        url_elem = article.find('a', href=True)
                        if not url_elem:
                            continue
                        url = url_elem['href']
                    
                    # ç›¸å¯¾URLã‚’çµ¶å¯¾URLã«å¤‰æ›
                    if url.startswith('/'):
                        url = urljoin(blog_url, url)
                    
                    # å¤–éƒ¨ãƒªãƒ³ã‚¯ã‚’ã‚¹ã‚­ãƒƒãƒ—
                    if not self.is_same_domain(url, blog_url):
                        continue
                    
                    # è¨˜äº‹ã®å…¬é–‹æ—¥ã‚’æŠ½å‡ºï¼ˆå¼·åŒ–ç‰ˆï¼‰
                    published_date = self.extract_article_date(article, url)
                    
                    # è¨˜äº‹æƒ…å ±ã‚’æ§‹é€ åŒ–
                    item = {
                        'title': title,
                        'url': url,
                        'published_at': published_date,
                        'summary': '',
                        'company_id': company_id,
                        'source_type': 'web_scraping',
                        'source_url': blog_url,
                        'content': ''
                    }
                    
                    valid_items.append(item)
                    
                except Exception as e:
                    continue
            
            # æ—¥ä»˜ã«ã‚ˆã‚‹äº‹å¾Œãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
            items = self.filter_by_date_range(valid_items, days_back)
            
            print(f"    âœ… ãƒ–ãƒ­ã‚°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Œäº†: {len(items)}ä»¶ï¼ˆãƒ•ã‚£ãƒ«ã‚¿å¾Œï¼‰")
            
        except Exception as e:
            print(f"    âŒ ãƒ–ãƒ­ã‚°ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
        
        return items
    
    def scrape_news_page(self, news_url: str, company_id: str, days_back: int) -> List[Dict[str, Any]]:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒšãƒ¼ã‚¸ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆãƒ–ãƒ­ã‚°ã¨åŒæ§˜ã®å‡¦ç†ï¼‰"""
        return self.scrape_blog_page(news_url, company_id, days_back)
    
    def scrape_research_page(self, research_url: str, company_id: str, days_back: int) -> List[Dict[str, Any]]:
        """ç ”ç©¶ãƒšãƒ¼ã‚¸ã®ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼ˆãƒ–ãƒ­ã‚°ã¨åŒæ§˜ã®å‡¦ç†ï¼‰"""
        return self.scrape_blog_page(research_url, company_id, days_back)
    
    def collect_newsapi_content(self, company_id: str, company_info: Dict, days_back: int) -> List[Dict[str, Any]]:
        """NewsAPI ã§ä¼æ¥­é–¢é€£ãƒ‹ãƒ¥ãƒ¼ã‚¹åé›†ï¼ˆãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ç‰ˆï¼‰"""
        items = []
        
        if not self.newsapi_key or not company_info.get('keywords'):
            print(f"    âš ï¸  NewsAPIã‚­ãƒ¼ã¾ãŸã¯ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒæœªè¨­å®š")
            return items
        
        try:
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–ï¼šå‰å›ãƒªã‚¯ã‚¨ã‚¹ãƒˆã‹ã‚‰é–“éš”ã‚’ã‚ã‘ã‚‹
            elapsed = time.time() - self.last_newsapi_request
            if elapsed < self.newsapi_min_interval:
                wait_time = self.newsapi_min_interval - elapsed
                print(f"    â³ ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–: {wait_time:.1f}ç§’å¾…æ©Ÿ")
                time.sleep(wait_time)
            
            print(f"  ğŸ“° NewsAPIæ¤œç´¢: {company_info['keywords']}")
            
            # ä¼æ¥­å + ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã§æ¤œç´¢
            search_query = f"{company_info['name']} OR " + " OR ".join(company_info['keywords'])
            
            from_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')
            
            url = "https://newsapi.org/v2/everything"
            params = {
                "q": search_query,
                "apiKey": self.newsapi_key,
                "language": "en",
                "sortBy": "publishedAt",
                "pageSize": 10,
                "from": from_date
            }
            
            self.last_newsapi_request = time.time()
            response = self.session.get(url, params=params, timeout=self.timeout_seconds)
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ã‚¨ãƒ©ãƒ¼ã®è©³ç´°å‡¦ç†
            if response.status_code == 429:
                print(f"    âš ï¸  NewsAPIãƒ¬ãƒ¼ãƒˆåˆ¶é™ã«é”ã—ã¾ã—ãŸ - ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                return []
            elif response.status_code == 401:
                print(f"    âš ï¸  NewsAPIã‚­ãƒ¼ãŒç„¡åŠ¹ã§ã™ - ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                return []
            elif response.status_code != 200:
                print(f"    âš ï¸  NewsAPIã‚¨ãƒ©ãƒ¼: {response.status_code} - ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
                return []
            
            data = response.json()
            
            if data.get('status') == 'ok' and data.get('articles'):
                for article in data['articles']:
                    # è¨˜äº‹æƒ…å ±ã‚’æ§‹é€ åŒ–
                    item = {
                        'title': article.get("title", ""),
                        'url': article.get("url", ""),
                        'published_at': article.get("publishedAt", ""),
                        'summary': article.get("description", ""),
                        'company_id': company_id,
                        'source_type': 'newsapi',
                        'source_url': 'NewsAPI',
                        'content': article.get("content", "")
                    }
                    items.append(item)
            
            # çµ±ä¸€ã•ã‚ŒãŸæ—¥ä»˜ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’è¿½åŠ é©ç”¨ï¼ˆäºŒé‡ãƒã‚§ãƒƒã‚¯ï¼‰
            filtered_items = self.filter_by_date_range(items, days_back)
            
            print(f"    âœ… NewsAPIæ¤œç´¢å®Œäº†: {len(filtered_items)}ä»¶ï¼ˆãƒ•ã‚£ãƒ«ã‚¿å¾Œï¼‰")
            return filtered_items
            
        except requests.exceptions.Timeout:
            print(f"    âš ï¸  NewsAPIã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ - ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            return []
        except requests.exceptions.RequestException as e:
            print(f"    âš ï¸  NewsAPIæ¥ç¶šã‚¨ãƒ©ãƒ¼: {str(e)[:50]}... - ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            return []
        except Exception as e:
            print(f"    âš ï¸  NewsAPIå‡¦ç†ã‚¨ãƒ©ãƒ¼: {str(e)[:50]}... - ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã™")
            return []
    
    def extract_content_from_url(self, url: str) -> str:
        """URLã‹ã‚‰è¨˜äº‹æœ¬æ–‡ã‚’æŠ½å‡ºï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        if not url:
            return ""
        
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ä¸€èˆ¬çš„ãªè¨˜äº‹æœ¬æ–‡ã®ã‚»ãƒ¬ã‚¯ã‚¿ã‚’è©¦è¡Œ
            content_selectors = [
                'article',
                '.content',
                '.post-content',
                '.entry-content',
                '.article-content',
                'main',
                '.main-content'
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    # ã‚¹ã‚¯ãƒªãƒ—ãƒˆã‚„ã‚¹ã‚¿ã‚¤ãƒ«ã‚’é™¤å»
                    for script in content_elem(["script", "style"]):
                        script.decompose()
                    
                    text = content_elem.get_text(strip=True)
                    if len(text) > 100:  # ååˆ†ãªé•·ã•ãŒã‚ã‚‹å ´åˆã®ã¿
                        return text[:1000]  # æœ€åˆã®1000æ–‡å­—
            
            return ""
            
        except Exception:
            return ""
    
    def remove_duplicates(self, items: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """é‡è¤‡è¨˜äº‹ã®é™¤å»"""
        seen_urls = set()
        seen_titles = set()
        unique_items = []
        
        for item in items:
            url = item.get('url', '')
            title = item.get('title', '').lower().strip()
            
            # URLé‡è¤‡ãƒã‚§ãƒƒã‚¯
            if url and url in seen_urls:
                continue
            
            # ã‚¿ã‚¤ãƒˆãƒ«é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆé¡ä¼¼åº¦ãƒ™ãƒ¼ã‚¹ï¼‰
            is_duplicate = False
            for seen_title in seen_titles:
                if self.calculate_similarity(title, seen_title) > 0.8:
                    is_duplicate = True
                    break
            
            if is_duplicate:
                continue
            
            # é‡è¤‡ãªã—ã®å ´åˆè¿½åŠ 
            unique_items.append(item)
            if url:
                seen_urls.add(url)
            if title:
                seen_titles.add(title)
        
        return unique_items
    
    def calculate_similarity(self, text1: str, text2: str) -> float:
        """ãƒ†ã‚­ã‚¹ãƒˆã®é¡ä¼¼åº¦è¨ˆç®—ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        if not text1 or not text2:
            return 0.0
        
        # å˜èªãƒ¬ãƒ™ãƒ«ã§ã®é¡ä¼¼åº¦
        words1 = set(text1.lower().split())
        words2 = set(text2.lower().split())
        
        if not words1 or not words2:
            return 0.0
        
        intersection = words1.intersection(words2)
        union = words1.union(words2)
        
        return len(intersection) / len(union)
    
    def is_same_domain(self, url1: str, url2: str) -> bool:
        """åŒã˜ãƒ‰ãƒ¡ã‚¤ãƒ³ã‹ãƒã‚§ãƒƒã‚¯"""
        try:
            domain1 = urlparse(url1).netloc
            domain2 = urlparse(url2).netloc
            return domain1 == domain2
        except:
            return False
    
    def parse_date(self, entry) -> str:
        """RSS ã‚¨ãƒ³ãƒˆãƒªã‹ã‚‰æ—¥ä»˜ã‚’è§£æ"""
        try:
            if hasattr(entry, 'published_parsed') and entry.published_parsed:
                return datetime(*entry.published_parsed[:6]).isoformat()
            elif hasattr(entry, 'published'):
                # æ–‡å­—åˆ—ã®æ—¥ä»˜ã‚’ãƒ‘ãƒ¼ã‚¹
                from dateutil import parser
                return parser.parse(entry.published).isoformat()
        except:
            pass
        
        return datetime.now().isoformat()

    def extract_article_date(self, article_element, article_url: str) -> str:
        """è¨˜äº‹ã‹ã‚‰å…¬é–‹æ—¥ã‚’æŠ½å‡ºï¼ˆæœ€é©åŒ–ç‰ˆï¼‰"""
        
        # æ‰‹æ³•1: è¨˜äº‹è¦ç´ å†…ã‹ã‚‰æ—¥ä»˜ã‚’æ¢ã™
        date_selectors = [
            'time',
            '.date',
            '.published',
            '.post-date',
            '.article-date',
            '[datetime]',
            '[class*="date"]',
            '[class*="time"]'
        ]
        
        for selector in date_selectors:
            date_elem = article_element.select_one(selector)
            if date_elem:
                # datetimeå±æ€§ã‚’ç¢ºèª
                if date_elem.has_attr('datetime'):
                    try:
                        from dateutil import parser
                        parsed_date = parser.parse(date_elem['datetime'])
                        return parsed_date.isoformat()
                    except:
                        pass
                
                # ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º
                date_text = date_elem.get_text(strip=True)
                extracted_date = self.parse_date_text(date_text)
                if extracted_date:
                    return extracted_date
        
        # æ‰‹æ³•2: è¨˜äº‹URLã‹ã‚‰æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º
        url_date = self.extract_date_from_url(article_url)
        if url_date:
            return url_date
        
        # æ‰‹æ³•3ã¯ç„¡åŠ¹åŒ–ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹æ”¹å–„ã®ãŸã‚ï¼‰
        # page_date = self.extract_date_from_article_page(article_url)
        # if page_date:
        #     return page_date
        
        # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç¾åœ¨æ™‚åˆ»ï¼ˆé‡è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¼ã‚Œé˜²æ­¢ã®ãŸã‚æœŸé–“å†…ã¨ã—ã¦æ‰±ã†ï¼‰
        return datetime.now().isoformat()

    def parse_date_text(self, date_text: str) -> Optional[str]:
        """ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰æ—¥ä»˜ã‚’è§£æ"""
        if not date_text or len(date_text) < 8:
            return None
        
        try:
            from dateutil import parser
            
            # ã‚ˆãã‚ã‚‹æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—
            clean_text = re.sub(r'^(Posted|Published|Date):\s*', '', date_text, flags=re.IGNORECASE)
            clean_text = re.sub(r'\s+', ' ', clean_text).strip()
            
            # æ—¥ä»˜è§£æã‚’è©¦è¡Œ
            parsed_date = parser.parse(clean_text, fuzzy=True)
            
            # æœªæ¥ã®æ—¥ä»˜ã¯ç„¡åŠ¹
            if parsed_date > datetime.now():
                return None
                
            # 10å¹´ä»¥ä¸Šå‰ã®æ—¥ä»˜ã‚‚ç„¡åŠ¹ã¨ã™ã‚‹
            if parsed_date < datetime.now() - timedelta(days=3650):
                return None
            
            return parsed_date.isoformat()
            
        except Exception:
            return None

    def extract_date_from_url(self, url: str) -> Optional[str]:
        """URLã‹ã‚‰æ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³ã‚’æŠ½å‡º"""
        if not url:
            return None
        
        try:
            # ä¸€èˆ¬çš„ãªURLæ—¥ä»˜ãƒ‘ã‚¿ãƒ¼ãƒ³
            patterns = [
                r'/(\d{4})/(\d{1,2})/(\d{1,2})/',  # /2025/06/22/
                r'/(\d{4})-(\d{1,2})-(\d{1,2})/',  # /2025-06-22/
                r'/(\d{4})(\d{2})(\d{2})/',        # /20250622/
                r'(\d{4}-\d{2}-\d{2})',           # 2025-06-22
                r'(\d{4}/\d{2}/\d{2})',           # 2025/06/22
            ]
            
            for pattern in patterns:
                match = re.search(pattern, url)
                if match:
                    if len(match.groups()) == 3:
                        year, month, day = match.groups()
                        try:
                            date_obj = datetime(int(year), int(month), int(day))
                            return date_obj.isoformat()
                        except ValueError:
                            continue
                    elif len(match.groups()) == 1:
                        date_str = match.group(1).replace('/', '-')
                        try:
                            from dateutil import parser
                            date_obj = parser.parse(date_str)
                            return date_obj.isoformat()
                        except:
                            continue
            
        except Exception:
            pass
        
        return None

    def extract_date_from_article_page(self, article_url: str) -> Optional[str]:
        """è¨˜äº‹ãƒšãƒ¼ã‚¸ã‹ã‚‰è©³ç´°ãªæ—¥ä»˜æŠ½å‡º"""
        try:
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¯¾ç­–
            time.sleep(1)
            
            response = self.session.get(article_url, timeout=10)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.content, 'html.parser')
            
            # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º
            meta_selectors = [
                'meta[property="article:published_time"]',
                'meta[name="publish-date"]',
                'meta[name="date"]',
                'meta[name="DC.date.issued"]',
                'meta[itemprop="datePublished"]'
            ]
            
            for selector in meta_selectors:
                meta_elem = soup.select_one(selector)
                if meta_elem:
                    content = meta_elem.get('content') or meta_elem.get('datetime')
                    if content:
                        try:
                            from dateutil import parser
                            parsed_date = parser.parse(content)
                            return parsed_date.isoformat()
                        except:
                            continue
            
            # JSON-LD æ§‹é€ åŒ–ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æŠ½å‡º
            scripts = soup.find_all('script', type='application/ld+json')
            for script in scripts:
                try:
                    json_data = json.loads(script.string)
                    if isinstance(json_data, dict):
                        date_published = json_data.get('datePublished')
                        if date_published:
                            from dateutil import parser
                            parsed_date = parser.parse(date_published)
                            return parsed_date.isoformat()
                except:
                    continue
                    
        except Exception:
            pass
        
        return None

    def filter_by_date_range(self, items: List[Dict[str, Any]], days_back: int) -> List[Dict[str, Any]]:
        """çµ±ä¸€ã•ã‚ŒãŸæ—¥ä»˜ç¯„å›²ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        if not items:
            return items
        
        cutoff_date = datetime.now() - timedelta(days=days_back)
        filtered_items = []
        excluded_count = 0
        
        for item in items:
            try:
                published_at = item.get('published_at', '')
                if not published_at:
                    # æ—¥ä»˜ãŒä¸æ˜ãªå ´åˆã¯å«ã‚ã‚‹ï¼ˆé‡è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¼ã‚Œé˜²æ­¢ï¼‰
                    filtered_items.append(item)
                    continue
                
                # ISOå½¢å¼ã®æ—¥ä»˜ã‚’è§£æ
                if isinstance(published_at, str):
                    from dateutil import parser
                    article_date = parser.parse(published_at.replace('Z', '+00:00'))
                    
                    # ã‚¿ã‚¤ãƒ ã‚¾ãƒ¼ãƒ³ã‚’é™¤å»ã—ã¦æ¯”è¼ƒ
                    if article_date.tzinfo:
                        article_date = article_date.replace(tzinfo=None)
                    
                    if article_date >= cutoff_date:
                        filtered_items.append(item)
                    else:
                        excluded_count += 1
                        print(f"    ğŸ“… æœŸé–“å¤–é™¤å¤–: {item['title'][:50]}... ({article_date.strftime('%Y-%m-%d')})")
                else:
                    # æ—¥ä»˜å‹ã®å ´åˆã¯ãã®ã¾ã¾æ¯”è¼ƒ
                    if published_at >= cutoff_date:
                        filtered_items.append(item)
                    else:
                        excluded_count += 1
                        
            except Exception as e:
                # æ—¥ä»˜è§£æã‚¨ãƒ©ãƒ¼ã®å ´åˆã¯å«ã‚ã‚‹ï¼ˆé‡è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹æ¼ã‚Œé˜²æ­¢ï¼‰
                print(f"    âš ï¸ æ—¥ä»˜è§£æã‚¨ãƒ©ãƒ¼ï¼ˆå«ã‚ã‚‹ï¼‰: {item['title'][:30]}... - {e}")
                filtered_items.append(item)
        
        if excluded_count > 0:
            print(f"    ğŸ“Š æœŸé–“ãƒ•ã‚£ãƒ«ã‚¿çµæœ: {len(filtered_items)}ä»¶æ¡ç”¨ã€{excluded_count}ä»¶é™¤å¤–")
        
        return filtered_items

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    collector = CompanyNewsCollector()
    news = collector.collect_all_company_news(days_back=3)
    
    print(f"\nğŸ“Š åé›†çµæœ: {len(news)}ä»¶")
    for item in news[:5]:
        print(f"  ğŸ¢ {item['company_id']}: {item['title'][:50]}...") 