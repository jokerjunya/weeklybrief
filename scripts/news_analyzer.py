#!/usr/bin/env python3
"""
ãƒ‹ãƒ¥ãƒ¼ã‚¹åˆ†æãƒ»é‡è¦åº¦åˆ¤å®šãƒ»æ—¥æœ¬èªè¦ç´„ã‚·ã‚¹ãƒ†ãƒ ï¼ˆç„¡æ–™ç‰ˆï¼‰
ãƒ­ãƒ¼ã‚«ãƒ«LLMï¼ˆOllamaï¼‰ã‚’ä½¿ç”¨ã—ã¦AIåˆ†æã‚’å®Ÿè¡Œ
"""

import json
import requests
import re
from datetime import datetime, timedelta
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from qwen3_llm import Qwen3Llm
import hashlib
import os
import asyncio
import aiohttp

@dataclass
class NewsItem:
    title: str
    url: str
    published_at: str
    summary: str
    content: str
    company_id: str
    source_type: str
    importance_score: float = 0.0
    japanese_summary: str = ""
    tags: List[str] = None

class EfficientNewsAnalyzer:
    """åŠ¹ç‡åŒ–ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹åˆ†æå™¨"""
    
    def __init__(self):
        self.llm = Qwen3Llm(model="ollama/qwen3:30b-a3b", api_url="http://localhost:11434/api/generate")
        self.cache_file = "cache/news_analysis_cache.json"
        self.cache = self.load_cache()
        
        # æ®µéšçš„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°è¨­å®š
        self.quick_filters = {
            'min_title_length': 20,
            'max_title_length': 200,
            'exclude_keywords': [
                'crypto', 'bitcoin', 'ethereum', 'nft', 'blockchain',
                'stock price', 'æ ªä¾¡', 'market close', 'earnings',
                'covid', 'coronavirus', 'pandemic'
            ],
            'priority_keywords': [
                'ai', 'artificial intelligence', 'machine learning', 'llm',
                'gpt', 'gemini', 'claude', 'chatbot', 'neural network',
                'deep learning', 'generative', 'äººå·¥çŸ¥èƒ½', 'AI'
            ]
        }
        
        # ä¼æ¥­åˆ¥é‡è¦åº¦ä¹—æ•°
        self.company_multipliers = {
            "OpenAI": 1.5,
            "Google AI": 1.4,
            "Anthropic": 1.3,
            "Meta AI": 1.2,
            "Microsoft AI": 1.2,
            "ElevenLabs": 1.1,
            "Perplexity": 1.1,
            "Genspark": 1.0,
            "Lovable": 1.0,
            "Stability AI": 1.0
        }

    def get_analysis_period(self) -> Dict[str, str]:
        """åˆ†æå¯¾è±¡æœŸé–“ã‚’å–å¾—"""
        end_date = datetime.now()
        start_date = end_date - timedelta(days=7)
        
        return {
            "start_date": start_date.strftime("%Yå¹´%mæœˆ%dæ—¥"),
            "end_date": end_date.strftime("%Yå¹´%mæœˆ%dæ—¥"),
            "period_description": f"{start_date.strftime('%m/%d')}ã€œ{end_date.strftime('%m/%d')}ï¼ˆéå»1é€±é–“ï¼‰"
        }

    async def generate_weekly_summary(self, analyzed_news: List[Dict]) -> str:
        """é€±æ¬¡ã‚µãƒãƒªãƒ¼ã‚’ç”Ÿæˆï¼ˆ300-400æ–‡å­—ï¼‰"""
        if not analyzed_news:
            return "ä»Šé€±ã¯AIãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼é–¢é€£ã®é‡è¦ãªãƒ‹ãƒ¥ãƒ¼ã‚¹ã¯ç¢ºèªã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚"
        
        # ä¸Šä½5ä»¶ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‚’æŠ½å‡º
        top_news = sorted(analyzed_news, key=lambda x: x.get('score', 0), reverse=True)[:5]
        
        # ä¼æ¥­åˆ¥çµ±è¨ˆ
        company_stats = {}
        for news in top_news:
            company = news.get('company_id', 'unknown')
            if company not in company_stats:
                company_stats[company] = 0
            company_stats[company] += 1
        
        # ã‚µãƒãƒªãƒ¼ç”Ÿæˆãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
        prompt = f"""ä»¥ä¸‹ã®AIãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼ãƒ‹ãƒ¥ãƒ¼ã‚¹ã‹ã‚‰é€±æ¬¡ã‚µãƒãƒªãƒ¼ã‚’300-400æ–‡å­—ã§ç”Ÿæˆã—ã¦ãã ã•ã„ã€‚

è¦æ±‚ï¼š
- å…¨ä½“çš„ãªãƒˆãƒ¬ãƒ³ãƒ‰ã‚„å‚¾å‘ã‚’åˆ†æ
- ä¸»è¦ä¼æ¥­ã®å‹•å‘ã‚’å«ã‚ã‚‹
- ãƒ“ã‚¸ãƒã‚¹ã¸ã®å½±éŸ¿ã‚’è€ƒæ…®
- èª­ã¿ã‚„ã™ãç°¡æ½”ã«
- å¿…ãš300-400æ–‡å­—ä»¥å†…

ä»Šé€±ã®ä¸»è¦ãƒ‹ãƒ¥ãƒ¼ã‚¹ï¼š
"""
        
        for i, news in enumerate(top_news, 1):
            prompt += f"{i}. {news.get('title', '')} (é‡è¦åº¦: {news.get('score', 0):.1f})\n"
            prompt += f"   è¦ç´„: {news.get('summary_jp', '')}\n\n"
        
        prompt += f"\nä¼æ¥­åˆ¥ãƒ‹ãƒ¥ãƒ¼ã‚¹æ•°: {dict(sorted(company_stats.items(), key=lambda x: x[1], reverse=True))}"
        
        try:
            print("ğŸ“ é€±æ¬¡ã‚µãƒãƒªãƒ¼ç”Ÿæˆä¸­...")
            summary = await self.llm.generate_content_async(prompt)
            
            # æ–‡å­—æ•°èª¿æ•´
            if len(summary) > 400:
                summary = summary[:397] + "..."
            elif len(summary) < 250:
                summary += "å¼•ãç¶šãAIãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼åˆ†é‡ã®å‹•å‘ã«æ³¨ç›®ã—ã¦ã„ãã¾ã™ã€‚"
            
            return summary.strip()
            
        except Exception as e:
            print(f"âš ï¸ ã‚µãƒãƒªãƒ¼ç”Ÿæˆã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ ã‚µãƒãƒªãƒ¼
            main_companies = list(company_stats.keys())[:3]
            return f"ä»Šé€±ã¯{', '.join(main_companies)}ãªã©ã‚’ä¸­å¿ƒã¨ã—ãŸAIãƒ»ãƒ†ã‚¯ãƒãƒ­ã‚¸ãƒ¼é–¢é€£ã®ç™ºè¡¨ãŒç›¸æ¬¡ãã¾ã—ãŸã€‚ç‰¹ã«{top_news[0].get('title', '')[:50]}ãªã©ã®å‹•å‘ãŒæ³¨ç›®ã•ã‚Œã¾ã™ã€‚AIæŠ€è¡“ã®å®Ÿç”¨åŒ–ã¨ä¼æ¥­é–“ã®æˆ¦ç•¥çš„ææºãŒåŠ é€Ÿã—ã¦ãŠã‚Šã€æ¥­ç•Œå…¨ä½“ã®ç«¶äº‰ãŒæ¿€åŒ–ã—ã¦ã„ã¾ã™ã€‚"

    def load_cache(self) -> Dict:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
        os.makedirs("cache", exist_ok=True)
        if os.path.exists(self.cache_file):
            try:
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    return json.load(f)
            except:
                return {}
        return {}

    def save_cache(self):
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ä¿å­˜"""
        with open(self.cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.cache, f, ensure_ascii=False, indent=2)

    def get_cache_key(self, news_item: Dict) -> str:
        """ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ"""
        content = f"{news_item['title']}{news_item['url']}"
        return hashlib.md5(content.encode()).hexdigest()

    def apply_quick_filters(self, news_list: List[Dict]) -> List[Dict]:
        """æ®µéšçš„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°ã‚’é©ç”¨"""
        filtered_news = []
        
        for news in news_list:
            title = news.get('title', '').lower()
            
            # åŸºæœ¬ãƒ•ã‚£ãƒ«ã‚¿
            if (len(title) < self.quick_filters['min_title_length'] or 
                len(title) > self.quick_filters['max_title_length']):
                continue
            
            # é™¤å¤–ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒã‚§ãƒƒã‚¯
            if any(keyword in title for keyword in self.quick_filters['exclude_keywords']):
                continue
            
            # å„ªå…ˆã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãƒœãƒ¼ãƒŠã‚¹
            priority_score = sum(2 for keyword in self.quick_filters['priority_keywords'] 
                                if keyword in title)
            
            # ä¼æ¥­é–¢é€£åº¦ãƒã‚§ãƒƒã‚¯
            company_relevance = 0
            for company in self.company_multipliers.keys():
                if company.lower() in title or company.lower() in news.get('description', '').lower():
                    company_relevance += 3
                    break
            
            # åŸºæœ¬ã‚¹ã‚³ã‚¢è¨ˆç®—ï¼ˆãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹ï¼‰
            base_score = 3.0 + priority_score + company_relevance
            
            # æœ€ä½ã‚¹ã‚³ã‚¢ä»¥ä¸Šã®ã‚‚ã®ã®ã¿é€šã™
            if base_score >= 4.0:  # AIåˆ¤å®šå¯¾è±¡ã‚’çµã‚Šè¾¼ã¿
                news['base_score'] = base_score
                filtered_news.append(news)
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆã€ä¸Šä½50ä»¶ã®ã¿AIåˆ†æå¯¾è±¡
        filtered_news.sort(key=lambda x: x['base_score'], reverse=True)
        return filtered_news[:50]  # AIåˆ†æå¯¾è±¡ã‚’50ä»¶ã«åˆ¶é™

    async def batch_analyze_with_ai(self, news_batch: List[Dict]) -> List[Dict]:
        """ãƒãƒƒãƒã§AIåˆ†æã‚’å®Ÿè¡Œ"""
        if not news_batch:
            return []
        
        # ãƒãƒƒãƒãƒ—ãƒ­ãƒ³ãƒ—ãƒˆä½œæˆ
        batch_prompt = """ä»¥ä¸‹ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ç¾¤ã‚’åˆ†æã—ã€å„è¨˜äº‹ã«ã¤ã„ã¦ä»¥ä¸‹ã®å½¢å¼ã§JSONã§å›ç­”ã—ã¦ãã ã•ã„ï¼š

{
  "analyses": [
    {
      "index": 0,
      "importance_score": 5.5,
      "japanese_summary": "è¨˜äº‹ã®è©³ç´°è¦ç´„ï¼ˆ80-120æ–‡å­—ç¨‹åº¦ï¼‰"
    }
  ]
}

è©•ä¾¡åŸºæº–ï¼š
- AI/æ©Ÿæ¢°å­¦ç¿’ã®æŠ€è¡“é©æ–°: +2ç‚¹
- å¤§æ‰‹ä¼æ¥­ã®é‡è¦ç™ºè¡¨: +1.5ç‚¹
- æ¥­ç•Œã¸ã®å½±éŸ¿åº¦: +1ç‚¹
- å®Ÿç”¨æ€§ãƒ»å•†ç”¨åŒ–: +1ç‚¹

è¨˜äº‹ä¸€è¦§ï¼š
"""
        
        for i, news in enumerate(news_batch):
            batch_prompt += f"\n{i}. ã‚¿ã‚¤ãƒˆãƒ«: {news['title']}\n"
            batch_prompt += f"   èª¬æ˜: {news.get('description', '')[:100]}...\n"
        
        try:
            print(f"ğŸ¤– AIåˆ†æä¸­... ({len(news_batch)}ä»¶ã‚’ãƒãƒƒãƒå‡¦ç†)")
            response = await self.llm.generate_content_async(batch_prompt)
            
            # JSONè§£æ
            import re
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                result = json.loads(json_match.group())
                analyses = result.get('analyses', [])
                
                # çµæœã‚’ãƒ‹ãƒ¥ãƒ¼ã‚¹ã«é©ç”¨
                for analysis in analyses:
                    idx = analysis.get('index', 0)
                    if 0 <= idx < len(news_batch):
                        news_batch[idx]['ai_score'] = analysis.get('importance_score', 5.0)
                        news_batch[idx]['summary_jp'] = analysis.get('japanese_summary', '')
                
                return news_batch
            
        except Exception as e:
            print(f"âš ï¸ ãƒãƒƒãƒAIåˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            # ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: åŸºæœ¬ã‚¹ã‚³ã‚¢ã‚’ä½¿ç”¨
            for news in news_batch:
                news['ai_score'] = news.get('base_score', 5.0)
                news['summary_jp'] = f"{news['title'][:30]}..."
        
        return news_batch

    async def analyze_news_efficient(self, news_list: List[Dict]) -> List[Dict]:
        """åŠ¹ç‡åŒ–ã•ã‚ŒãŸãƒ‹ãƒ¥ãƒ¼ã‚¹åˆ†æ"""
        print(f"ğŸ“Š åŠ¹ç‡åŒ–åˆ†æé–‹å§‹: {len(news_list)}ä»¶")
        
        # 1. æ®µéšçš„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        print("ğŸ” æ®µéšçš„ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å®Ÿè¡Œä¸­...")
        filtered_news = self.apply_quick_filters(news_list)
        print(f"   âœ… ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°å¾Œ: {len(filtered_news)}ä»¶ (å‰Šæ¸›ç‡: {((len(news_list)-len(filtered_news))/len(news_list)*100):.1f}%)")
        
        # 2. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
        print("ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯ä¸­...")
        cache_hits = 0
        ai_analysis_needed = []
        
        for news in filtered_news:
            cache_key = self.get_cache_key(news)
            if cache_key in self.cache:
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ
                cached_data = self.cache[cache_key]
                news.update(cached_data)
                cache_hits += 1
            else:
                ai_analysis_needed.append(news)
        
        print(f"   âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ’ãƒƒãƒˆ: {cache_hits}ä»¶, AIåˆ†æå¿…è¦: {len(ai_analysis_needed)}ä»¶")
        
        # 3. ãƒãƒƒãƒAIåˆ†æ
        if ai_analysis_needed:
            print("ğŸ¤– ãƒãƒƒãƒAIåˆ†æå®Ÿè¡Œä¸­...")
            batch_size = 10  # 10ä»¶ãšã¤ãƒãƒƒãƒå‡¦ç†
            
            for i in range(0, len(ai_analysis_needed), batch_size):
                batch = ai_analysis_needed[i:i+batch_size]
                analyzed_batch = await self.batch_analyze_with_ai(batch)
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                for news in analyzed_batch:
                    cache_key = self.get_cache_key(news)
                    self.cache[cache_key] = {
                        'ai_score': news.get('ai_score', 5.0),
                        'summary_jp': news.get('summary_jp', '')
                    }
        
        # 4. æœ€çµ‚ã‚¹ã‚³ã‚¢è¨ˆç®—
        print("ğŸ“Š æœ€çµ‚ã‚¹ã‚³ã‚¢è¨ˆç®—ä¸­...")
        for news in filtered_news:
            base_score = news.get('base_score', 5.0)
            ai_score = news.get('ai_score', 5.0)
            
            # ä¼æ¥­åˆ¥ä¹—æ•°é©ç”¨
            company = news.get('company', 'ãã®ä»–')
            multiplier = self.company_multipliers.get(company, 1.0)
            
            # æœ€çµ‚ã‚¹ã‚³ã‚¢ = (åŸºæœ¬ã‚¹ã‚³ã‚¢ + AI ã‚¹ã‚³ã‚¢) / 2 * ä¼æ¥­ä¹—æ•°
            final_score = ((base_score + ai_score) / 2) * multiplier
            news['score'] = round(final_score, 1)
        
        # 5. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜
        self.save_cache()
        
        # 6. ã‚¹ã‚³ã‚¢é †ã‚½ãƒ¼ãƒˆ
        filtered_news.sort(key=lambda x: x['score'], reverse=True)
        
        print(f"âœ… åŠ¹ç‡åŒ–åˆ†æå®Œäº†: æœ€é«˜ã‚¹ã‚³ã‚¢ {filtered_news[0]['score']:.1f}")
        return filtered_news

# å¾“æ¥ã®NewsAnalyzerã‚¯ãƒ©ã‚¹ã‚‚æ®‹ã™ï¼ˆäº’æ›æ€§ã®ãŸã‚ï¼‰
class NewsAnalyzer:
    """å¾“æ¥ã®ãƒ‹ãƒ¥ãƒ¼ã‚¹åˆ†æå™¨ï¼ˆäº’æ›æ€§ç¶­æŒï¼‰"""
    
    def __init__(self):
        self.efficient_analyzer = EfficientNewsAnalyzer()
    
    async def analyze_news(self, news_list: List[Dict], progress_callback=None) -> List[Dict]:
        """åŠ¹ç‡åŒ–åˆ†æå™¨ã«è»¢é€"""
        return await self.efficient_analyzer.analyze_news_efficient(news_list)

def analyze_company_news(news_data: List[Dict[str, Any]], top_n: int = 10) -> List[NewsItem]:
    """
    ä¼æ¥­ãƒ‹ãƒ¥ãƒ¼ã‚¹ã®åˆ†æï¼ˆãƒ¡ã‚¤ãƒ³é–¢æ•°ï¼‰
    
    Args:
        news_data: åé›†ã—ãŸãƒ‹ãƒ¥ãƒ¼ã‚¹ãƒ‡ãƒ¼ã‚¿
        top_n: ä¸Šä½ä½•ä»¶ã‚’è¿”ã™ã‹
        
    Returns:
        åˆ†ææ¸ˆã¿ãƒ‹ãƒ¥ãƒ¼ã‚¹è¨˜äº‹ã®ãƒªã‚¹ãƒˆ
    """
    analyzer = NewsAnalyzer()
    
    # Qwen3ã®åˆ©ç”¨å¯èƒ½æ€§ã‚’ãƒã‚§ãƒƒã‚¯
    if analyzer.is_qwen3_available():
        print("ğŸ¤– Qwen3 LLM ã‚’ä½¿ç”¨ã—ã¦åˆ†æã‚’å®Ÿè¡Œã—ã¾ã™")
    else:
        print("âš ï¸ Qwen3 LLM ãŒåˆ©ç”¨ã§ãã¾ã›ã‚“ã€‚ãƒ«ãƒ¼ãƒ«ãƒ™ãƒ¼ã‚¹åˆ†æã®ã¿å®Ÿè¡Œã—ã¾ã™")
    
    # åˆ†æå®Ÿè¡Œ
    analyzed_items = analyzer.analyze_news(news_data, top_n)
    
    return analyzed_items

if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆç”¨ã®ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿
    sample_news = [
        {
            "title": "OpenAI Announces GPT-5 with Revolutionary Capabilities",
            "url": "https://example.com/gpt5",
            "published_at": "2025-06-22T10:00:00Z",
            "summary": "OpenAI unveils GPT-5 with groundbreaking multimodal capabilities",
            "content": "OpenAI today announced GPT-5, featuring advanced reasoning and multimodal processing...",
            "company_id": "openai",
            "source_type": "rss"
        }
    ]
    
    # åˆ†æãƒ†ã‚¹ãƒˆ
    results = analyze_company_news(sample_news, top_n=5)
    
    print(f"\nğŸ“Š åˆ†æçµæœ:")
    for i, item in enumerate(results, 1):
        print(f"{i}. {item.title}")
        print(f"   é‡è¦åº¦: {item.importance_score:.1f}")
        print(f"   ä¼æ¥­: {item.company_id}")
        if item.japanese_summary:
            print(f"   è¦ç´„: {item.japanese_summary[:100]}...")
        print() 