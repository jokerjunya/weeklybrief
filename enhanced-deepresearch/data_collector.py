#!/usr/bin/env python3
"""
Enhanced DeepResearch - çµ±åˆãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ³ã‚¸ãƒ³

æ—¢å­˜ã®CompanyNewsCollectorã¨data-processing.pyã®æ©Ÿèƒ½ã‚’çµ±åˆã—ã€ä»¥ä¸‹ã‚’æä¾›ï¼š
- RSS ãƒ•ã‚£ãƒ¼ãƒ‰åé›†
- ãƒ‹ãƒ¥ãƒ¼ã‚¹APIçµ±åˆ
- ã‚½ãƒ¼ã‚·ãƒ£ãƒ«ãƒ¡ãƒ‡ã‚£ã‚¢æƒ…å ±åé›†
- è«–æ–‡ãƒ»æŠ€è¡“æ–‡æ›¸åé›†
- ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–ãƒ»ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'scripts'))

from company_news_collector import CompanyNewsCollector
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, field
from datetime import datetime, timedelta
import json
import asyncio
import aiohttp
import hashlib

@dataclass
class DataSource:
    """ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å®šç¾©"""
    name: str
    type: str  # "rss", "api", "social", "academic"
    url: str
    priority: int  # 1-10 (é«˜ã„ã»ã©å„ªå…ˆ)
    reliability_score: float  # 0.0-1.0
    update_frequency: int  # æ›´æ–°é »åº¦ï¼ˆåˆ†ï¼‰
    last_updated: datetime = None
    active: bool = True

@dataclass
class CollectedItem:
    """åé›†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿ã‚¢ã‚¤ãƒ†ãƒ """
    id: str
    title: str
    content: str
    source: str
    url: str
    published_at: datetime
    collected_at: datetime
    data_type: str  # "news", "social", "academic", "technical"
    relevance_score: float
    metadata: Dict[str, Any] = field(default_factory=dict)

class DataCollector(CompanyNewsCollector):
    """
    çµ±åˆãƒ‡ãƒ¼ã‚¿åé›†ã‚¨ãƒ³ã‚¸ãƒ³
    
    æ—¢å­˜ã®CompanyNewsCollectorã‚’ç¶™æ‰¿ãƒ»æ‹¡å¼µã—ã€ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’è¿½åŠ ï¼š
    - å¤šæ§˜ãªãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å¯¾å¿œ
    - ãƒªã‚¢ãƒ«ã‚¿ã‚¤ãƒ åé›†
    - ãƒ‡ãƒ¼ã‚¿å“è³ªè©•ä¾¡
    - é‡è¤‡æ’é™¤
    """
    
    def __init__(self, config_path: str = "config/settings.json"):
        super().__init__(config_path)
        
        # æ‹¡å¼µãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹å®šç¾©
        self.data_sources = {
            # RSS ãƒ•ã‚£ãƒ¼ãƒ‰
            "techcrunch": DataSource(
                name="TechCrunch",
                type="rss",
                url="https://techcrunch.com/feed/",
                priority=8,
                reliability_score=0.85,
                update_frequency=60
            ),
            "ars_technica": DataSource(
                name="Ars Technica",
                type="rss", 
                url="https://feeds.arstechnica.com/arstechnica/index",
                priority=9,
                reliability_score=0.9,
                update_frequency=120
            ),
            "the_verge": DataSource(
                name="The Verge",
                type="rss",
                url="https://www.theverge.com/rss/index.xml",
                priority=7,
                reliability_score=0.8,
                update_frequency=60
            ),
            
            # AI å°‚é–€ã‚½ãƒ¼ã‚¹
            "ai_news": DataSource(
                name="AI News",
                type="rss",
                url="https://www.artificialintelligence-news.com/feed/",
                priority=8,
                reliability_score=0.8,
                update_frequency=180
            ),
            
            # å­¦è¡“ã‚½ãƒ¼ã‚¹
            "arxiv_cs_ai": DataSource(
                name="arXiv CS.AI",
                type="academic",
                url="http://export.arxiv.org/rss/cs.AI",
                priority=6,
                reliability_score=0.95,
                update_frequency=1440  # 1æ—¥1å›
            )
        }
        
        # åé›†è¨­å®š
        self.collection_config = {
            "max_items_per_source": 50,
            "relevance_threshold": 0.3,
            "deduplication_threshold": 0.8,
            "max_collection_time": 300,  # 5åˆ†
            "concurrent_sources": 5
        }
        
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¨ã‚¹ãƒˆãƒ¬ãƒ¼ã‚¸
        self.collected_items: List[CollectedItem] = []
        self.item_cache = {}
        self.collection_stats = {
            "total_collected": 0,
            "duplicates_removed": 0,
            "low_relevance_filtered": 0,
            "last_collection": None
        }
    
    async def collect_comprehensive_data(
        self, 
        topics: List[str], 
        time_range: timedelta = timedelta(days=7),
        source_types: List[str] = None
    ) -> List[CollectedItem]:
        """
        åŒ…æ‹¬çš„ãƒ‡ãƒ¼ã‚¿åé›†
        
        Args:
            topics: åé›†å¯¾è±¡ãƒˆãƒ”ãƒƒã‚¯
            time_range: åé›†å¯¾è±¡æœŸé–“
            source_types: åé›†ã™ã‚‹ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—ï¼ˆNone=å…¨ã¦ï¼‰
        
        Returns:
            List[CollectedItem]: åé›†ã•ã‚ŒãŸãƒ‡ãƒ¼ã‚¿
        """
        print(f"ğŸ” åŒ…æ‹¬çš„ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹: {len(topics)}ãƒˆãƒ”ãƒƒã‚¯, {time_range.days}æ—¥é–“")
        
        # ã‚½ãƒ¼ã‚¹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        active_sources = {
            name: source for name, source in self.data_sources.items()
            if source.active and (source_types is None or source.type in source_types)
        }
        
        print(f"ğŸ“¡ {len(active_sources)}ã®ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹ã‹ã‚‰åé›†ä¸­...")
        
        # ä¸¦è¡Œåé›†å®Ÿè¡Œ
        collection_tasks = []
        semaphore = asyncio.Semaphore(self.collection_config["concurrent_sources"])
        
        for source_name, source in active_sources.items():
            task = self._collect_from_source(source, topics, time_range, semaphore)
            collection_tasks.append(task)
        
        # å…¨ã¦ã®åé›†ã‚¿ã‚¹ã‚¯ã‚’å®Ÿè¡Œ
        collection_results = await asyncio.gather(*collection_tasks, return_exceptions=True)
        
        # çµæœã‚’çµ±åˆ
        all_items = []
        for result in collection_results:
            if isinstance(result, list):
                all_items.extend(result)
            elif isinstance(result, Exception):
                print(f"âš ï¸ åé›†ã‚¨ãƒ©ãƒ¼: {result}")
        
        # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨é‡è¤‡æ’é™¤
        cleaned_items = await self._clean_and_deduplicate(all_items)
        
        # é–¢é€£æ€§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        relevant_items = self._filter_by_relevance(cleaned_items, topics)
        
        # åé›†çµ±è¨ˆæ›´æ–°
        self._update_collection_stats(all_items, cleaned_items, relevant_items)
        
        print(f"âœ… ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†: {len(relevant_items)}ä»¶ï¼ˆå…ƒ: {len(all_items)}ä»¶ï¼‰")
        
        self.collected_items = relevant_items
        return relevant_items
    
    async def _collect_from_source(
        self, 
        source: DataSource, 
        topics: List[str], 
        time_range: timedelta,
        semaphore: asyncio.Semaphore
    ) -> List[CollectedItem]:
        """å˜ä¸€ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿åé›†"""
        async with semaphore:
            try:
                print(f"ğŸ“¥ {source.name}ã‹ã‚‰åé›†ä¸­...")
                
                if source.type == "rss":
                    return await self._collect_from_rss(source, topics, time_range)
                elif source.type == "api":
                    return await self._collect_from_api(source, topics, time_range)
                elif source.type == "academic":
                    return await self._collect_from_academic(source, topics, time_range)
                else:
                    print(f"âš ï¸ æœªå¯¾å¿œã®ã‚½ãƒ¼ã‚¹ã‚¿ã‚¤ãƒ—: {source.type}")
                    return []
                    
            except Exception as e:
                print(f"âŒ {source.name}åé›†ã‚¨ãƒ©ãƒ¼: {e}")
                return []
    
    async def _collect_from_rss(
        self, 
        source: DataSource, 
        topics: List[str], 
        time_range: timedelta
    ) -> List[CollectedItem]:
        """RSS ãƒ•ã‚£ãƒ¼ãƒ‰ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿åé›†"""
        try:
            import feedparser
            
            # RSS ãƒ•ã‚£ãƒ¼ãƒ‰ã‚’å–å¾—
            feed = feedparser.parse(source.url)
            
            if not feed.entries:
                print(f"âš ï¸ {source.name}: RSSã‚¨ãƒ³ãƒˆãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
                return []
            
            items = []
            cutoff_date = datetime.now() - time_range
            
            for entry in feed.entries[:self.collection_config["max_items_per_source"]]:
                # æ—¥ä»˜ãƒã‚§ãƒƒã‚¯
                try:
                    published_at = datetime(*entry.published_parsed[:6])
                    if published_at < cutoff_date:
                        continue
                except:
                    published_at = datetime.now()
                
                # ã‚¢ã‚¤ãƒ†ãƒ ä½œæˆ
                item = CollectedItem(
                    id=self._generate_item_id(entry.link),
                    title=entry.title,
                    content=entry.get('summary', ''),
                    source=source.name,
                    url=entry.link,
                    published_at=published_at,
                    collected_at=datetime.now(),
                    data_type="news",
                    relevance_score=self._calculate_relevance(entry.title + " " + entry.get('summary', ''), topics),
                    metadata={
                        "author": entry.get('author', ''),
                        "tags": entry.get('tags', []),
                        "rss_source": source.url
                    }
                )
                items.append(item)
            
            print(f"ğŸ“° {source.name}: {len(items)}ä»¶åé›†")
            return items
            
        except Exception as e:
            print(f"âŒ RSSåé›†ã‚¨ãƒ©ãƒ¼ ({source.name}): {e}")
            return []
    
    async def _collect_from_api(
        self, 
        source: DataSource, 
        topics: List[str], 
        time_range: timedelta
    ) -> List[CollectedItem]:
        """API ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿åé›†"""
        # å°†æ¥ã®æ‹¡å¼µç”¨ï¼šNews APIã€Twitter APIã€GitHub APIç­‰
        try:
            async with aiohttp.ClientSession() as session:
                # APIå®Ÿè£…ã¯å…·ä½“çš„ãªAPIã«ã‚ˆã£ã¦å¤‰ã‚ã‚‹ãŸã‚ã€
                # ã“ã“ã§ã¯åŸºæœ¬çš„ãªæ§‹é€ ã®ã¿å®Ÿè£…
                items = []
                
                # APIå‘¼ã³å‡ºã—ä¾‹ï¼ˆå®Ÿéš›ã®APIã«åˆã‚ã›ã¦å®Ÿè£…ï¼‰
                # async with session.get(source.url) as response:
                #     data = await response.json()
                #     # ãƒ‡ãƒ¼ã‚¿å‡¦ç†...
                
                print(f"ğŸ”Œ {source.name}: APIåé›†ï¼ˆæœªå®Ÿè£…ï¼‰")
                return items
                
        except Exception as e:
            print(f"âŒ APIåé›†ã‚¨ãƒ©ãƒ¼ ({source.name}): {e}")
            return []
    
    async def _collect_from_academic(
        self, 
        source: DataSource, 
        topics: List[str], 
        time_range: timedelta
    ) -> List[CollectedItem]:
        """å­¦è¡“ã‚½ãƒ¼ã‚¹ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿åé›†"""
        try:
            import feedparser
            
            feed = feedparser.parse(source.url)
            items = []
            cutoff_date = datetime.now() - time_range
            
            for entry in feed.entries[:self.collection_config["max_items_per_source"]]:
                try:
                    published_at = datetime(*entry.published_parsed[:6])
                    if published_at < cutoff_date:
                        continue
                except:
                    published_at = datetime.now()
                
                # å­¦è¡“è«–æ–‡ã®å ´åˆã€ã‚¢ãƒ–ã‚¹ãƒˆãƒ©ã‚¯ãƒˆã‚’å«ã‚ã‚‹
                content = entry.get('summary', '')
                if hasattr(entry, 'arxiv_comment'):
                    content += f"\n\nComment: {entry.arxiv_comment}"
                
                item = CollectedItem(
                    id=self._generate_item_id(entry.link),
                    title=entry.title,
                    content=content,
                    source=source.name,
                    url=entry.link,
                    published_at=published_at,
                    collected_at=datetime.now(),
                    data_type="academic",
                    relevance_score=self._calculate_relevance(entry.title + " " + content, topics),
                    metadata={
                        "authors": entry.get('author', ''),
                        "arxiv_id": entry.get('id', '').split('/')[-1] if 'arxiv' in source.name else '',
                        "categories": entry.get('tags', []),
                        "academic_source": source.url
                    }
                )
                items.append(item)
            
            print(f"ğŸ“ {source.name}: {len(items)}ä»¶åé›†")
            return items
            
        except Exception as e:
            print(f"âŒ å­¦è¡“åé›†ã‚¨ãƒ©ãƒ¼ ({source.name}): {e}")
            return []
    
    def _calculate_relevance(self, text: str, topics: List[str]) -> float:
        """ãƒ†ã‚­ã‚¹ãƒˆã®é–¢é€£æ€§ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—"""
        if not text or not topics:
            return 0.0
        
        text_lower = text.lower()
        topic_matches = 0
        total_weight = 0
        
        for topic in topics:
            topic_lower = topic.lower()
            
            # å®Œå…¨ä¸€è‡´
            if topic_lower in text_lower:
                topic_matches += 2
            
            # éƒ¨åˆ†ä¸€è‡´
            topic_words = topic_lower.split()
            for word in topic_words:
                if len(word) > 2 and word in text_lower:
                    topic_matches += 1
            
            total_weight += 2  # å®Œå…¨ä¸€è‡´ã®æœ€å¤§ã‚¹ã‚³ã‚¢
        
        relevance_score = min(topic_matches / max(total_weight, 1), 1.0)
        return relevance_score
    
    async def _clean_and_deduplicate(self, items: List[CollectedItem]) -> List[CollectedItem]:
        """ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°ã¨é‡è¤‡æ’é™¤"""
        if not items:
            return []
        
        print(f"ğŸ§¹ ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°é–‹å§‹: {len(items)}ä»¶")
        
        # 1. åŸºæœ¬ã‚¯ãƒªãƒ¼ãƒ‹ãƒ³ã‚°
        cleaned_items = []
        for item in items:
            # ã‚¿ã‚¤ãƒˆãƒ«ã¨å†…å®¹ã®åŸºæœ¬ãƒã‚§ãƒƒã‚¯
            if len(item.title.strip()) < 10:
                continue
            if len(item.content.strip()) < 20:
                continue
            
            # ä¸æ­£ãªURLé™¤å¤–
            if not item.url or not item.url.startswith('http'):
                continue
            
            cleaned_items.append(item)
        
        # 2. é‡è¤‡æ’é™¤
        unique_items = []
        seen_hashes = set()
        
        for item in cleaned_items:
            # ã‚¿ã‚¤ãƒˆãƒ«ã¨å†…å®¹ã®çµ„ã¿åˆã‚ã›ã§ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—
            content_hash = self._calculate_content_hash(item.title, item.content)
            
            if content_hash not in seen_hashes:
                seen_hashes.add(content_hash)
                unique_items.append(item)
        
        duplicates_removed = len(cleaned_items) - len(unique_items)
        if duplicates_removed > 0:
            print(f"ğŸ”„ é‡è¤‡é™¤å»: {duplicates_removed}ä»¶")
        
        return unique_items
    
    def _calculate_content_hash(self, title: str, content: str) -> str:
        """ã‚³ãƒ³ãƒ†ãƒ³ãƒ„ã®ãƒãƒƒã‚·ãƒ¥å€¤ã‚’è¨ˆç®—"""
        # ã‚¿ã‚¤ãƒˆãƒ«ã¨å†…å®¹ã®æœ€åˆã®200æ–‡å­—ã‚’ä½¿ã£ã¦ãƒãƒƒã‚·ãƒ¥ã‚’è¨ˆç®—
        content_sample = (title + " " + content)[:200].lower().strip()
        return hashlib.md5(content_sample.encode()).hexdigest()
    
    def _filter_by_relevance(self, items: List[CollectedItem], topics: List[str]) -> List[CollectedItem]:
        """é–¢é€£æ€§ã«ã‚ˆã‚‹ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°"""
        threshold = self.collection_config["relevance_threshold"]
        relevant_items = [item for item in items if item.relevance_score >= threshold]
        
        filtered_count = len(items) - len(relevant_items)
        if filtered_count > 0:
            print(f"ğŸ¯ é–¢é€£æ€§ãƒ•ã‚£ãƒ«ã‚¿: {filtered_count}ä»¶é™¤å¤–")
        
        # é–¢é€£æ€§ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆ
        relevant_items.sort(key=lambda x: x.relevance_score, reverse=True)
        
        return relevant_items
    
    def _generate_item_id(self, url: str) -> str:
        """ã‚¢ã‚¤ãƒ†ãƒ IDã‚’ç”Ÿæˆ"""
        return hashlib.md5(url.encode()).hexdigest()[:16]
    
    def _update_collection_stats(
        self, 
        raw_items: List[CollectedItem], 
        cleaned_items: List[CollectedItem], 
        final_items: List[CollectedItem]
    ) -> None:
        """åé›†çµ±è¨ˆã‚’æ›´æ–°"""
        self.collection_stats.update({
            "total_collected": len(raw_items),
            "duplicates_removed": len(raw_items) - len(cleaned_items),
            "low_relevance_filtered": len(cleaned_items) - len(final_items),
            "last_collection": datetime.now(),
            "sources_used": len(set(item.source for item in raw_items)),
            "final_items": len(final_items)
        })
    
    def get_collection_summary(self) -> Dict[str, Any]:
        """åé›†ã‚µãƒãƒªãƒ¼ã‚’å–å¾—"""
        if not self.collected_items:
            return {"message": "ã¾ã ãƒ‡ãƒ¼ã‚¿ãŒåé›†ã•ã‚Œã¦ã„ã¾ã›ã‚“"}
        
        # ã‚½ãƒ¼ã‚¹åˆ¥çµ±è¨ˆ
        source_stats = {}
        type_stats = {}
        
        for item in self.collected_items:
            # ã‚½ãƒ¼ã‚¹åˆ¥
            if item.source not in source_stats:
                source_stats[item.source] = 0
            source_stats[item.source] += 1
            
            # ã‚¿ã‚¤ãƒ—åˆ¥
            if item.data_type not in type_stats:
                type_stats[item.data_type] = 0
            type_stats[item.data_type] += 1
        
        # é–¢é€£æ€§ã‚¹ã‚³ã‚¢çµ±è¨ˆ
        relevance_scores = [item.relevance_score for item in self.collected_items]
        avg_relevance = sum(relevance_scores) / len(relevance_scores)
        
        return {
            "collection_stats": self.collection_stats,
            "source_breakdown": source_stats,
            "type_breakdown": type_stats,
            "relevance_stats": {
                "average": avg_relevance,
                "max": max(relevance_scores),
                "min": min(relevance_scores)
            },
            "time_range": {
                "earliest": min(item.published_at for item in self.collected_items).isoformat(),
                "latest": max(item.published_at for item in self.collected_items).isoformat()
            }
        }
    
    async def collect_for_research(self, research_topic: str) -> List[CollectedItem]:
        """
        Enhanced DeepResearchç”¨ã®ãƒ‡ãƒ¼ã‚¿åé›†
        
        Args:
            research_topic: ç ”ç©¶ãƒˆãƒ”ãƒƒã‚¯
        
        Returns:
            List[CollectedItem]: é–¢é€£ãƒ‡ãƒ¼ã‚¿
        """
        # ãƒˆãƒ”ãƒƒã‚¯ã‚’ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã«åˆ†è§£
        topics = [research_topic]
        
        # AIé–¢é€£ã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’è¿½åŠ ï¼ˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæ‹¡å¼µï¼‰
        ai_keywords = ["artificial intelligence", "machine learning", "AI", "LLM", "neural network"]
        topics.extend(ai_keywords)
        
        # éå»1é€±é–“ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†
        time_range = timedelta(days=7)
        
        # ãƒ‹ãƒ¥ãƒ¼ã‚¹ã¨å­¦è¡“è«–æ–‡ã‚’é‡ç‚¹çš„ã«åé›†
        source_types = ["rss", "academic"]
        
        collected_data = await self.collect_comprehensive_data(
            topics=topics,
            time_range=time_range,
            source_types=source_types
        )
        
        # ãƒˆãƒƒãƒ—20ä»¶ã«çµã‚Šè¾¼ã¿ï¼ˆå“è³ªé‡è¦–ï¼‰
        top_items = sorted(collected_data, key=lambda x: x.relevance_score, reverse=True)[:20]
        
        print(f"ğŸ¯ ç ”ç©¶ç”¨ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†: {len(top_items)}ä»¶ï¼ˆé–¢é€£åº¦: {top_items[0].relevance_score:.2f} - {top_items[-1].relevance_score:.2f}ï¼‰")
        
        return top_items 