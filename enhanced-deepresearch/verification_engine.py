#!/usr/bin/env python3
"""
Enhanced DeepResearch - ä¿¡é ¼æ€§æ¤œè¨¼ã‚¨ãƒ³ã‚¸ãƒ³

æ—¢å­˜ã®EfficientNewsAnalyzerã‚’ç¶™æ‰¿ãƒ»æ‹¡å¼µã—ã€ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’è¿½åŠ ï¼š
- ã‚¯ãƒ­ã‚¹ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹æ¤œè¨¼
- ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯
- ä¿¡é ¼æ€§ã‚¹ã‚³ã‚¢ç®—å‡º
- æƒ…å ±æºã®å“è³ªè©•ä¾¡
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'scripts'))

from news_analyzer import EfficientNewsAnalyzer
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime
import json
import asyncio
import hashlib

@dataclass
class VerificationResult:
    """æ¤œè¨¼çµæœ"""
    claim: str
    verification_status: str  # "verified", "disputed", "unverified", "unknown"
    confidence_score: float
    supporting_evidence: List[str]
    contradicting_evidence: List[str]
    information_sources: List[str]
    quality_score: float
    verification_timestamp: datetime = None

    def __post_init__(self):
        if self.verification_timestamp is None:
            self.verification_timestamp = datetime.now()

@dataclass 
class SourceQuality:
    """æƒ…å ±æºå“è³ªè©•ä¾¡"""
    source_name: str
    reliability_score: float  # 0.0-1.0
    expertise_score: float    # 0.0-1.0
    bias_score: float        # 0.0-1.0 (ä½ã„ã»ã©åè¦‹ãŒå°‘ãªã„)
    freshness_score: float   # 0.0-1.0 (æƒ…å ±ã®æ–°é®®åº¦)
    overall_quality: float   # ç·åˆå“è³ªã‚¹ã‚³ã‚¢

class VerificationEngine(EfficientNewsAnalyzer):
    """
    ä¿¡é ¼æ€§æ¤œè¨¼ã‚¨ãƒ³ã‚¸ãƒ³
    
    æ—¢å­˜ã®EfficientNewsAnalyzerã‚’ç¶™æ‰¿ã—ã€ä»¥ä¸‹ã®æ©Ÿèƒ½ã‚’è¿½åŠ ï¼š
    - æƒ…å ±ã®ä¿¡é ¼æ€§æ¤œè¨¼
    - ã‚¯ãƒ­ã‚¹ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹åˆ†æ
    - æƒ…å ±æºå“è³ªè©•ä¾¡
    - ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯æ©Ÿèƒ½
    """
    
    def __init__(self):
        super().__init__()
        
        # ä¿¡é ¼ã§ãã‚‹æƒ…å ±æºã®å®šç¾©
        self.trusted_sources = {
            "TechCrunch": {"reliability": 0.85, "expertise": 0.9, "bias": 0.8},
            "The Verge": {"reliability": 0.8, "expertise": 0.85, "bias": 0.75},
            "Ars Technica": {"reliability": 0.9, "expertise": 0.95, "bias": 0.85},
            "Reuters": {"reliability": 0.95, "expertise": 0.85, "bias": 0.9},
            "Bloomberg": {"reliability": 0.9, "expertise": 0.9, "bias": 0.8},
            "VentureBeat": {"reliability": 0.75, "expertise": 0.8, "bias": 0.7},
            "MIT Technology Review": {"reliability": 0.95, "expertise": 0.95, "bias": 0.9}
        }
        
        # æ¤œè¨¼è¨­å®š
        self.verification_config = {
            "min_sources_for_verification": 2,
            "cross_reference_threshold": 0.7,
            "fact_check_confidence_threshold": 0.8,
            "max_verification_time": 300  # 5åˆ†
        }
        
        # æ¤œè¨¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥
        self.verification_cache = {}
    
    async def verify_claims(self, claims: List[str], context: Dict[str, Any] = None) -> List[VerificationResult]:
        """
        è¤‡æ•°ã®ä¸»å¼µã‚’æ¤œè¨¼
        
        Args:
            claims: æ¤œè¨¼ã™ã‚‹ä¸»å¼µã®ãƒªã‚¹ãƒˆ
            context: è¿½åŠ ã®ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆæƒ…å ±
        
        Returns:
            List[VerificationResult]: æ¤œè¨¼çµæœã®ãƒªã‚¹ãƒˆ
        """
        verification_results = []
        
        print(f"ğŸ” {len(claims)}ä»¶ã®ä¸»å¼µã‚’æ¤œè¨¼ä¸­...")
        
        for i, claim in enumerate(claims, 1):
            print(f"ğŸ“ æ¤œè¨¼ {i}/{len(claims)}: {claim[:50]}...")
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒã‚§ãƒƒã‚¯
            cache_key = self._get_verification_cache_key(claim)
            if cache_key in self.verification_cache:
                print("ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰")
                verification_results.append(self.verification_cache[cache_key])
                continue
            
            # æ¤œè¨¼å®Ÿè¡Œ
            try:
                result = await self._verify_single_claim(claim, context or {})
                verification_results.append(result)
                
                # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                self.verification_cache[cache_key] = result
                
            except Exception as e:
                print(f"âš ï¸ æ¤œè¨¼ã‚¨ãƒ©ãƒ¼: {e}")
                # ã‚¨ãƒ©ãƒ¼æ™‚ã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯çµæœ
                result = VerificationResult(
                    claim=claim,
                    verification_status="unknown",
                    confidence_score=0.0,
                    supporting_evidence=[],
                    contradicting_evidence=[],
                    information_sources=[],
                    quality_score=0.0
                )
                verification_results.append(result)
        
        print(f"âœ… æ¤œè¨¼å®Œäº†: {len(verification_results)}ä»¶")
        return verification_results
    
    async def _verify_single_claim(self, claim: str, context: Dict[str, Any]) -> VerificationResult:
        """å˜ä¸€ã®ä¸»å¼µã‚’æ¤œè¨¼"""
        
        # Step 1: ã‚¯ãƒ­ã‚¹ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹åˆ†æ
        cross_ref_analysis = await self._cross_reference_analysis(claim, context)
        
        # Step 2: ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯
        fact_check_result = await self._fact_check_claim(claim, cross_ref_analysis)
        
        # Step 3: æƒ…å ±æºå“è³ªè©•ä¾¡
        source_quality = self._evaluate_source_quality(cross_ref_analysis["sources"])
        
        # Step 4: ç·åˆåˆ¤å®š
        verification_status, confidence_score = self._determine_verification_status(
            cross_ref_analysis, fact_check_result, source_quality
        )
        
        return VerificationResult(
            claim=claim,
            verification_status=verification_status,
            confidence_score=confidence_score,
            supporting_evidence=cross_ref_analysis["supporting_evidence"],
            contradicting_evidence=cross_ref_analysis["contradicting_evidence"],
            information_sources=cross_ref_analysis["sources"],
            quality_score=source_quality["overall_quality"]
        )
    
    async def _cross_reference_analysis(self, claim: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """ã‚¯ãƒ­ã‚¹ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹åˆ†æ"""
        
        # AIåˆ†æã§ã‚¯ãƒ­ã‚¹ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹æƒ…å ±ã‚’å–å¾—
        cross_ref_prompt = f"""ä»¥ä¸‹ã®ä¸»å¼µã«ã¤ã„ã¦ã€ã‚¯ãƒ­ã‚¹ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹åˆ†æã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š

ä¸»å¼µ: {claim}
ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ: {json.dumps(context, ensure_ascii=False)}

åˆ†æé …ç›®:
1. ã“ã®ä¸»å¼µã‚’æ”¯æŒã™ã‚‹è¨¼æ‹ ãƒ»æƒ…å ±æº
2. ã“ã®ä¸»å¼µã«çŸ›ç›¾ã™ã‚‹è¨¼æ‹ ãƒ»æƒ…å ±æº  
3. é–¢é€£ã™ã‚‹æ—¢çŸ¥ã®äº‹å®Ÿ
4. æ¤œè¨¼ã«å¿…è¦ãªè¿½åŠ æƒ…å ±

å›ç­”ã‚’JSONå½¢å¼ã§ï¼š
{{
    "supporting_evidence": ["æ”¯æŒã™ã‚‹è¨¼æ‹ 1", "æ”¯æŒã™ã‚‹è¨¼æ‹ 2"],
    "contradicting_evidence": ["çŸ›ç›¾ã™ã‚‹è¨¼æ‹ 1"],
    "related_facts": ["é–¢é€£äº‹å®Ÿ1", "é–¢é€£äº‹å®Ÿ2"],
    "sources": ["æƒ…å ±æº1", "æƒ…å ±æº2"],
    "confidence": 0.8,
    "analysis_summary": "åˆ†æã®è¦ç´„"
}}
"""
        
        try:
            response = await self.llm.generate_content_async(
                cross_ref_prompt,
                agent_name="ã‚¯ãƒ­ã‚¹ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹åˆ†æã‚¨ãƒ³ã‚¸ãƒ³"
            )
            
            cross_ref_data = json.loads(response.strip())
            
            # ãƒ‡ãƒ¼ã‚¿ã®æ­£è¦åŒ–
            if "supporting_evidence" not in cross_ref_data:
                cross_ref_data["supporting_evidence"] = []
            if "contradicting_evidence" not in cross_ref_data:
                cross_ref_data["contradicting_evidence"] = []
            if "sources" not in cross_ref_data:
                cross_ref_data["sources"] = ["AIåˆ†æ"]
                
            return cross_ref_data
            
        except Exception as e:
            print(f"âš ï¸ ã‚¯ãƒ­ã‚¹ãƒªãƒ•ã‚¡ãƒ¬ãƒ³ã‚¹åˆ†æã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "supporting_evidence": [],
                "contradicting_evidence": [],
                "sources": ["åˆ†æã‚¨ãƒ©ãƒ¼"],
                "confidence": 0.0
            }
    
    async def _fact_check_claim(self, claim: str, cross_ref_data: Dict[str, Any]) -> Dict[str, Any]:
        """ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ"""
        
        fact_check_prompt = f"""ä»¥ä¸‹ã®ä¸»å¼µã®ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š

ä¸»å¼µ: {claim}

å‚è€ƒæƒ…å ±:
æ”¯æŒã™ã‚‹è¨¼æ‹ : {cross_ref_data.get('supporting_evidence', [])}
çŸ›ç›¾ã™ã‚‹è¨¼æ‹ : {cross_ref_data.get('contradicting_evidence', [])}

ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯é …ç›®:
1. ä¸»å¼µã®å…·ä½“çš„ãªè¦ç´ ã®æ¤œè¨¼
2. æ•°å€¤ãƒ»æ—¥ä»˜ãƒ»äººåãƒ»ä¼æ¥­åã®æ­£ç¢ºæ€§
3. è«–ç†çš„æ•´åˆæ€§
4. æ—¢çŸ¥ã®äº‹å®Ÿã¨ã®ç…§åˆ

å›ç­”ã‚’JSONå½¢å¼ã§ï¼š
{{
    "fact_check_status": "accurate|partially_accurate|inaccurate|unverifiable",
    "accuracy_score": 0.8,
    "verified_facts": ["æ¤œè¨¼æ¸ˆã¿äº‹å®Ÿ1"],
    "disputed_facts": ["äº‰ç‚¹ã®ã‚ã‚‹äº‹å®Ÿ1"],
    "logical_consistency": 0.9,
    "fact_check_summary": "ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯è¦ç´„"
}}
"""
        
        try:
            response = await self.llm.generate_content_async(
                fact_check_prompt,
                agent_name="ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ã‚¨ãƒ³ã‚¸ãƒ³"
            )
            
            fact_check_result = json.loads(response.strip())
            
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤è¨­å®š
            if "accuracy_score" not in fact_check_result:
                fact_check_result["accuracy_score"] = 0.5
            if "logical_consistency" not in fact_check_result:
                fact_check_result["logical_consistency"] = 0.5
                
            return fact_check_result
            
        except Exception as e:
            print(f"âš ï¸ ãƒ•ã‚¡ã‚¯ãƒˆãƒã‚§ãƒƒã‚¯ã‚¨ãƒ©ãƒ¼: {e}")
            return {
                "fact_check_status": "unverifiable",
                "accuracy_score": 0.0,
                "logical_consistency": 0.0
            }
    
    def _evaluate_source_quality(self, sources: List[str]) -> Dict[str, float]:
        """æƒ…å ±æºã®å“è³ªè©•ä¾¡"""
        if not sources:
            return {"overall_quality": 0.0}
        
        quality_scores = []
        
        for source in sources:
            source_quality = self._get_source_quality(source)
            quality_scores.append(source_quality.overall_quality)
        
        # å¹³å‡å“è³ªã‚¹ã‚³ã‚¢
        overall_quality = sum(quality_scores) / len(quality_scores)
        
        return {
            "overall_quality": overall_quality,
            "source_count": len(sources),
            "max_quality": max(quality_scores) if quality_scores else 0.0,
            "min_quality": min(quality_scores) if quality_scores else 0.0
        }
    
    def _get_source_quality(self, source_name: str) -> SourceQuality:
        """å˜ä¸€ã®æƒ…å ±æºã®å“è³ªã‚’è©•ä¾¡"""
        
        # æ—¢çŸ¥ã®ä¿¡é ¼ã§ãã‚‹æƒ…å ±æºã‚’ãƒã‚§ãƒƒã‚¯
        for trusted_source, scores in self.trusted_sources.items():
            if trusted_source.lower() in source_name.lower():
                overall_quality = (
                    scores["reliability"] * 0.4 +
                    scores["expertise"] * 0.3 + 
                    scores["bias"] * 0.3
                )
                
                return SourceQuality(
                    source_name=source_name,
                    reliability_score=scores["reliability"],
                    expertise_score=scores["expertise"],
                    bias_score=scores["bias"],
                    freshness_score=0.8,  # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤
                    overall_quality=overall_quality
                )
        
        # æœªçŸ¥ã®æƒ…å ±æºã®å ´åˆã€ä¸­ç¨‹åº¦ã®å“è³ªã‚¹ã‚³ã‚¢ã‚’è¨­å®š
        return SourceQuality(
            source_name=source_name,
            reliability_score=0.5,
            expertise_score=0.5,
            bias_score=0.5,
            freshness_score=0.5,
            overall_quality=0.5
        )
    
    def _determine_verification_status(
        self, 
        cross_ref_data: Dict[str, Any], 
        fact_check_result: Dict[str, Any], 
        source_quality: Dict[str, float]
    ) -> tuple[str, float]:
        """ç·åˆçš„ãªæ¤œè¨¼ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã¨ä¿¡é ¼åº¦ã‚’æ±ºå®š"""
        
        # å„è¦ç´ ã®ã‚¹ã‚³ã‚¢
        cross_ref_confidence = cross_ref_data.get("confidence", 0.0)
        fact_check_accuracy = fact_check_result.get("accuracy_score", 0.0)
        logical_consistency = fact_check_result.get("logical_consistency", 0.0)
        source_quality_score = source_quality.get("overall_quality", 0.0)
        
        # é‡ã¿ä»˜ãå¹³å‡ã§ç·åˆä¿¡é ¼åº¦ã‚’è¨ˆç®—
        confidence_score = (
            cross_ref_confidence * 0.3 +
            fact_check_accuracy * 0.4 +
            logical_consistency * 0.2 +
            source_quality_score * 0.1
        )
        
        # æ¤œè¨¼ã‚¹ãƒ†ãƒ¼ã‚¿ã‚¹ã®æ±ºå®š
        fact_check_status = fact_check_result.get("fact_check_status", "unverifiable")
        
        if confidence_score >= 0.8 and fact_check_status in ["accurate", "partially_accurate"]:
            verification_status = "verified"
        elif confidence_score >= 0.6 and fact_check_status == "partially_accurate":
            verification_status = "partially_verified"
        elif confidence_score < 0.4 or fact_check_status == "inaccurate":
            verification_status = "disputed"
        else:
            verification_status = "unverified"
        
        return verification_status, confidence_score
    
    def _get_verification_cache_key(self, claim: str) -> str:
        """æ¤œè¨¼ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚­ãƒ¼ã‚’ç”Ÿæˆ"""
        return hashlib.md5(claim.encode()).hexdigest()
    
    async def verify_research_result(self, research_result) -> Dict[str, Any]:
        """
        Enhanced DeepResearchã®çµæœã‚’æ¤œè¨¼
        
        Args:
            research_result: ResearchResultã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
        
        Returns:
            Dict: æ¤œè¨¼çµæœ
        """
        # ä¸»è¦ãªä¸»å¼µã‚’æŠ½å‡º
        claims = []
        claims.append(research_result.final_answer)
        
        # æ€è€ƒã‚¹ãƒ†ãƒƒãƒ—ã‹ã‚‰ã‚‚ä¸»å¼µã‚’æŠ½å‡º
        for step in research_result.thinking_steps:
            if step.phase == "reasoning":
                try:
                    step_data = json.loads(step.output_data)
                    conclusion = step_data.get("integrated_conclusion", "")
                    if conclusion and len(conclusion) > 20:
                        claims.append(conclusion)
                except:
                    continue
        
        # æ¤œè¨¼å®Ÿè¡Œ
        verification_results = await self.verify_claims(claims)
        
        # æ¤œè¨¼ã‚µãƒãƒªãƒ¼ä½œæˆ
        verified_count = sum(1 for vr in verification_results if vr.verification_status == "verified")
        disputed_count = sum(1 for vr in verification_results if vr.verification_status == "disputed")
        avg_confidence = sum(vr.confidence_score for vr in verification_results) / len(verification_results)
        avg_quality = sum(vr.quality_score for vr in verification_results) / len(verification_results)
        
        return {
            "verification_summary": {
                "total_claims": len(verification_results),
                "verified_claims": verified_count,
                "disputed_claims": disputed_count,
                "average_confidence": avg_confidence,
                "average_quality": avg_quality
            },
            "detailed_results": verification_results,
            "overall_trustworthiness": avg_confidence * avg_quality,
            "recommendation": self._generate_trust_recommendation(avg_confidence, avg_quality)
        }
    
    def _generate_trust_recommendation(self, confidence: float, quality: float) -> str:
        """ä¿¡é ¼æ€§ã«åŸºã¥ãæ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        trust_score = confidence * quality
        
        if trust_score >= 0.8:
            return "é«˜ä¿¡é ¼æ€§ï¼šã“ã®åˆ†æçµæœã¯ååˆ†ã«ä¿¡é ¼ã§ãã¾ã™ã€‚"
        elif trust_score >= 0.6:
            return "ä¸­ç¨‹åº¦ã®ä¿¡é ¼æ€§ï¼šè¿½åŠ æ¤œè¨¼ã‚’æ¨å¥¨ã—ã¾ã™ã€‚"
        elif trust_score >= 0.4:
            return "ä½ä¿¡é ¼æ€§ï¼šæ…é‡ãªåˆ¤æ–­ãŒå¿…è¦ã§ã™ã€‚"
        else:
            return "ä¿¡é ¼æ€§ä¸è¶³ï¼šå†åˆ†æã‚’æ¨å¥¨ã—ã¾ã™ã€‚" 