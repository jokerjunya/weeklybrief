#!/usr/bin/env python3
"""
Enhanced DeepResearch - ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚¨ãƒ³ã‚¸ãƒ³

æ—¢å­˜ã®data-processing.pyã®æ©Ÿèƒ½ã‚’çµ±åˆã—ã€ä»¥ä¸‹ã‚’æä¾›ï¼š
- ãƒ‡ãƒ¼ã‚¿æ­£è¦åŒ–ãƒ»å¤‰æ›
- åˆ†æçµæœã®æ°¸ç¶šåŒ–
- ã‚­ãƒ£ãƒƒã‚·ãƒ¥ç®¡ç†
- JSON/CSV ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'scripts'))

import json
import csv
from typing import Dict, List, Any, Optional
from dataclasses import dataclass, asdict
from datetime import datetime
import sqlite3
import pickle
import hashlib

@dataclass
class AnalysisRecord:
    """åˆ†æè¨˜éŒ²"""
    id: str
    topic: str
    analysis_result: Dict[str, Any]
    verification_result: Dict[str, Any]
    created_at: datetime
    confidence_score: float
    analysis_time: float
    data_sources: List[str]

class DataManager:
    """
    ãƒ‡ãƒ¼ã‚¿ç®¡ç†ã‚¨ãƒ³ã‚¸ãƒ³
    
    åˆ†æçµæœã®ä¿å­˜ã€ç®¡ç†ã€ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆæ©Ÿèƒ½ã‚’æä¾›
    """
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = data_dir
        self.db_path = os.path.join(data_dir, "enhanced_deepresearch.db")
        self.cache_dir = os.path.join(data_dir, "cache")
        
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
        os.makedirs(data_dir, exist_ok=True)
        os.makedirs(self.cache_dir, exist_ok=True)
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–
        self._init_database()
    
    def _init_database(self):
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹åˆæœŸåŒ–"""
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                CREATE TABLE IF NOT EXISTS analysis_records (
                    id TEXT PRIMARY KEY,
                    topic TEXT NOT NULL,
                    analysis_result TEXT NOT NULL,
                    verification_result TEXT NOT NULL,
                    created_at TEXT NOT NULL,
                    confidence_score REAL NOT NULL,
                    analysis_time REAL NOT NULL,
                    data_sources TEXT NOT NULL
                )
            """)
            
            conn.execute("""
                CREATE TABLE IF NOT EXISTS thinking_steps (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    analysis_id TEXT NOT NULL,
                    step_id INTEGER NOT NULL,
                    phase TEXT NOT NULL,
                    confidence REAL NOT NULL,
                    input_data TEXT,
                    output_data TEXT,
                    timestamp TEXT NOT NULL,
                    FOREIGN KEY (analysis_id) REFERENCES analysis_records (id)
                )
            """)
    
    def save_analysis_result(
        self, 
        research_result,
        verification_result: Dict[str, Any] = None
    ) -> str:
        """åˆ†æçµæœã‚’ä¿å­˜"""
        analysis_id = self._generate_analysis_id(research_result.topic)
        
        record = AnalysisRecord(
            id=analysis_id,
            topic=research_result.topic,
            analysis_result=self._serialize_research_result(research_result),
            verification_result=verification_result or {},
            created_at=datetime.now(),
            confidence_score=research_result.confidence_score,
            analysis_time=research_result.time_taken,
            data_sources=research_result.sources_analyzed
        )
        
        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ã«ä¿å­˜
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT OR REPLACE INTO analysis_records 
                (id, topic, analysis_result, verification_result, created_at, 
                 confidence_score, analysis_time, data_sources)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            """, (
                record.id,
                record.topic,
                json.dumps(record.analysis_result, ensure_ascii=False),
                json.dumps(record.verification_result, ensure_ascii=False),
                record.created_at.isoformat(),
                record.confidence_score,
                record.analysis_time,
                json.dumps(record.data_sources)
            ))
            
            # æ€è€ƒã‚¹ãƒ†ãƒƒãƒ—ã‚‚ä¿å­˜
            for step in research_result.thinking_steps:
                conn.execute("""
                    INSERT INTO thinking_steps 
                    (analysis_id, step_id, phase, confidence, input_data, output_data, timestamp)
                    VALUES (?, ?, ?, ?, ?, ?, ?)
                """, (
                    analysis_id,
                    step.step_id,
                    step.phase,
                    step.confidence,
                    step.input_data,
                    step.output_data,
                    step.timestamp.isoformat()
                ))
        
        print(f"ğŸ’¾ åˆ†æçµæœã‚’ä¿å­˜: {analysis_id}")
        return analysis_id
    
    def load_analysis_result(self, analysis_id: str) -> Optional[AnalysisRecord]:
        """åˆ†æçµæœã‚’èª­ã¿è¾¼ã¿"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                SELECT * FROM analysis_records WHERE id = ?
            """, (analysis_id,))
            
            row = cursor.fetchone()
            if not row:
                return None
            
            return AnalysisRecord(
                id=row[0],
                topic=row[1],
                analysis_result=json.loads(row[2]),
                verification_result=json.loads(row[3]),
                created_at=datetime.fromisoformat(row[4]),
                confidence_score=row[5],
                analysis_time=row[6],
                data_sources=json.loads(row[7])
            )
    
    def get_recent_analyses(self, limit: int = 10) -> List[AnalysisRecord]:
        """æœ€è¿‘ã®åˆ†æçµæœã‚’å–å¾—"""
        with sqlite3.connect(self.db_path) as conn:
            cursor = conn.execute("""
                SELECT * FROM analysis_records 
                ORDER BY created_at DESC 
                LIMIT ?
            """, (limit,))
            
            records = []
            for row in cursor.fetchall():
                record = AnalysisRecord(
                    id=row[0],
                    topic=row[1],
                    analysis_result=json.loads(row[2]),
                    verification_result=json.loads(row[3]),
                    created_at=datetime.fromisoformat(row[4]),
                    confidence_score=row[5],
                    analysis_time=row[6],
                    data_sources=json.loads(row[7])
                )
                records.append(record)
            
            return records
    
    def export_to_json(self, analysis_id: str, output_path: str) -> bool:
        """JSONå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        try:
            record = self.load_analysis_result(analysis_id)
            if not record:
                return False
            
            export_data = {
                "analysis_record": asdict(record),
                "export_timestamp": datetime.now().isoformat(),
                "format_version": "1.0"
            }
            
            # datetime ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’æ–‡å­—åˆ—ã«å¤‰æ›
            export_data["analysis_record"]["created_at"] = record.created_at.isoformat()
            
            with open(output_path, 'w', encoding='utf-8') as f:
                json.dump(export_data, f, ensure_ascii=False, indent=2)
            
            print(f"ğŸ“¤ JSON ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {output_path}")
            return True
            
        except Exception as e:
            print(f"âŒ JSON ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def export_summary_to_csv(self, output_path: str) -> bool:
        """ã‚µãƒãƒªãƒ¼ã‚’CSVå½¢å¼ã§ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆ"""
        try:
            records = self.get_recent_analyses(limit=100)
            
            with open(output_path, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                
                # ãƒ˜ãƒƒãƒ€ãƒ¼
                writer.writerow([
                    "ID", "ãƒˆãƒ”ãƒƒã‚¯", "ä½œæˆæ—¥æ™‚", "ä¿¡é ¼åº¦", "åˆ†ææ™‚é–“(ç§’)",
                    "ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹æ•°", "æœ€çµ‚å›ç­”"
                ])
                
                # ãƒ‡ãƒ¼ã‚¿
                for record in records:
                    final_answer = record.analysis_result.get("final_answer", "")
                    if len(final_answer) > 100:
                        final_answer = final_answer[:97] + "..."
                    
                    writer.writerow([
                        record.id,
                        record.topic,
                        record.created_at.strftime("%Y-%m-%d %H:%M:%S"),
                        f"{record.confidence_score:.2f}",
                        f"{record.analysis_time:.2f}",
                        len(record.data_sources),
                        final_answer
                    ])
            
            print(f"ğŸ“Š CSV ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆå®Œäº†: {output_path}")
            return True
            
        except Exception as e:
            print(f"âŒ CSV ã‚¨ã‚¯ã‚¹ãƒãƒ¼ãƒˆã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def cache_data(self, key: str, data: Any, ttl_hours: int = 24) -> bool:
        """ãƒ‡ãƒ¼ã‚¿ã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥"""
        try:
            cache_file = os.path.join(self.cache_dir, f"{key}.pkl")
            cache_data = {
                "data": data,
                "cached_at": datetime.now(),
                "ttl_hours": ttl_hours
            }
            
            with open(cache_file, 'wb') as f:
                pickle.dump(cache_data, f)
            
            return True
            
        except Exception as e:
            print(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def get_cached_data(self, key: str) -> Optional[Any]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—"""
        try:
            cache_file = os.path.join(self.cache_dir, f"{key}.pkl")
            
            if not os.path.exists(cache_file):
                return None
            
            with open(cache_file, 'rb') as f:
                cache_data = pickle.load(f)
            
            # TTL ãƒã‚§ãƒƒã‚¯
            cached_at = cache_data["cached_at"]
            ttl_hours = cache_data["ttl_hours"]
            expires_at = cached_at.replace(
                hour=cached_at.hour + ttl_hours
            )
            
            if datetime.now() > expires_at:
                os.remove(cache_file)
                return None
            
            return cache_data["data"]
            
        except Exception as e:
            print(f"âŒ ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None
    
    def get_analysis_statistics(self) -> Dict[str, Any]:
        """åˆ†æçµ±è¨ˆã‚’å–å¾—"""
        with sqlite3.connect(self.db_path) as conn:
            # åŸºæœ¬çµ±è¨ˆ
            cursor = conn.execute("SELECT COUNT(*) FROM analysis_records")
            total_analyses = cursor.fetchone()[0]
            
            cursor = conn.execute("SELECT AVG(confidence_score) FROM analysis_records")
            avg_confidence = cursor.fetchone()[0] or 0
            
            cursor = conn.execute("SELECT AVG(analysis_time) FROM analysis_records")
            avg_time = cursor.fetchone()[0] or 0
            
            # æœ€è¿‘ã®æ´»å‹•
            cursor = conn.execute("""
                SELECT COUNT(*) FROM analysis_records 
                WHERE created_at > datetime('now', '-7 days')
            """)
            recent_analyses = cursor.fetchone()[0]
            
            return {
                "total_analyses": total_analyses,
                "average_confidence": avg_confidence,
                "average_analysis_time": avg_time,
                "recent_analyses_7days": recent_analyses,
                "database_path": self.db_path,
                "cache_directory": self.cache_dir
            }
    
    def _generate_analysis_id(self, topic: str) -> str:
        """åˆ†æIDã‚’ç”Ÿæˆ"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        topic_hash = hashlib.md5(topic.encode()).hexdigest()[:8]
        return f"analysis_{timestamp}_{topic_hash}"
    
    def _serialize_research_result(self, research_result) -> Dict[str, Any]:
        """ResearchResultã‚’è¾æ›¸ã«å¤‰æ›"""
        return {
            "topic": research_result.topic,
            "final_answer": research_result.final_answer,
            "confidence_score": research_result.confidence_score,
            "time_taken": research_result.time_taken,
            "sources_analyzed": research_result.sources_analyzed,
            "quality_metrics": research_result.quality_metrics,
            "thinking_steps_count": len(research_result.thinking_steps)
        } 